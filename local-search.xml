<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>文献阅读|样本量对滑坡易感性模型的影响：启发式、统计、机器学习、深度学习和集成学习模型与SHAP分析的比较研究</title>
    <link href="/2024/12/04/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%E6%A0%B7%E6%9C%AC%E9%87%8F%E5%AF%B9%E6%BB%91%E5%9D%A1%E6%98%93%E6%84%9F%E6%80%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%BD%B1%E5%93%8D%EF%BC%9A%E5%90%AF%E5%8F%91%E5%BC%8F%E3%80%81%E7%BB%9F%E8%AE%A1%E3%80%81%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%81%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%92%8C%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E4%B8%8ESHAP%E5%88%86%E6%9E%90%E7%9A%84%E6%AF%94%E8%BE%83%E7%A0%94%E7%A9%B6/"/>
    <url>/2024/12/04/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%E6%A0%B7%E6%9C%AC%E9%87%8F%E5%AF%B9%E6%BB%91%E5%9D%A1%E6%98%93%E6%84%9F%E6%80%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%BD%B1%E5%93%8D%EF%BC%9A%E5%90%AF%E5%8F%91%E5%BC%8F%E3%80%81%E7%BB%9F%E8%AE%A1%E3%80%81%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%81%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%92%8C%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E4%B8%8ESHAP%E5%88%86%E6%9E%90%E7%9A%84%E6%AF%94%E8%BE%83%E7%A0%94%E7%A9%B6/</url>
    
    <content type="html"><![CDATA[<p><a href="https://www.sciencedirect.com/science/article/pii/S0098300424002061">原文链接</a><br><a href="https://mp.weixin.qq.com/s/B0t4IFCohjRHlcudmNLL0A">推荐链接</a></p><h2 id="内容概述">内容概述</h2><h3 id="样本量、时变特征因子影响验证">样本量、时变特征因子影响验证</h3><p>通过比较启发式、统计、机器学习和集成学习模型（层次分析过程（AHP）、频率比（FR）、逻辑回归（LR）、基于Keras的深度学习（KBDL）、XGBoost和LightGBM）在六种不同样本大小（100%、90%、75%、50%、25%和10%）下的表现。结果显示，在各种样本大小下，XGBoost和LightGBM始终优于其他模型。LR和KBDL模型紧随其后，而FR模型受样本大小变化影响最大。AHP作为经验模型，在样本大小变化下保持不受影响。通过SHAP分析，海拔、NDVI、坡度、土地利用以及距离道路和河流等因素被确认为研究区滑坡发生的关键指标，表明人类活动显著影响这些事件。通过验证人类活动和气候的五个时间变化指标，证实了这一推断，为识别滑坡触发因素提供了新方法，尤其是在人类活动强烈的地区。<br><img src="/image/1-s2.0-S0098300424002061-gr2.jpg" alt="方法框架"><br><em><strong>样本量对滑坡易感性模型的影响</strong></em></p><h3 id="LSA">LSA</h3><p>六种模型、样本量、特征、SHAP分析得到影响滑坡发生的关键特征因子、验证时变指标<br><strong>滑坡敏感性评估</strong> landslide susceptibility assessment LSA：根据各种指标在空间上估计山体滑坡更可能发生的“位置”，包括准备指标（例如，岩性、坡度、高程、坡向和土壤）和触发指标（例如，降雨、地震和人类活动）<br>LSA 模型可分为启发式、确定性和统计类型</p><table><thead><tr><th>模型</th><th>例如</th><th>优劣</th></tr></thead><tbody><tr><td>启发式模型</td><td>层次分析过程（<strong>AHP</strong>）、模糊逻辑、多标准决策分析 （MCDA）</td><td>依赖于专家判断来评估易感性指标，但由于专家知识的可变性，可能会引入显着的主观性</td></tr><tr><td>确定式模型</td><td>RIGRS、SHALSTAB 模型</td><td>应用物理原理对边坡不稳定性进行建模，提供对特定破坏机制的见解，但由于密集的数据需求和对简单滑坡类型的限制，在大规模应用中往往受到限制</td></tr><tr><td>统计式模型</td><td>使用历史数据预测山体滑坡：频率比 （<strong>FR</strong>） 、证据权重等传统技术；更高级的机器学习 （ML） 模型，包括逻辑回归 （<strong>LR</strong>） 和支持向量机 （SVM）；尖端的深度学习 （DL） 和集成学习 （EL） 模型，如卷积神经网络、<strong>基于 Keras 的深度学习 （KBDL）、XGBoost、LightGBM</strong></td><td>客观性、有效性；XGBoost：性能卓越、效率高、调整超参数灵活；统计模型严重依赖于准确和全面的山体滑坡清单，因为它们是基于历史山体滑坡模式可以预测未来事件的假设。清单的完整性和准确性至关重要，因为即使数据质量或样本量的微小变化也会显着影响模型性能因此<strong>探索样本量对不同模型的影响对于 LSA 至关重要</strong></td></tr></tbody></table><p><strong>样本量</strong><br>为了评估样本量对模型有效性的影响，将 70% 的滑坡点训练样本（“1”值）分为 10%、25%、50%、75%、90% 和 100% 的六个子集。为了进行验证，随机选择了总滑坡点的 30% （277）。非滑坡训练点（“0”值）同样被划分为 6 个平衡子集，并且还创建了 277 个非滑坡点用于模型验证。此过程是使用 ArcGIS 10.8 中的“创建随机点”工具执行的。对于四个 ML 模型，模型构建中涉及其他步骤。我们使用 ArcGIS 10.8 中的“提取值到点”工具从训练滑坡和非滑坡点的位置提取指标值。具体来说，对于岩性和 LULC 等分类指标，我们采用分类编码将这些特征转换为数字形式。</p><p><strong>模型</strong></p><table><thead><tr><th>模型</th><th>解释</th></tr></thead><tbody><tr><td>AHP</td><td>一种结构化的决策方法，旨在通过将复杂问题分解为分层子问题来解决问题</td></tr><tr><td>FR</td><td>LSA 中广泛使用的一种双变量统计技术，通过分析滑坡与特征因子的关系来计算滑坡发生的条件概率</td></tr><tr><td>LR</td><td>一种传统的 ML 模型，用于对某个类别或事件发生的概率进行建模，该模型也广泛用于 LSA</td></tr><tr><td>KBDL</td><td>是一个开源神经网络框架，专为快速 DL 实验而设计。在 LSA 中，它采用深度神经网络来捕获指标与滑坡发生之间的复杂非线性关系</td></tr><tr><td>XGBoost</td><td>最先进的 EL 模型，属于梯度提升决策树 （GBDT） 系列。当应用于 LSA 时，XGBoost 可以通过迭代改进其预测来有效地处理各种指标和山体滑坡事件之间的非线性依赖关系</td></tr><tr><td>LightGBM</td><td>EL模型，一种高效的梯度提升框架，针对大型高维数据集进行了优化，在 LSA 中具有显着的应用</td></tr></tbody></table><p><strong>模型校准</strong><br>TPE方法：用于超参数优化，通过从先前的评估中学习以专注于有前途的搜索空间领域来提高效率<br>构造了两个概率模型：l(X)和g(x)，分别反映好超参数和坏超参数的分布</p><p><strong>模型验证</strong><br>Precision、Recall、<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>F</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">F_{1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> score、OA（总体准确率）、AP（平均精确率）、AU ROC（受试者工作特征下面积）、AMP（平均模型性能）</p><p><strong>SHAP分析</strong><br>SHAP 分析作为可解释 ML 技术的一种事后方法，通过量化每个特征对预测的贡献，提供了一种详细的方法来解释机器学习模型的预测。这项技术阐明了输入特征在影响模型结果中的作用，这对于显示输入和输出错综复杂的复杂模型的行为至关重要</p><h2 id="实验结果">实验结果</h2><p>1、相关性分析和多重共线性分析：Pearson相关系数（热力图）、VIF和TOL（VIF倒数）</p><p>2、LS映射：用自然断点法将 31 个（AHP1，其他5个模型不同样本量各6）模型的 LS 映射分为五个级别（非常高、高、中等、低和非常低）</p><p>3、模型性能对比</p><p>4、指标分析和触发因素分析<br>对不同样本量的XGBoost进行SHAP分析<br>对不同样本量的LightGBM进行SHAP分析<br>结果：滑坡由人类引起<br>验证：选取与人类活动相关的特征因子，滑坡发生的FR在不同时变特征因子上的分布</p><h2 id="讨论">讨论</h2><p><strong>样本量</strong><br>提升样本量模型的稳定性和准确性，确保了对不完整或稀疏数据集的弹性，这些模型中EL模型表现最好，但是对样本量的敏感性最小</p><p><strong>触发因素分析和人类活动</strong><br>SHAP分析</p><p><strong>改进的LSA模型选择框架</strong><br><img src="/image/1-s2.0-S0098300424002061-gr14.jpg" alt=""></p><h2 id="总结">总结</h2><p>局限：<br>1、不同环境条件下的样本量对模型的影响不一致，本研究没有泛用性<br>2、没有从时间尺度、季节角度分析滑坡</p><p>内容：<br>滑坡清查的质量和完整性可能会显著影响 LSA，并进一步影响后续的滑坡管理、土地规划和区域可持续发展。本研究评估了竹山县 6 个样本量 （100%、90%、75%、50%、25% 和 10%） 的 6 个 LSA 模型 （AHP、FR、LR、KBDL、XGBoost 和 LightGBM） 的性能，具体探讨了滑坡清单的完整性对不同模型的影响程度。结果揭示了不同模型受样本量影响的方式存在显著差异，使其适用于不同的场景。当滑坡清单的质量和完整性不确定时，建议使用 XGBoost 和 LightGBM 模型，因为它们在 LSA 中显示出样本量变化的影响最小。对于极度稀缺或没有库存的情况，建议使用 AHP，因为它不依赖数据进行模型构建。尽管 FR 模型对样本量敏感，但其合理的性能、低计算机资源消耗和易于操作使其成为完整库存场景的可行选择。因此，了解样本量对 LSA 的影响至关重要，但为 LSA 选择合适的模型仍然需要考虑各种情况。<br>SHAP 分析可视化了各种滑坡指标如何促进模型构建，并通过与海拔、NDVI、坡度、与河流和道路的接近程度以及 LULC 相关的结果正确推断出人类活动作为研究区域的触发因素。关于人类活动和气候的五个时变指标的 FR 验证了这一推论，这为识别滑坡触发因素提供了一种新方法。拟议的修订后的 LSA 框架旨在通过整合对景观特征的透彻理解来加强区域滑坡管理，建议在全球范围内的 LSA 工作中更广泛地采用该框架。未来的研究应侧重于扩大模型多样性以解决各种条件下数据不完整的影响，增强 LSA 框架的适应性，使用可解释的 ML 技术深入分析人类活动对山体滑坡的影响，解决 LSA 中的时间库存不完整问题，并批判性地评估模型对多个学科之间样本量变化的敏感性。</p>]]></content>
    
    
    
    <tags>
      
      <tag>文献</tag>
      
      <tag>滑坡</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>随笔|我的生日</title>
    <link href="/2024/11/22/%E9%9A%8F%E7%AC%94%E6%88%91%E7%9A%84%E7%94%9F%E6%97%A5/"/>
    <url>/2024/11/22/%E9%9A%8F%E7%AC%94%E6%88%91%E7%9A%84%E7%94%9F%E6%97%A5/</url>
    
    <content type="html"><![CDATA[<p>我每年都过两个生日，一个阴历的，一个阳历的，今天是我的阳历生日。</p><p>人一生只出生一次，却像庆祝节日般每年都庆祝“生日”，我更是贪心地乘以二，因此我受到了惩罚————我好像没有过生日。</p><p>“一个人，出生了，这就不再是一个可以辩论的问题，而只是上帝交给他的一个事实”。</p><p>生日有什么好庆祝的？回忆我给别人过生日时听到的祝福语，无非是“祝你生日快乐”，连生日快乐歌也只循环这一句话，祝你生日快乐，是祝你在出生的这一天获得快乐？难道人只有在生日的这一天才能获得快乐吗？好吧，或许它的意思是祝你出生后获得快乐。对于老人，或许还有些新词，祝你寿比南山，这就合理很多，庆祝又活过了一岁，我愿意给老人过生日，因为他们时日无多，而年轻人过生日是没有必要的。</p><p>我总觉得我出生得不是时候，我的生日往往伴随着考试，即使我已经22岁，我也下周一考试，呵呵，谁允许研究生考试的？除此之外，我从初中开始就不在老家上学，大学更是在很远的地方，所以也没有亲戚给我过生日，连“生日快乐”都听不到，生日不光没有带给我快乐，甚至让我常有压力，对我而言似乎不是一个节日，每当想起这些我更是不快乐。</p><p>我妈在我生日时叫我请她喝奶茶，她说她生我时很痛苦，所以我的生日应该请她，我不请，我又不想出生。</p><p>今天早上我吃了虎皮瑞士卷，中午吃了金牌韩式泡菜拌面，晚上吃了肉松咸蛋黄饭团。好吧，这和我平时吃的差不多，我真的没有过生日。</p><p>我觉得“过生日”是有标准的，要么和亲戚在一起，要么和朋友在一起，只可惜这两者在此时此境都没有。</p><p>我听过最好的生日寄言是“感谢上帝让你诞生于今日”，虽然那个人并不信上帝，但好在查重率0%，也不是让我感谢她，我讨厌感谢。我的记性很不好，我都忘了我们是怎么闹矛盾的？反正我只能在梦里见到她，并且她总是带着讥讽，那我也讨厌她。</p><p>唉，今天是我的生日。</p><p>既然无法获得快乐，那我祝我寿比南山。</p>]]></content>
    
    
    
    <tags>
      
      <tag>随笔</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>滑坡实验Geodetector</title>
    <link href="/2024/11/19/%E6%BB%91%E5%9D%A1%E5%AE%9E%E9%AA%8CGeodetector/"/>
    <url>/2024/11/19/%E6%BB%91%E5%9D%A1%E5%AE%9E%E9%AA%8CGeodetector/</url>
    
    <content type="html"><![CDATA[<p><a href="https://blog.csdn.net/zhebushibiaoshifu/article/details/124526189">Geodetector软件下载、地理探测器的应用实践与结果解读</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> sklearn.cluster <span class="hljs-keyword">import</span> KMeans<br><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> calinski_harabasz_score<br><br><br><span class="hljs-comment"># 自动选择最佳聚类数（使用 CH 指数）</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">recommend_clusters_by_ch</span>(<span class="hljs-params">values, max_clusters=<span class="hljs-number">10</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    根据 CH 指数推荐最佳聚类数</span><br><span class="hljs-string">    :param values: 输入数据 (array-like)</span><br><span class="hljs-string">    :param max_clusters: 最大聚类数</span><br><span class="hljs-string">    :return: 推荐的聚类数</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    ch_scores = []<br>    <span class="hljs-keyword">for</span> n_clusters <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">2</span>, max_clusters + <span class="hljs-number">1</span>):<br>        kmeans = KMeans(n_clusters=n_clusters, random_state=<span class="hljs-number">0</span>)<br>        labels = kmeans.fit_predict(values)<br>        ch_score = calinski_harabasz_score(values, labels)<br>        ch_scores.append(ch_score)<br><br>    <span class="hljs-comment"># 选择 CH 指数的最大值对应的聚类数</span><br>    best_n_clusters = <span class="hljs-built_in">range</span>(<span class="hljs-number">2</span>, max_clusters + <span class="hljs-number">1</span>)[np.argmax(ch_scores)]<br>    <span class="hljs-keyword">return</span> best_n_clusters<br><br><br><span class="hljs-comment"># 处理每列数据并自动选择聚类数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">cluster_each_column_with_ch</span>(<span class="hljs-params">data, max_clusters=<span class="hljs-number">10</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    对每列数据分别聚类并保存结果</span><br><span class="hljs-string">    :param data: DataFrame 输入数据</span><br><span class="hljs-string">    :param max_clusters: 最大聚类数</span><br><span class="hljs-string">    :return: 聚类后的 DataFrame</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    clustered_data = data.copy()<br>    <span class="hljs-keyword">for</span> column <span class="hljs-keyword">in</span> data.columns:<br>        values = data[column].dropna().values.reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)  <span class="hljs-comment"># 去除空值并转换为二维</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(np.unique(values)) &lt; <span class="hljs-number">2</span>:  <span class="hljs-comment"># 如果所有值相同，无需聚类</span><br>            clustered_data[<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;column&#125;</span>_Cluster&quot;</span>] = -<span class="hljs-number">1</span>  <span class="hljs-comment"># 标记为 -1</span><br>            <span class="hljs-keyword">continue</span><br><br>        <span class="hljs-comment"># 根据 CH 指数推荐聚类数</span><br>        n_clusters = recommend_clusters_by_ch(values, max_clusters=max_clusters)<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;列 &#x27;<span class="hljs-subst">&#123;column&#125;</span>&#x27; 推荐的聚类数: <span class="hljs-subst">&#123;n_clusters&#125;</span>&quot;</span>)<br><br>        <span class="hljs-comment"># 使用推荐的聚类数进行聚类</span><br>        kmeans = KMeans(n_clusters=n_clusters, random_state=<span class="hljs-number">0</span>)<br>        clustered_data[<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;column&#125;</span>_Cluster&quot;</span>] = kmeans.fit_predict(values)<br><br>    <span class="hljs-keyword">return</span> clustered_data<br><br><br><span class="hljs-comment"># 主程序</span><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    input_file = <span class="hljs-string">&quot;cluster0.xlsx&quot;</span>  <span class="hljs-comment"># 替换为你的文件路径</span><br>    output_file = <span class="hljs-string">&quot;clustered_output.xlsx&quot;</span><br><br>    <span class="hljs-comment"># 读取数据</span><br>    data = pd.read_excel(input_file)<br><br>    <span class="hljs-comment"># 对每列数据进行聚类并保存结果</span><br>    clustered_data = cluster_each_column_with_ch(data, max_clusters=<span class="hljs-number">10</span>)<br>    clustered_data.to_excel(output_file, index=<span class="hljs-literal">False</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;处理完成，结果已保存到 <span class="hljs-subst">&#123;output_file&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure>]]></content>
    
    
    
    <tags>
      
      <tag>滑坡</tag>
      
      <tag>代码</tag>
      
      <tag>聚类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>滑坡实验|我的问题</title>
    <link href="/2024/11/19/%E6%BB%91%E5%9D%A1%E5%AE%9E%E9%AA%8C%E6%88%91%E7%9A%84%E9%97%AE%E9%A2%98/"/>
    <url>/2024/11/19/%E6%BB%91%E5%9D%A1%E5%AE%9E%E9%AA%8C%E6%88%91%E7%9A%84%E9%97%AE%E9%A2%98/</url>
    
    <content type="html"><![CDATA[<p><strong>滑坡易发性预测的目的是什么</strong><br>-山泥倾泻经常造成严重的财产损失和人员伤亡。因此，拥有快速准确的滑坡测绘 （LM） 以支持<strong>震后</strong>滑坡损失评估和紧急救援工作至关重要<br>-高效、精准、可靠的滑坡易发性评价方法是<strong>灾前</strong>科学预警和全面防治的关键手段</p>]]></content>
    
    
    
    <tags>
      
      <tag>大思考</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>文献笔记|基于混合CNN-transformer网络的滑坡制图和基于地形和光谱特征的遥感图像的深度迁移学习</title>
    <link href="/2024/11/19/%E5%9F%BA%E4%BA%8E%E6%B7%B7%E5%90%88CNN-transformer%E7%BD%91%E7%BB%9C%E7%9A%84%E6%BB%91%E5%9D%A1%E5%88%B6%E5%9B%BE%E5%92%8C%E5%9F%BA%E4%BA%8E%E5%9C%B0%E5%BD%A2%E5%92%8C%E5%85%89%E8%B0%B1%E7%89%B9%E5%BE%81%E7%9A%84%E9%81%A5%E6%84%9F%E5%9B%BE%E5%83%8F%E7%9A%84%E6%B7%B1%E5%BA%A6%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/"/>
    <url>/2024/11/19/%E5%9F%BA%E4%BA%8E%E6%B7%B7%E5%90%88CNN-transformer%E7%BD%91%E7%BB%9C%E7%9A%84%E6%BB%91%E5%9D%A1%E5%88%B6%E5%9B%BE%E5%92%8C%E5%9F%BA%E4%BA%8E%E5%9C%B0%E5%BD%A2%E5%92%8C%E5%85%89%E8%B0%B1%E7%89%B9%E5%BE%81%E7%9A%84%E9%81%A5%E6%84%9F%E5%9B%BE%E5%83%8F%E7%9A%84%E6%B7%B1%E5%BA%A6%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/</url>
    
    <content type="html"><![CDATA[<p>所属期刊：International Journal of Applied Earth Observation and Geoinformation<br><a href="https://www.sciencedirect.com/science/article/pii/S1569843223004363?via%3Dihub">原文链接</a></p><h2 id="论文摘要">论文摘要</h2><p>山泥倾泻经常造成严重的财产损失和人员伤亡。因此，拥有快速准确的滑坡测绘 （LM） 以支持震后滑坡损失评估和紧急救援工作至关重要。近年来，关于使用遥感图像 （RSI） 的自动 LM 方法的应用进行了许多研究。然而，由于滑坡之间特征和尺度差异大，以及光学 RSI 中不同地面物体之间的相似性，现有方法在准确区分滑坡方面面临挑战。在这里，我们提出了一种称为 SCDUNet++ 的语义分割模型，它结合了卷积神经网络 （CNN） 和 transformer 的优点，以增强对滑坡特征的区分和提取。然后，我们使用 Sentinel-2 和 NASADEM 数据构建了泸定和九寨沟地震区多通道滑坡数据集。我们评估了 SCDUNet++ 在此数据集上的性能。结果表明，SCDUNet++ 可以更有效地提取和融合光谱和地形信息。与其他最先进的模型相比，SCDUNet++ 在所有四个测试领域都获得了最高的 IoU 和 F1 分数。此外，经过知识转移和微调，这些模型在绘制九寨沟地区滑坡方面取得了显著改进。与直接预测相比，实施深度迁移学习 （DTL） 后，DeepLabv3+、Segformer、TransUNet、SwinUNet、STUNet、UNet、UNet++ 和 SCDUNet++ 八个模型显示出 IoU 从 8.33% 到 27.5% 和 F1 从 6.58% 到 23.67% 的改进。这一发现突出了在数据匮乏地区使用 DTL 进行跨域 LM 的重要实用性。</p><h2 id="存在的问题">存在的问题</h2><p>通过视觉解释和实地调查进行的传统滑坡测绘 （LM） 是劳动密集型和耗时的，由于人类经验和判断的差异，存在不一致和遗漏的风险<br>机器学习方法仍然需要一些手动特征提取和解释规则设计，并且设计中算法改进的潜力是有限的<br>CNN 的归纳偏差限制了其提取全球信息的能力<br>transformer 结构所依赖的自注意机制的时间和空间复杂性在输入长度上是二次的。这在处理高分辨率图像时带来了挑战<br>Swin Transformer通过多尺度建模构建了类似于 CNN 的分层特征，并使用移位窗口方法降低了自注意的计算成本。Swin Transformer 被用作提取特征的骨干，以绘制汶川地震引发的滑坡，并取得了最佳性能。此外，研究人员还使用 Swin Transformer 基于滑坡特征构建 LM 网络，但他们都只使用 Swin Transformer 来提取特征，而没有结合 CNN 的优势<br>LM模型直接在目标区域进行训练可能会遇到 RSI 过度混浊和可用数据不足等问题<br>稳健深度迁移学习 （DTL） 对使用少量数据的跨域 LM 的可行性尚未得到证实<br>现有的研究缺乏一个合适的模型来充分利用滑坡的光谱和地形特征知识</p><h2 id="主要贡献">主要贡献</h2><p>(1)为 LM 设计了一个名为 SCDUNet++ 的模型。构建了全局局部特征提取 （GLFE） 模块以提取更准确可靠的特征，并构建了详细空间光谱聚合 （DSSA） 模块以提取滑坡的多尺度特征、空间特征和光谱特征。该模型的可靠性和泛化性在四个方面进行了评估，表明我们的模型优于其他模型，包括 FCN、DeepLabv3+、Segformer、TransUNet、SwinUNet、STUNet、UNet 和 UNet++<br>(2)我们使用在源域中训练的模型在目标域中使用少量数据执行 LM。实验表明，DTL 后的映射结果显著优于直接预测的结果。在需要准确 LM 的情况下，这一发现具有重要意义，尤其是当目标区域中的可用数据有限时<br>(3)在这项研究中，超过 3000 平方公里对泸定和九寨沟滑坡清单图进行解释生成，并为LM制作了多通道滑坡数据集。该数据集有助于证明，结合相关的光谱和地形信息可以产生更准确的结果</p><h2 id="研究方法">研究方法</h2><h3 id="数据准备">数据准备</h3><p><strong>Sentinel-2 多光谱数据</strong>：<br><a href="https://scihub.copernicus.eu/dhus/#/home">https://scihub.copernicus.eu/dhus/#/home</a> 下载，并使用 Sen2Cor 转换为大气底层产品插件，用于大气顶部产品的大气校正。然后使用 SNAP 中的 Sen2Res插件将所有波段处理到 10 m 分辨率<br>坐标系：WGS 1984 UTM Zone 48 N<br>使用 Sentinel-2 波段生成了四个光谱指数：归一化差值植被指数 （NDVI）、归一化差值水分指数 （NDWI）、亮度指数 （BI）和裸土指数 （BSI）<br>计算公式见原文<br><strong>NASADEM 数据和地形因素</strong><br>NASADEM 数据是从 <a href="https://lpdaac.usgs.gov/products/nasadem_hgtv001/">https://lpdaac.usgs.gov/products/nasadem_hgtv001/</a> 获得，使用 Python 的线性插值重新采样至 10 m 分辨率。坡度、坡向、曲率、山体阴影和地形湿度指数 （TWI） 来自数字高程模型 （DEM） 数据<br><strong>滑坡清单和数据集</strong><br>首先，将来自 13 个 Sentinel-2 波段、4 个 SIF 和 6 个 TF 的数据堆叠在一起，创建一个 23 通道的数据。然后使用最小-最大标准化对每个数据层进行标准化。其次，将数据分为 128 × 128 个大小，无重叠。泸定地区获得 128 × 128 × 23 形状的 1800 个数据，而在九寨沟地区获得 40 个相同形状的数据。最后，将数据以 HDF5 格式存储为数据集。</p><h3 id="方法">方法</h3><p><strong>1、模型架构</strong><br>SCDUNet++ 的整体架构如图所示。该模型中使用的编码器是 GLFE 模块，它是一个混合 CNN-transformer 模块。同时，DSSA Block 旨在对光谱和空间特征进行更精细的提取。此外，使用具有密集跳跃连接 （DSC） 的嵌套解码器来恢复每个阶段特征的分辨率。最后，对每个阶段的特征进行加权和融合，得到最终结果。实现代码将在 <a href="https://github.com/lewuu/SCDUNetPP">https://github.com/lewuu/SCDUNetPP</a> 上提供<br><img src="/image/11191.jpg" alt="（a） SCDUNet++;（b） 全局局部特征提取 （GLFE） 块;（c） 嵌套解码器;（d） 密集跳跃连接 （DSC）;（e） 详细空间光谱聚合 （DSSA） 模块"><br>全局局部特征提取块<br>详细的空间光谱 Aggregation 模块</p><p><strong>2、深度迁移学习方法</strong><br>采用了一种简单的基于模型的 DTL 方法来评估 DTL 提高 LM 和我们模型的传输性能的可行性，如图 5 所示。来自源域模型的某些层的参数将传输到目标模型，从而使目标模型能够从源域中学习知识。随后，可以使用少量目标数据对目标模型进行微调，以获得适用于目标域的模型<br><img src="/image/1-s2.0-S1569843223004363-gr5.jpg" alt="一种基于网络的深度迁移学习 （DTL） 方法"></p><p><strong>3、 实验设置</strong><br>损失函数：Lovász-Softmax 损失和Tversky 损失被合并为平衡的 Lovász-softmax Tversky 损失<br>评估指标：精确度、召回率、F1、IoU、MIoU 和 MCC<br>实现细节</p><h2 id="主要结论">主要结论</h2><p>快速准确地绘制滑坡地图是一项具有挑战性的任务。为了验证使用多通道数据对 LM 的语义分割模型的有效性，我们在泸定和九寨沟地震区进行了一项研究。可以得出以下结论：<br>(1)向多光谱数据添加光谱和地形信息对 LM 有利，地形信息起着更重要的作用。添加 TF 和 SIF 后，模型的 IoU 提高了 11.4% 至 17.37%，F1 的 IoU 提高了 10.76% 至 15.39%。<br>(2)与其他八个模型（包括 FCN、DeepLabv3+、Segformer、TransUNet、SwinUNet、STUNet、UNet 和 UNet++）相比，SCDUNet++ 的 IoU 和 F1 分别提高了 1.91% 至 24.42% 和 1.26% 至 18.54%。这是因为 SCDUNet++ 能够提取更全面的滑坡多尺度特征、空间特征和光谱特征。这样可以准确提取更多小型滑坡和更好的边界。此外，它可以更准确地区分山体滑坡和具有相似特征的物体，例如泥泞的河流、岩石和山路。<br>(3)实验中使用的所有模型在多通道数据上都取得了良好的 LM 结果。但考虑到防灾减灾的需要，建议使用 STUNet、UNet++ 和 SCDUNet++，因为它们可以实现更高的召回率。但是，STUNet 的训练和测试成本较高，而 SCDUNet++ 在全面性方面是最佳选择。<br>(4)当将训练后的模型应用于其他数据匮乏的区域时，与直接预测相比，可以以较低的成本对模型进行微调，以实现显著的性能改进。DTL 后的 8 款车型的 IoU 提高了 8.33% 至 27.5%，F1 的 IoU 提高了 6.58% 至 23.67%。此外，借助可不断扩展的源域数据和训练后的模型，可以在地震后获取目标区域的数据后快速生成目标区域的滑坡清单图。这具有很高的实际应用价值。</p>]]></content>
    
    
    
    <tags>
      
      <tag>文献</tag>
      
      <tag>滑坡</tag>
      
      <tag>迁移学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>算法课笔记</title>
    <link href="/2024/11/16/%E7%AE%97%E6%B3%95%E8%AF%BE%E7%AC%94%E8%AE%B0/"/>
    <url>/2024/11/16/%E7%AE%97%E6%B3%95%E8%AF%BE%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<h1>第一章 算法</h1><p>算法是使用计算机求解一类问题的过程、步骤、方法</p><h2 id="计算机求解问题分类">计算机求解问题分类</h2><p>P问题(Polynomial)<br>能在多项式时间内求解的问题。<br>NP问题(Non-Deterministic Polynomial)<br>在多项式时间内可验证的问题。<br>NPC问题(Non-Deterministic Polynomial Complete)<br>1)是一个NP问题；2)所有NP问题都能规约到它。<br>NP-Hard问题(Non-deterministic Polynomial hard)<br>1)所有NP问题都能规约到它。</p><h2 id="算法的五个重要特性">算法的五个重要特性</h2><p>确定性、能行性、输入、输出、有穷性</p><h2 id="算法分类">算法分类</h2><p>多项式时间算法：可用多项式（函数）对其计算时间限界的算法。<br>常见的多项式限界函数有：<br>Ο(1) &lt; Ο(logn) &lt; Ο(n) &lt; Ο(nlogn) &lt; Ο(<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>n</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">n^{2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span>) &lt; Ο(<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>n</mi><mn>3</mn></msup></mrow><annotation encoding="application/x-tex">n^{3}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">3</span></span></span></span></span></span></span></span></span></span></span></span>)<br>指数时间算法：计算时间用指数函数限界的算法<br>常见的指数时间限界函数：<br>Ο(<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mn>2</mn><mi>n</mi></msup></mrow><annotation encoding="application/x-tex">2^{n}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6644em;"></span><span class="mord"><span class="mord">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6644em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span></span></span></span></span></span></span></span></span></span></span></span>) &lt; Ο(n！) &lt; Ο(<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>n</mi><mi>n</mi></msup></mrow><annotation encoding="application/x-tex">n^{n}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6644em;"></span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6644em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span></span></span></span></span></span></span></span></span></span></span></span>)</p><h2 id="分析方法">分析方法</h2><p>事前分析：理论分析得出算法特性——时间空间特征函数<br>事后测试：物理实现得到执行时间和空间占用资料<br><img src="/image/1124.png" alt=""></p><p>空间换时间、时间换空间思想：</p><h1>第二章 分治法</h1><p>适用类型：对大规模问题的求解</p><p>基本思想：分而治之再合并结果<br>在问题的输入规模很大时，无法直接求解，则采用将整个问题分成若干个小问题后分而治之</p><p>算法特点：子问题结构相似但不重叠</p><h2 id="证明基于比较的检索算法在最坏情况时间复杂度下界为Ω-logn">证明基于比较的检索算法在最坏情况时间复杂度下界为Ω(logn)</h2><p>以比较为基础的算法：假定算法中只允许进行元素间的比较，而不允许对它们实施其它运算<br>任何以比较为基础的检索算法，其执行过程都可以用<strong>二元比较树</strong>来描述。<br>从模拟求解检索问题算法的比较树可知，FIND(n)不大于树中由根到一个叶子的最长路经的距离。<br>而所有树中必定有n个内结点与x在A中的n种可能的出现相对应。<br>如果一棵二元树的所有内结点所在的级数小于或等于k，则最多有 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mn>2</mn><mi>k</mi></msup></mrow><annotation encoding="application/x-tex">2^{k}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8491em;"></span><span class="mord"><span class="mord">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span></span></span></span></span></span></span></span></span>-1个内结点。故n≤<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mn>2</mn><mi>k</mi></msup></mrow><annotation encoding="application/x-tex">2^{k}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8491em;"></span><span class="mord"><span class="mord">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span></span></span></span></span></span></span></span></span>-1,即FIND(n)=k &gt;= log(n+1)</p><h2 id="任何以关键字比较为基础的分类算法，其最坏情况下的时间下界都为：Ω-nlogn">任何以关键字比较为基础的分类算法，其最坏情况下的时间下界都为：Ω(nlogn)</h2><p>利用二元比较树证明<br>假设参加分类的n个关键字A(1),A(2),…,A(n)互异。任<br>意两个关键字的比较必导致A(i)&lt;A(j)或A(i)&gt;A(j)的结果。<br>以二元比较树描述元素间的比较过程：<br>若A(i)&lt;A(j)，进入下一级的左分支<br>若A(i)&gt;A(j)，进入下一级的右分支<br>算法在外部结点终止。<br>从根到某外结点的路径代表某个特定输入情况下一种唯一的分类排序序列。路径长度表示生成该序列代表的分类表所需要的比较次数。而最长的路径代表算法在最坏情况下的执行情况，该路径的长度即是算法在最坏情况下所作的比较数。<br>故，以比较为基础的分类算法的最坏情况下界等于该算法对应的比较树的最小高度。<br>① 由于n个关键字有n!种可能的排列，所以二元比较树中将有n!个外部结点：每种排列对应于某种特定输入情况下的分类情况，每个外部结点表示一种可能的分类序列。<br>② 设一棵二元比较树的所有内结点的级数均小于或等于k，则该树中最多有 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mn>2</mn><mi>k</mi></msup></mrow><annotation encoding="application/x-tex">2^k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8491em;"></span><span class="mord"><span class="mord">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span></span></span></span></span></span></span></span> 个外结点。记算法在最坏情况下所作的比较次数为T(n)，则有T(n)=k, k为确定一种排列所需的比较次数；<br>根据①和②的分析，有：<br>n!≤<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mn>2</mn><mrow><mi>T</mi><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">2^{T(n)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.888em;"></span><span class="mord"><span class="mord">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.888em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span><span class="mopen mtight">(</span><span class="mord mathnormal mtight">n</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span></span></span></span><br>取对数，化简</p><p>二分检索、快排（算法的分析方法）为什么比冒泡法快？这些东西是怎么分析出来的<br>快速分类算法最坏情况下O(<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>n</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">n^{2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span>)，平均情况下O(nlogn)</p><h1>第三章 贪心法</h1><p>适用类型：问题的解是由n个输入的、满足某些事先给定的条件的子集组成</p><p>核心思想：只走一次，每一步都选择当前对自己最优的决策</p><p>算法特点：最优子结构、无后效性、贪心选择性（通过局部最优的选择能产生全局最优）</p><p>关键：选取能够得到问题最优解的量度标准</p><h2 id="证明贪心解即是问题的最优解">证明贪心解即是问题的最优解</h2><p><img src="/image/105058.png" alt=""><br><img src="/image/105309.png" alt=""><br><img src="/image/105423.png" alt=""></p><h2 id="Strassen矩阵算法">Strassen矩阵算法</h2><h1>第四章 动态规划</h1><p>适用类型：多阶段决策问题、满足最优性原理<br>多阶段决策最优解模型</p><p>核心思想：将问题分解成多个阶段，记录各阶段可达的状态集合，通过当前阶段的状态集合推导下一阶段的状态集合，动态向前推进，空间换时间</p><p>前提：1) 证明问题满足最优性原理；2) 获得问题状态的递推关系式</p><p>算法特点：最优子结构、无后效性、重复子问题</p><p>原理：通过把原问题分解成相对简单的子问题的方式来解决复杂问题的方法。它的基本思想是将待求解问题分解成不同部分（即子问题），然后依据子问题的解以得出原问题的解，而子问题又可递归地分解为子子问题（到此跟分治法的思想类似，后面是不同）。通常许多子问题可能会重复出现(重复子问题)，DP试图仅仅解每个子问题一次，在求得每个子问题的解后将其保存起来，下次再需要求解相同子问题时，直接查表得到,从而减少计算量，它的精髓在于记住求过的解来节省时间，体现了以空间换时间的算法思想，这也是其与分治法最大的区别</p><p>优缺点：避免了重复计算、解决许多问题并有效优化计算时间和空间、可以求解最优策略问题；设计复杂，占用空间、无法拓展</p><h2 id="多段图">多段图</h2><h2 id="01背包问题">01背包问题</h2><h2 id="TSP问题">TSP问题</h2><h2 id="最优二分检索树">最优二分检索树</h2><h2 id="Dijkstra算法">Dijkstra算法</h2><h1>第五章 检索</h1><h1>其他</h1><h2 id="回溯法">回溯法</h2><p>适用类型： 求解n元组问题<br>适用范围很广但时间复杂度是指数级的<br>(当问题是要求满足某种性质（约束条件）的所有解或最优解时)</p><p>思想：使用限界函数的深度优先结点生成方法<br>（把问题的解空间转化成了图或者树的结构表示，然后使用深度优先搜索策略进行遍历，遍历的过程中记录和寻找所有可行解或者最优解）<br>（深度优先+限界函数）</p><p>算法特点：适用广，复杂度高</p><p>用回溯法做n-皇后问题和TSP问题和01背包问题</p><h2 id="分枝界限法">分枝界限法</h2><p>适用类型:<br>思想：生成当前E-结点的所有孩子节点后生成其他活结点的孩子节点，用限界函数帮助避免生成不包含答案结点子树的状态空间的检索方法<br>（广度优先+限界函数+least cost策略）</p><p>用分枝限界法做01背包问题</p><p>与回溯法区别：<br>相同点<br>都是建立解空间树来求解<br>都需要设置限界函数<br>不同点<br>回溯法是用深度优先方式进行搜索，但分支限界法是通过广度优先进行搜索<br>在回溯法中，每个结点可以有1次及以上的机会成为活结点，但是在分支限界法中只有一次机会成为活结点<br>回溯法的求解目标是找出解空间树中满足约束条件的所有解，而分支限界法的求解目标则是找出满足约束条件的一个解，或是在满足约束条件的解中找出在某种意义下的最优解</p>]]></content>
    
    
    
    <tags>
      
      <tag>算法</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>滑坡实验|特征因子选择</title>
    <link href="/2024/11/12/%E6%BB%91%E5%9D%A1%E5%AE%9E%E9%AA%8C%E7%89%B9%E5%BE%81%E5%9B%A0%E5%AD%90%E9%80%89%E6%8B%A9/"/>
    <url>/2024/11/12/%E6%BB%91%E5%9D%A1%E5%AE%9E%E9%AA%8C%E7%89%B9%E5%BE%81%E5%9B%A0%E5%AD%90%E9%80%89%E6%8B%A9/</url>
    
    <content type="html"><![CDATA[<h1>VIF</h1><p><a href="https://mp.weixin.qq.com/s/CnJiMmO2DzkX6E2E2rnKwQ">去除多重共线性的5种方法</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">from</span> statsmodels.regression.linear_model <span class="hljs-keyword">import</span> OLS<br><span class="hljs-keyword">import</span> statsmodels.api <span class="hljs-keyword">as</span> sm<br><span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LinearRegression<br><span class="hljs-keyword">from</span> statsmodels.stats.outliers_influence <span class="hljs-keyword">import</span> variance_inflation_factor<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">from</span> math <span class="hljs-keyword">import</span> pi<br><br><span class="hljs-comment"># 使用 statsmodels 计算 VIF，基于相关系数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">variance_inflation_factor_sm_corr</span>(<span class="hljs-params">exog, exog_idx</span>):<br>    k_vars = exog.shape[<span class="hljs-number">1</span>]  <span class="hljs-comment"># 特征总数</span><br>    exog = np.asarray(exog)<br>    x_i = exog[:, exog_idx]  <span class="hljs-comment"># 当前特征</span><br>    mask = np.arange(k_vars) != exog_idx  <span class="hljs-comment"># 掩码，选择非当前特征</span><br>    x_noti = exog[:, mask]  <span class="hljs-comment"># 其他特征</span><br>    r_squared_i = OLS(x_i, sm.add_constant(x_noti)).fit().rsquared  <span class="hljs-comment"># R²</span><br>    vif = <span class="hljs-number">1.</span> / (<span class="hljs-number">1.</span> - r_squared_i)  <span class="hljs-comment"># VIF计算</span><br>    <span class="hljs-keyword">return</span> vif<br><br><br><span class="hljs-comment"># 使用 sklearn 计算 VIF</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">variance_inflation_factor_sklearn</span>(<span class="hljs-params">exog, exog_idx</span>):<br>    k_vars = exog.shape[<span class="hljs-number">1</span>]<br>    exog = np.asarray(exog)<br>    x_i = exog[:, exog_idx]<br>    mask = np.arange(k_vars) != exog_idx<br>    x_noti = exog[:, mask]<br>    r_squared_i = LinearRegression(fit_intercept=<span class="hljs-literal">True</span>).fit(x_noti, x_i).score(x_noti, x_i)<br>    vif = <span class="hljs-number">1.</span> / (<span class="hljs-number">1.</span> - r_squared_i)<br>    <span class="hljs-keyword">return</span> vif<br><br><br><span class="hljs-comment"># 计算数据集的 VIF，支持多种计算方法</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">calc_vif</span>(<span class="hljs-params">df, method=<span class="hljs-string">&#x27;all&#x27;</span></span>):<br>    vif_data = pd.DataFrame()  <span class="hljs-comment"># 保存 VIF 数据</span><br>    vif_data[<span class="hljs-string">&quot;feature&quot;</span>] = df.columns<br><br>    <span class="hljs-keyword">if</span> method == <span class="hljs-string">&#x27;sm_corr&#x27;</span>:  <span class="hljs-comment"># 使用 statsmodels 相关系数方法</span><br>        vif_data[<span class="hljs-string">&quot;VIF&quot;</span>] = [variance_inflation_factor_sm_corr(df.values, i)<br>                           <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(df.columns))]<br>    <span class="hljs-keyword">elif</span> method == <span class="hljs-string">&#x27;sklearn&#x27;</span>:  <span class="hljs-comment"># 使用 sklearn</span><br>        vif_data[<span class="hljs-string">&quot;VIF&quot;</span>] = [variance_inflation_factor_sklearn(df.values, i)<br>                           <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(df.columns))]<br>    <span class="hljs-keyword">elif</span> method == <span class="hljs-string">&#x27;sm&#x27;</span>:  <span class="hljs-comment"># 使用 statsmodels OLS</span><br>        vif_data[<span class="hljs-string">&quot;VIF&quot;</span>] = [variance_inflation_factor(df.values, i)<br>                           <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(df.columns))]<br>    <span class="hljs-keyword">else</span>:  <span class="hljs-comment"># 默认计算所有方法的 VIF</span><br>        vif_data[<span class="hljs-string">&quot;VIF_sm_corr&quot;</span>] = [variance_inflation_factor_sm_corr(df.values, i)<br>                                   <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(df.columns))]<br>        vif_data[<span class="hljs-string">&quot;VIF_sklearn&quot;</span>] = [variance_inflation_factor_sklearn(df.values, i)<br>                                   <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(df.columns))]<br>        vif_data[<span class="hljs-string">&quot;VIF_sm&quot;</span>] = [variance_inflation_factor(df.values, i)<br>                              <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(df.columns))]<br><br>    <span class="hljs-keyword">return</span> vif_data<br><br><br><span class="hljs-comment"># 读取数据</span><br>df1 = pd.read_excel(<span class="hljs-string">&#x27;cluster0.xlsx&#x27;</span>)<br>df2 = pd.read_excel(<span class="hljs-string">&#x27;cluster1.xlsx&#x27;</span>)<br><br><br>df = [df1, df2]<br>i = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> df <span class="hljs-keyword">in</span> df:<br>    num_df = df.drop(columns=[<span class="hljs-string">&#x27;class&#x27;</span>, <span class="hljs-string">&#x27;cluster&#x27;</span>, <span class="hljs-string">&#x27;X&#x27;</span>, <span class="hljs-string">&#x27;Y&#x27;</span>])<br>    result = calc_vif(num_df, method=<span class="hljs-string">&#x27;all&#x27;</span>)<br>    <span class="hljs-built_in">print</span>(result)<br><br><br>    <span class="hljs-comment"># 假设 VIF 结果已保存在 result DataFrame 中，选择 &quot;feature&quot; 和一个 VIF 列（如 &quot;VIF_sm_corr&quot;）用于绘制</span><br>    vif_data = result[[<span class="hljs-string">&#x27;feature&#x27;</span>, <span class="hljs-string">&#x27;VIF_sm_corr&#x27;</span>]]<br><br>    <span class="hljs-comment"># 准备数据</span><br>    labels = vif_data[<span class="hljs-string">&#x27;feature&#x27;</span>].values  <span class="hljs-comment"># 特征名称</span><br>    stats = vif_data[<span class="hljs-string">&#x27;VIF_sm_corr&#x27;</span>].values  <span class="hljs-comment"># VIF 值</span><br><br>    <span class="hljs-comment"># 雷达图设置</span><br>    num_vars = <span class="hljs-built_in">len</span>(labels)<br><br>    <span class="hljs-comment"># 计算每个特征的角度</span><br>    angles = np.linspace(<span class="hljs-number">0</span>, <span class="hljs-number">2</span> * np.pi, num_vars, endpoint=<span class="hljs-literal">False</span>).tolist()<br><br>    <span class="hljs-comment"># 为了使雷达图闭合，连接起点</span><br>    stats = np.concatenate((stats, [stats[<span class="hljs-number">0</span>]]))<br>    angles += angles[:<span class="hljs-number">1</span>]<br><br>    <span class="hljs-comment"># 创建雷达图</span><br>    fig, ax = plt.subplots(figsize=(<span class="hljs-number">8</span>, <span class="hljs-number">8</span>), subplot_kw=<span class="hljs-built_in">dict</span>(polar=<span class="hljs-literal">True</span>))<br><br>    <span class="hljs-comment"># 绘制填充区域和边界线</span><br>    ax.fill(angles, stats, color=<span class="hljs-string">&#x27;red&#x27;</span>, alpha=<span class="hljs-number">0.25</span>)<br>    ax.plot(angles, stats, color=<span class="hljs-string">&#x27;red&#x27;</span>, linewidth=<span class="hljs-number">2</span>)<br><br>    <span class="hljs-comment"># 设置特征名称（每个轴的标签）</span><br>    ax.set_xticks(angles[:-<span class="hljs-number">1</span>])<br>    ax.set_xticklabels(labels, fontsize=<span class="hljs-number">12</span>)<br><br>    <span class="hljs-comment"># 设置径向坐标的标签（显示每个VIF值的大小）</span><br>    ax.set_yticks(<span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-built_in">int</span>(<span class="hljs-built_in">max</span>(stats)) + <span class="hljs-number">2</span>))<br>    ax.set_yticklabels(<span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-built_in">int</span>(<span class="hljs-built_in">max</span>(stats)) + <span class="hljs-number">2</span>), color=<span class="hljs-string">&quot;grey&quot;</span>, size=<span class="hljs-number">10</span>)<br><br>    <span class="hljs-comment"># 添加标题</span><br>    plt.title(<span class="hljs-string">f&#x27;VIF cluster<span class="hljs-subst">&#123;i&#125;</span>&#x27;</span>, size=<span class="hljs-number">15</span>, color=<span class="hljs-string">&#x27;black&#x27;</span>, y=<span class="hljs-number">1.1</span>)<br>    i +=<span class="hljs-number">1</span><br><span class="hljs-comment"># 显示图形</span><br>plt.show()<br><br></code></pre></td></tr></table></figure><h1>Boruta算法</h1><p><a href="https://mp.weixin.qq.com/s/UktBNhErmS4HdfKsEDYcww">机器学习|特征选择利器：森林之神(Boruta)</a><br><a href="https://mp.weixin.qq.com/s/wX1B_2wxp3kUVjQKQE5XLw">特征选择：基于随机森林的Boruta算法应用</a><br><a href="https://mp.weixin.qq.com/s/rp1qDlACzvvtdQbdEL6Lbg">特征选择：Lasso和Boruta算法的结合应用</a></p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br></pre></td><td class="code"><pre><code class="hljs routeros">import seaborn as sns<br>import pandas as pd<br>import numpy as np<br>import matplotlib.pyplot as plt<br><span class="hljs-keyword">from</span> sklearn.model_selection import train_test_split<br><br><br>df1 = pd.read_excel(<span class="hljs-string">&#x27;cluster1.xlsx&#x27;</span>)<br>df0 = pd.read_excel(<span class="hljs-string">&#x27;cluster0.xlsx&#x27;</span>)<br>df = [df0 , df1]<br><span class="hljs-attribute">j</span>=0<br><span class="hljs-keyword">for</span> df <span class="hljs-keyword">in</span> df:<br>    # 划分特征和目标变量<br>    X = df.drop([<span class="hljs-string">&#x27;class&#x27;</span>, <span class="hljs-string">&#x27;cluster&#x27;</span>, <span class="hljs-string">&#x27;X&#x27;</span>, <span class="hljs-string">&#x27;Y&#x27;</span>], <span class="hljs-attribute">axis</span>=1)<br>    y = df[<span class="hljs-string">&#x27;class&#x27;</span>]<br>    # 划分训练集和测试集<br>    X_train, X_test, y_train, y_test = train_test_split(X, y, <span class="hljs-attribute">test_size</span>=0.2,<br>                                                        <span class="hljs-attribute">random_state</span>=42, <span class="hljs-attribute">stratify</span>=df[<span class="hljs-string">&#x27;class&#x27;</span>])<br>    df.head()<br><br><br>    <span class="hljs-keyword">from</span> sklearn.ensemble import RandomForestClassifier<br>    <span class="hljs-keyword">from</span> boruta import BorutaPy<br>    <span class="hljs-string">&#x27;&#x27;</span><span class="hljs-string">&#x27;</span><br><span class="hljs-string">    多次运行Boruta算法以评估特征排名稳定性</span><br><span class="hljs-string">    &#x27;</span><span class="hljs-string">&#x27;&#x27;</span><br>    # 初始化随机森林模型<br>    rf = RandomForestClassifier(<span class="hljs-attribute">n_jobs</span>=-1, <span class="hljs-attribute">class_weight</span>=<span class="hljs-string">&#x27;balanced&#x27;</span>, <span class="hljs-attribute">max_depth</span>=5)<br><br>    # 初始化存储特征排名的 DataFrame<br>    ranking_df = pd.DataFrame(<span class="hljs-attribute">index</span>=range(1, 2), <span class="hljs-attribute">columns</span>=X_train.columns)<br>    support_df = pd.DataFrame(<span class="hljs-attribute">index</span>=range(1, 2), <span class="hljs-attribute">columns</span>=X_train.columns)<br>    # 运行 Boruta 20 次<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(1):<br>        # <span class="hljs-built_in">print</span>(f<span class="hljs-string">&quot;Iteration &#123;i + 1&#125;&quot;</span>)<br><br>        # 初始化Boruta特征选择器<br>        boruta_selector = BorutaPy(rf, <span class="hljs-attribute">n_estimators</span>=<span class="hljs-string">&#x27;auto&#x27;</span>, <span class="hljs-attribute">verbose</span>=2, <span class="hljs-attribute">random_state</span>=i, <span class="hljs-attribute">max_iter</span>=50)<br><br>        # 对训练数据进行特征选择<br>        boruta_selector.fit(X_train.values, y_train.values)<br><br>        # 获取特征排名<br>        feature_ranks = boruta_selector.ranking_<br>        feature_support = boruta_selector.support_<br><br>        # 将特征排名保存到 DataFrame 中<br>        ranking_df.loc[i + 1] = feature_ranks<br>        support_df.loc[i + 1] = feature_support<br>    ranking_df.to_csv(f<span class="hljs-string">&#x27;cluster-&#123;i&#125;-boruta-feature-ranking.csv&#x27;</span>, <span class="hljs-attribute">index</span>=<span class="hljs-literal">False</span>)<br>    support_df.to_csv(f<span class="hljs-string">&#x27;cluster-&#123;i&#125;-boruta-feature-support.csv&#x27;</span>, <span class="hljs-attribute">index</span>=<span class="hljs-literal">False</span>)<br>    #水平柱状图#<br><br>    numeric_ranking_df = ranking_df.apply(pd.to_numeric, <span class="hljs-attribute">errors</span>=<span class="hljs-string">&#x27;coerce&#x27;</span>)<br><br>    # 计算每个特征的中位数<br>    median_values = 1/numeric_ranking_df.median()<br><br>    # 根据中位数对特征进行排序<br>    sorted_median_values = median_values.sort_values(<span class="hljs-attribute">ascending</span>=<span class="hljs-literal">False</span>)  # 值越大，特征越重要<br><br>    # 创建水平柱状图<br>    plt.figure(figsize=(6, 5))<br>    sns.barplot(<span class="hljs-attribute">x</span>=sorted_median_values.values, <span class="hljs-attribute">y</span>=sorted_median_values.index, <span class="hljs-attribute">palette</span>=<span class="hljs-string">&quot;viridis&quot;</span>)<br><br>    # 设置图表标题和标签<br>    plt.title(f<span class="hljs-string">&quot;Cluster &#123;j&#125; Feature Importance by Boruta (Horizontal Bar Chart)&quot;</span>, <span class="hljs-attribute">fontsize</span>=10)<br>    plt.xlabel(<span class="hljs-string">&quot;Feature Importance (1 / Median Ranking))&quot;</span>, <span class="hljs-attribute">fontsize</span>=10)<br>    plt.ylabel(<span class="hljs-string">&quot;Features&quot;</span>, <span class="hljs-attribute">fontsize</span>=10)<br>    j = j+1<br><br><br><span class="hljs-comment"># 显示图表</span><br>plt.tight_layout()<br>plt.show()<br><br><span class="hljs-string">&#x27;&#x27;</span><span class="hljs-string">&#x27;</span><br><span class="hljs-string">可视化排名稳定性</span><br><span class="hljs-string">&#x27;</span><span class="hljs-string">&#x27;&#x27;</span><br><span class="hljs-comment"># # 确保数据集中只有数值列</span><br><span class="hljs-comment"># numeric_ranking_df = ranking_df.apply(pd.to_numeric, errors=&#x27;coerce&#x27;)</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment"># # 计算每个特征的中位数</span><br><span class="hljs-comment"># median_values = numeric_ranking_df.median()</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment"># # 根据中位数对列进行排序</span><br><span class="hljs-comment"># sorted_columns = median_values.sort_values().index</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment"># # 设置绘图风格</span><br><span class="hljs-comment"># plt.figure(figsize=(15, 8))</span><br><span class="hljs-comment"># sns.set(style=&quot;whitegrid&quot;)</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment"># # 绘制箱线图</span><br><span class="hljs-comment"># sns.boxplot(data=numeric_ranking_df[sorted_columns], palette=&quot;Greens&quot;)</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment"># plt.xticks(rotation=90)</span><br><span class="hljs-comment"># plt.title(&quot;Sorted Feature Ranking Distribution by Boruta&quot;, fontsize=16)</span><br><span class="hljs-comment"># plt.xlabel(&quot;Attributes&quot;, fontsize=14)</span><br><span class="hljs-comment"># plt.ylabel(&quot;Importance&quot;, fontsize=14)</span><br><span class="hljs-comment"># plt.tight_layout()</span><br><span class="hljs-comment"># plt.show()</span><br></code></pre></td></tr></table></figure>]]></content>
    
    
    
    <tags>
      
      <tag>代码</tag>
      
      <tag>算法</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>机器学习|融合模型</title>
    <link href="/2024/11/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%9E%8D%E5%90%88%E6%A8%A1%E5%9E%8B/"/>
    <url>/2024/11/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%9E%8D%E5%90%88%E6%A8%A1%E5%9E%8B/</url>
    
    <content type="html"><![CDATA[<p><a href="https://mp.weixin.qq.com/s?__biz=MzU2MjczMTU0OA==&amp;mid=2247486230&amp;idx=1&amp;sn=9955e586d7dca0c6bbda12d2784e32fa&amp;chksm=fc644465cb13cd73e347650c1b8643760d9c605215a633f6d357a02240f2777e7ec36429a159&amp;token=900428772&amp;lang=zh_CN&amp;scene=21#wechat_redirect">模型竞赛大杀器-融合模型(stacking)</a><br><a href="https://mp.weixin.qq.com/s/QmU7LAFub8BGB3zUNJAo8A">机器学习融合模型stacking14条经验总结和5个成功案例(互联网最全，硬核收藏)</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>文献笔记|基于栅格单元的区域滑坡易发性评价关键影响因素研究</title>
    <link href="/2024/11/07/%E6%96%87%E7%8C%AE%E7%AC%94%E8%AE%B0%E5%9F%BA%E4%BA%8E%E6%A0%85%E6%A0%BC%E5%8D%95%E5%85%83%E7%9A%84%E5%8C%BA%E5%9F%9F%E6%BB%91%E5%9D%A1%E6%98%93%E5%8F%91%E6%80%A7%E8%AF%84%E4%BB%B7%E5%85%B3%E9%94%AE%E5%BD%B1%E5%93%8D%E5%9B%A0%E7%B4%A0%E7%A0%94%E7%A9%B6/"/>
    <url>/2024/11/07/%E6%96%87%E7%8C%AE%E7%AC%94%E8%AE%B0%E5%9F%BA%E4%BA%8E%E6%A0%85%E6%A0%BC%E5%8D%95%E5%85%83%E7%9A%84%E5%8C%BA%E5%9F%9F%E6%BB%91%E5%9D%A1%E6%98%93%E5%8F%91%E6%80%A7%E8%AF%84%E4%BB%B7%E5%85%B3%E9%94%AE%E5%BD%B1%E5%93%8D%E5%9B%A0%E7%B4%A0%E7%A0%94%E7%A9%B6/</url>
    
    <content type="html"><![CDATA[<p><a href="https://mp.weixin.qq.com/s/jmdc68hLV6WO4mGBj683AA">推荐链接</a><br><a href="https://www.sciencedirect.com/science/article/pii/S1674987124000069">原文链接</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>文献</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>滑坡实验|聚类</title>
    <link href="/2024/11/06/%E6%BB%91%E5%9D%A1%E5%AE%9E%E9%AA%8C%E8%81%9A%E7%B1%BB/"/>
    <url>/2024/11/06/%E6%BB%91%E5%9D%A1%E5%AE%9E%E9%AA%8C%E8%81%9A%E7%B1%BB/</url>
    
    <content type="html"><![CDATA[<h2 id="基础知识">基础知识</h2><p><strong>聚类</strong><br><a href="https://mp.weixin.qq.com/s/o09VSfP07Y0uymGKzYUQIw">7种常见的聚类算法原理及代码实现</a></p><p><strong>空间聚类</strong><br><a href="https://mp.weixin.qq.com/s/XsKeP1etV_void_bociznw">【方法解读】Nat Methods：如何选择最佳的空间聚类方法</a><br><a href="https://zhuanlan.zhihu.com/p/401518862">新版白话空间统计（39）：空间聚类的特点与分类</a><br>经纬度数值越大权重越大————&gt;<strong>归一化</strong>（Normalization：数据变成（0，1）或者之间的小数）/<strong>标准化</strong>（Standardization：使每个特征中的数值平均变为0、标准差变为1）</p><p>数据归一化</p><h2 id="实例">实例</h2><p><a href="https://mp.weixin.qq.com/s/-ikGHsL0G0LWQrRe3w2qMw">Plotly+Pandas+Sklearn：实现用户聚类分群！</a><br><a href="https://mp.weixin.qq.com/s/s6147o2ZlPrctM0KcNIjlQ">顶刊论文复现：一种网络视角下的空间聚类算法的实现与应用-以城市道路分区为例</a></p><h2 id="其他">其他</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">from</span> sklearn.cluster <span class="hljs-keyword">import</span> KMeans, AgglomerativeClustering, DBSCAN<br><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> calinski_harabasz_score<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> StandardScaler<br><br><br><span class="hljs-comment"># 加载数据</span><br>df = pd.read_excel(<span class="hljs-string">&quot;yibingforagnes.xlsx&quot;</span>)  <span class="hljs-comment"># 替换为实际数据路径</span><br><span class="hljs-comment"># 删除或填充整个data中的NaN值（以均值填充为例）</span><br>df = df.fillna(df.mean())<br><span class="hljs-comment"># 定义features</span><br>data = df.drop(columns=[<span class="hljs-string">&#x27;class&#x27;</span>]).values<br><span class="hljs-comment"># data = df.drop(columns=[&#x27;class&#x27;, &#x27;X&#x27;, &#x27;Y&#x27;]).values</span><br><span class="hljs-comment"># 标准化数据</span><br>scaler = StandardScaler()<br>data = scaler.fit_transform(data)<br><span class="hljs-comment"># 定义簇数范围</span><br>n_clusters_range = <span class="hljs-built_in">range</span>(<span class="hljs-number">2</span>, <span class="hljs-number">15</span>)<br><br><span class="hljs-comment"># 存储各簇数下的CH指数</span><br>ch_scores_kmeans = []<br>ch_scores_agg = []<br><br><span class="hljs-comment"># 距离度量方法列表</span><br>linkage_methods = [<span class="hljs-string">&#x27;single&#x27;</span>, <span class="hljs-string">&#x27;complete&#x27;</span>, <span class="hljs-string">&#x27;average&#x27;</span>]<br>ch_scores = &#123;method: [] <span class="hljs-keyword">for</span> method <span class="hljs-keyword">in</span> linkage_methods&#125;<br><br><span class="hljs-comment"># 计算不同距离度量方法下的CH指数</span><br>cluster_range = <span class="hljs-built_in">range</span>(<span class="hljs-number">2</span>, <span class="hljs-number">15</span>)<br><span class="hljs-keyword">for</span> method <span class="hljs-keyword">in</span> linkage_methods:<br>    <span class="hljs-keyword">for</span> n_clusters <span class="hljs-keyword">in</span> cluster_range:<br>        agnes = AgglomerativeClustering(n_clusters=n_clusters, linkage=method)<br>        labels = agnes.fit_predict(data)<br>        score = calinski_harabasz_score(data, labels)<br>        ch_scores[method].append(score)<br><span class="hljs-comment"># K-Means聚类</span><br><span class="hljs-keyword">for</span> n_clusters <span class="hljs-keyword">in</span> n_clusters_range:<br>    kmeans = KMeans(n_clusters=n_clusters, random_state=<span class="hljs-number">0</span>)<br>    labels = kmeans.fit_predict(data)<br>    ch_score = calinski_harabasz_score(data, labels)<br>    ch_scores_kmeans.append(ch_score)<br><br><span class="hljs-comment"># 层次聚类</span><br><span class="hljs-keyword">for</span> n_clusters <span class="hljs-keyword">in</span> n_clusters_range:<br>    agg = AgglomerativeClustering(n_clusters=n_clusters)<br>    labels = agg.fit_predict(data)<br>    ch_score = calinski_harabasz_score(data, labels)<br>    ch_scores_agg.append(ch_score)<br><br><span class="hljs-comment"># 绘制CH指数图</span><br>plt.figure(figsize=(<span class="hljs-number">10</span>, <span class="hljs-number">6</span>))<br>plt.plot(n_clusters_range, ch_scores_kmeans, label=<span class="hljs-string">&#x27;K-Means&#x27;</span>, marker=<span class="hljs-string">&#x27;o&#x27;</span>)<br>plt.plot(n_clusters_range, ch_scores_agg, label=<span class="hljs-string">&#x27;Agglomerative Clustering&#x27;</span>, marker=<span class="hljs-string">&#x27;s&#x27;</span>)<br><span class="hljs-keyword">for</span> method, scores <span class="hljs-keyword">in</span> ch_scores.items():<br>    plt.plot(cluster_range, scores, marker=<span class="hljs-string">&#x27;o&#x27;</span>, label=<span class="hljs-string">f&#x27;Distance: <span class="hljs-subst">&#123;method&#125;</span>&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;Number of Clusters&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Calinski-Harabasz Index&#x27;</span>)<br>plt.title(<span class="hljs-string">&#x27;CH Index for Different Clustering Methods&#x27;</span>)<br>plt.legend()<br><span class="hljs-comment"># plt.show()</span><br><br><br>n_clusters = <span class="hljs-number">2</span><br>alg = KMeans(n_clusters=n_clusters, random_state=<span class="hljs-number">0</span>)<br><span class="hljs-comment"># alg = AgglomerativeClustering(n_clusters=n_clusters)</span><br>df[<span class="hljs-string">&#x27;cluster&#x27;</span>] = alg.fit_predict(data)<br><br><span class="hljs-comment"># 导出含有聚类标签的 Excel 文件</span><br>output_file = <span class="hljs-string">&#x27;clusters.xlsx&#x27;</span><br><span class="hljs-keyword">with</span> pd.ExcelWriter(output_file) <span class="hljs-keyword">as</span> writer:<br>    df.to_excel(writer, sheet_name=<span class="hljs-string">&#x27;Clustered_Data&#x27;</span>, index=<span class="hljs-literal">False</span>)<br><br><span class="hljs-comment"># 绘制聚类图</span><br>plt.figure(figsize=(<span class="hljs-number">10</span>, <span class="hljs-number">8</span>))<br><span class="hljs-keyword">for</span> cluster_id <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n_clusters):<br>    cluster_data = df[df[<span class="hljs-string">&#x27;cluster&#x27;</span>] == cluster_id]<br>    plt.scatter(cluster_data[<span class="hljs-string">&#x27;X&#x27;</span>], cluster_data[<span class="hljs-string">&#x27;Y&#x27;</span>], label=<span class="hljs-string">f&#x27;Cluster <span class="hljs-subst">&#123;cluster_id&#125;</span>&#x27;</span>, s=<span class="hljs-number">50</span>)<br><br><span class="hljs-comment"># 添加图例和标题</span><br>plt.xlabel(<span class="hljs-string">&#x27;X Coordinate&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Y Coordinate&#x27;</span>)<br>plt.title(<span class="hljs-string">&#x27;Geographic Points with Cluster Labels&#x27;</span>)<br>plt.legend()<br>plt.grid(<span class="hljs-literal">True</span>)<br>plt.show()<br><br><br></code></pre></td></tr></table></figure>]]></content>
    
    
    
    <tags>
      
      <tag>代码</tag>
      
      <tag>算法</tag>
      
      <tag>聚类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ArcGIS|gis距离分析和地貌因子计算</title>
    <link href="/2024/11/05/ArcGISgis%E8%B7%9D%E7%A6%BB%E5%88%86%E6%9E%90/"/>
    <url>/2024/11/05/ArcGISgis%E8%B7%9D%E7%A6%BB%E5%88%86%E6%9E%90/</url>
    
    <content type="html"><![CDATA[<h1>道路/河流/断层 POI</h1><h2 id="计算欧式距离时，单位问题">计算欧式距离时，单位问题</h2><p>将输入栅格数据（通常来自osm）坐标系转化<br><a href="https://www.cnblogs.com/bltstop/p/18090135">osm数据说明</a><br>ArcToolbox-数据管理工具-投影与变换-要素-投影<br><a href="https://blog.csdn.net/weixin_45481399/article/details/142679030">解决方法</a></p><h2 id="距离计算">距离计算</h2><p>空间分析工具-距离-欧式距离<br>可在环境中选择计算范围<br><a href="https://blog.csdn.net/weixin_45481399/article/details/142685036"></a></p><h2 id="值提取至点，点要素图层属性表不显示值？">值提取至点，点要素图层属性表不显示值？</h2><p>用多值提取至点</p><p><a href="https://www.jianshu.com/p/3bca53f2334f">gmt文件转化为shp文件</a><br>ogr2ogr -lco encoding=UTF-8 -f “ESRI Shapefile” C:\temp\cnFaults.shp C:\temp\CN-faults-name.gmt</p><h1>地貌</h1><p><a href="https://developer.aliyun.com/article/1350615#:~:text=%E5%9C%B0%E5%BD%A2%E8%B5%B7%E4%BC%8F%E5%BA%A6%E3%80%81%E5%9C%B0%E5%BD%A2"></a></p>]]></content>
    
    
    
    <tags>
      
      <tag>gis</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>滑坡实验|stacking集成</title>
    <link href="/2024/11/03/%E6%BB%91%E5%9D%A1%E5%AE%9E%E9%AA%8Cstacking%E9%9B%86%E6%88%90/"/>
    <url>/2024/11/03/%E6%BB%91%E5%9D%A1%E5%AE%9E%E9%AA%8Cstacking%E9%9B%86%E6%88%90/</url>
    
    <content type="html"><![CDATA[<p><a href="https://blog.csdn.net/ueke1/article/details/137190677">代码基于博客</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> matplotlib <span class="hljs-keyword">as</span> mpl<br><br>mpl.rcParams[<span class="hljs-string">&#x27;font.sans-serif&#x27;</span>] = [<span class="hljs-string">u&#x27;simHei&#x27;</span>]<br><br><span class="hljs-keyword">import</span> time<br><br><span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> LabelEncoder<br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<br><span class="hljs-keyword">from</span> sklearn.neighbors <span class="hljs-keyword">import</span> KNeighborsClassifier<br><span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> RandomForestClassifier, GradientBoostingClassifier<br><span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LogisticRegression<br><span class="hljs-keyword">from</span> sklearn.pipeline <span class="hljs-keyword">import</span> Pipeline<br><span class="hljs-keyword">from</span> sklearn.svm <span class="hljs-keyword">import</span> SVC<br><span class="hljs-keyword">from</span> mlxtend.classifier <span class="hljs-keyword">import</span> StackingClassifier<br><span class="hljs-keyword">from</span> sklearn.neural_network <span class="hljs-keyword">import</span> MLPClassifier<br><br><br><span class="hljs-keyword">from</span> mlxtend.feature_selection <span class="hljs-keyword">import</span> ColumnSelector<br><br><span class="hljs-comment">###1、读取数据</span><br><span class="hljs-comment"># datas = pd.read_csv(&#x27;iris.data&#x27;, sep=&#x27;,&#x27;, header=None, names=[&#x27;X1&#x27;, &#x27;X2&#x27;, &#x27;X3&#x27;, &#x27;X4&#x27;, &#x27;Y&#x27;])</span><br><span class="hljs-comment"># print(datas.head())</span><br><span class="hljs-comment"># print(datas.info())</span><br><br><span class="hljs-comment">### 2、数据清洗</span><br><br><span class="hljs-comment">### 3、获取特征属性X和目标属性Y</span><br><span class="hljs-comment"># X = datas.iloc[:, :-1]</span><br><span class="hljs-comment"># Y = datas.iloc[:, -1]</span><br><span class="hljs-comment"># print(X.shape)</span><br><span class="hljs-comment"># print(Y.shape)</span><br><span class="hljs-comment"># print(Y.value_counts())  ##看下目标属性的值</span><br><br>df_X = pd.read_excel(<span class="hljs-string">&#x27;DataX.xlsx&#x27;</span>)<br>feature_names = df_X.columns.tolist()  <span class="hljs-comment"># 将文件列名转换为Python列表</span><br><br>X = df_X.values  <span class="hljs-comment"># 输入特征，返回给定DataFrame的Numpy表现形式</span><br><br>Y = pd.DataFrame(pd.read_excel(<span class="hljs-string">&#x27;DataY.xlsx&#x27;</span>)).values.ravel()  <span class="hljs-comment"># 目标变量</span><br><br><span class="hljs-comment"># LabelEncoder  0,1, 2</span><br>labelencoder = LabelEncoder()<br><span class="hljs-comment"># print(Y.ravel())</span><br>Y = labelencoder.fit_transform(Y)<br><span class="hljs-comment"># print(Y)</span><br><br><span class="hljs-comment">### 4、分割数据集</span><br>x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=<span class="hljs-number">0.2</span>, random_state=<span class="hljs-number">28</span>)<br><span class="hljs-comment"># print(x_train.shape)</span><br><span class="hljs-comment"># print(y_train.shape)</span><br><br><span class="hljs-comment">### 5、特征工程</span><br><br><span class="hljs-comment">### 模型构建</span><br><span class="hljs-comment"># a、构造基学习器 knn、RF、softmax、GBDT。。。。</span><br>knn = KNeighborsClassifier(n_neighbors=<span class="hljs-number">7</span>)<br><span class="hljs-comment"># softmax = LogisticRegression(C=0.1, solver=&#x27;lbfgs&#x27;, multi_class=&#x27;multinomial&#x27;, fit_intercept=False)</span><br><span class="hljs-comment"># gbdt = GradientBoostingClassifier(learning_rate=0.1, n_estimators=100, max_depth=3)</span><br><span class="hljs-comment"># rf = RandomForestClassifier(max_depth=5, n_estimators=150)</span><br>svm = SVC(<br>    C=<span class="hljs-number">1.0</span>,<br>    kernel=<span class="hljs-string">&#x27;rbf&#x27;</span>,<br>    gamma=<span class="hljs-string">&#x27;scale&#x27;</span>,<br>    probability=<span class="hljs-literal">True</span><br>)<br>bpnn = MLPClassifier(hidden_layer_sizes=(<span class="hljs-number">50</span>,), max_iter=<span class="hljs-number">300</span>)<br><br><span class="hljs-comment"># b、元学习器</span><br>lr = LogisticRegression(C=<span class="hljs-number">0.1</span>, solver=<span class="hljs-string">&#x27;lbfgs&#x27;</span>, multi_class=<span class="hljs-string">&#x27;multinomial&#x27;</span>)<br><br><span class="hljs-comment">### stacking学习器</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">1、最基本的使用方法，用前面基学习器的输出作为元学习器的输入</span><br><span class="hljs-string">2、使用基学习器的输出类别的概率值作为元学习器输入，use_probas=True,若average_probas=True，那么这些基分类器对每一个类别产生的概率进行平均，否者直接拼接</span><br><span class="hljs-string"> classifier1  = [0.2,0.5,0.3]</span><br><span class="hljs-string"> classifier2  = [0.3,0.3,0.4]</span><br><span class="hljs-string">  average_probas=True: [0.25,0.4,0.35]</span><br><span class="hljs-string">  average_probas=Flase: [0.2,0.5,0.3,0.3,0.3,0.4]</span><br><span class="hljs-string"></span><br><span class="hljs-string">3、对训练集中的特征维度进行操作，每次训练不同的基学习器的时候用不同的特征，比如我再训练KNN的时候只用前两个特征，训练RF的时候用其他的几个特征</span><br><span class="hljs-string">    通过pipline来实现</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">classifiers, 基学习器</span><br><span class="hljs-string">meta_classifier, 元学习器</span><br><span class="hljs-string">use_probas=False, </span><br><span class="hljs-string">drop_last_proba=False,</span><br><span class="hljs-string">average_probas=False, </span><br><span class="hljs-string">verbose=0,</span><br><span class="hljs-string">use_features_in_secondary=False,</span><br><span class="hljs-string">store_train_meta_features=False,</span><br><span class="hljs-string">use_clones=True</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><br><span class="hljs-comment">###方式一</span><br>stacking01 = StackingClassifier(meta_classifier=lr,<br>                                <span class="hljs-comment"># classifiers=[knn, softmax, gbdt, rf]</span><br>                                classifiers=[knn, svm, bpnn]<br>                                )<br><br><span class="hljs-comment">###方式二</span><br>stacking02 = StackingClassifier(meta_classifier=lr,<br>                                <span class="hljs-comment"># classifiers=[knn, softmax, gbdt, rf],</span><br>                                classifiers=[knn, svm, bpnn],<br>                                use_probas=<span class="hljs-literal">True</span>,<br>                                average_probas=<span class="hljs-literal">False</span>)<br><br><span class="hljs-comment">###方式三</span><br><span class="hljs-comment"># 基学习器</span><br>pipe_knn = Pipeline([(<span class="hljs-string">&#x27;x&#x27;</span>, ColumnSelector([<span class="hljs-number">0</span>, <span class="hljs-number">5</span>])),<br>                     (<span class="hljs-string">&#x27;knn&#x27;</span>, knn)])<br><span class="hljs-comment"># pipe_softmax = Pipeline([(&#x27;x&#x27;, ColumnSelector([5, 10])),</span><br><span class="hljs-comment">#                          (&#x27;softmax&#x27;, softmax)])</span><br><span class="hljs-comment"># pipe_rf = Pipeline([(&#x27;x&#x27;, ColumnSelector([10, 17])),</span><br><span class="hljs-comment">#                     (&#x27;rf&#x27;, rf)])</span><br><span class="hljs-comment"># pipe_gbdt = Pipeline([(&#x27;x&#x27;, ColumnSelector([0, 15])),</span><br><span class="hljs-comment">#                       (&#x27;gbdt&#x27;, gbdt)])</span><br>pipe_svm = Pipeline([(<span class="hljs-string">&#x27;x&#x27;</span>, ColumnSelector([<span class="hljs-number">5</span>, <span class="hljs-number">11</span>])),<br>                     (<span class="hljs-string">&#x27;svm&#x27;</span>, svm)])<br>pipe_bpnn = Pipeline([(<span class="hljs-string">&#x27;x&#x27;</span>, ColumnSelector([<span class="hljs-number">0</span>, <span class="hljs-number">17</span>])),<br>                     (<span class="hljs-string">&#x27;bpnn&#x27;</span>, bpnn)])<br><span class="hljs-comment">##stacking</span><br>stacking03 = StackingClassifier(meta_classifier=lr,<br>                                <span class="hljs-comment"># classifiers=[pipe_knn, pipe_softmax, pipe_rf, pipe_gbdt]</span><br>                                classifiers=[pipe_knn, pipe_svm, pipe_bpnn]<br>                                )<br><br><span class="hljs-comment">###模型训练与比较</span><br>scores_train = []<br>scores_test = []<br>models = []<br>times = []<br><br><span class="hljs-keyword">for</span> clf, modelname <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(<br>                        <span class="hljs-comment"># [knn, softmax, gbdt, rf, stacking01, stacking02, stacking03],</span><br>                        <span class="hljs-comment">#   [&#x27;knn&#x27;, &#x27;softmax&#x27;, &#x27;gbdt&#x27;, &#x27;rf&#x27;, &#x27;stacking01&#x27;, &#x27;stacking02&#x27;, &#x27;stacking03&#x27;]):</span><br>                            [knn, svm, bpnn, stacking01, stacking02, stacking03],<br>                            [<span class="hljs-string">&#x27;knn&#x27;</span>, <span class="hljs-string">&#x27;svm&#x27;</span>, <span class="hljs-string">&#x27;bpnn&#x27;</span>, <span class="hljs-string">&#x27;stacking01&#x27;</span>, <span class="hljs-string">&#x27;stacking02&#x27;</span>, <span class="hljs-string">&#x27;stacking03&#x27;</span>]):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;start:%s&#x27;</span> % (modelname))<br>    start = time.time()<br>    clf.fit(x_train, y_train)<br>    end = time.time()<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;耗时：&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(end - start))<br>    score_train = clf.score(x_train, y_train)<br>    score_test = clf.score(x_test, y_test)<br>    scores_train.append(score_train)<br>    scores_test.append(score_test)<br>    models.append(modelname)<br>    times.append(end - start)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;scores_train:&#x27;</span>, scores_train)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;scores_test&#x27;</span>, scores_test)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;models:&#x27;</span>, models)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;开始画图----------&#x27;</span>)<br>plt.figure(num=<span class="hljs-number">1</span>)<br>plt.plot([<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>], scores_train, <span class="hljs-string">&#x27;r&#x27;</span>, label=<span class="hljs-string">u&#x27;训练集&#x27;</span>)<br>plt.plot([<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>], scores_test, <span class="hljs-string">&#x27;b&#x27;</span>, label=<span class="hljs-string">u&#x27;测试集&#x27;</span>)<br>plt.title(<span class="hljs-string">u&#x27;滑坡数据不同分类器准确率比较&#x27;</span>, fontsize=<span class="hljs-number">16</span>)<br>plt.xticks([<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>], models, rotation=<span class="hljs-number">0</span>)<br>plt.legend(loc=<span class="hljs-string">&#x27;lower left&#x27;</span>)<br>plt.figure(num=<span class="hljs-number">2</span>)<br>plt.plot([<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>], times)<br>plt.title(<span class="hljs-string">u&#x27;滑坡数据不同分类器训练时间比较&#x27;</span>, fontsize=<span class="hljs-number">16</span>)<br>plt.xticks([<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>], models, rotation=<span class="hljs-number">0</span>)<br>plt.legend(loc=<span class="hljs-string">&#x27;lower left&#x27;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure>]]></content>
    
    
    
    <tags>
      
      <tag>代码</tag>
      
      <tag>实验</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>文献笔记|顾及空间异质性和特征优选的滑坡易发性评价方法</title>
    <link href="/2024/10/28/%E6%96%87%E7%8C%AE%E7%AC%94%E8%AE%B0%E9%A1%BE%E5%8F%8A%E7%A9%BA%E9%97%B4%E5%BC%82%E8%B4%A8%E6%80%A7%E5%92%8C%E7%89%B9%E5%BE%81%E4%BC%98%E9%80%89%E7%9A%84%E6%BB%91%E5%9D%A1%E6%98%93%E5%8F%91%E6%80%A7%E8%AF%84%E4%BB%B7%E6%96%B9%E6%B3%95/"/>
    <url>/2024/10/28/%E6%96%87%E7%8C%AE%E7%AC%94%E8%AE%B0%E9%A1%BE%E5%8F%8A%E7%A9%BA%E9%97%B4%E5%BC%82%E8%B4%A8%E6%80%A7%E5%92%8C%E7%89%B9%E5%BE%81%E4%BC%98%E9%80%89%E7%9A%84%E6%BB%91%E5%9D%A1%E6%98%93%E5%8F%91%E6%80%A7%E8%AF%84%E4%BB%B7%E6%96%B9%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<p><a href="https://mp.weixin.qq.com/s/y564_uB6Li0-kzHLI4Q6lw">文献推荐链接</a></p><h2 id="论文摘要">论文摘要</h2><p>高效、精准、可靠的滑坡易发性评价方法是灾前科学预警和全面防治的关键手段。然而，传统滑坡易发性评价方法未能有效解决空间异质性和冗余特征造成的预测偏差。针对该问题，本文提出了一种顾及空间异质性和特征优选的滑坡易发性评价方法（spatial feature optimized Stacking，SF-Stacking）。该方法首先使用AGNES聚类（agglomerative nesting）将研究区全局栅格单元分成若干个局部子区，然后采用一种特征优选策略为每个子区选择最优致灾因子组合，最后采用Stacking集成技术耦合多种机器学习算法实现滑坡易发性评价。以宜宾市为研究区，基于滑坡灾害易发性分区图和统计学指标，将SF-Stacking方法与7种传统方法对比表明，SF-Stacking方法的准确性最优，稳健性最强，可解释性最高。</p><h2 id="论文创新点">论文创新点</h2><p>提出一种顾及空间异质性和特征优选的滑坡易发性评价方法（spatial feature optimized Stacking，SF-Stacking）<br>①采用聚类分析将研究区分成若干个均质子区，消除空间特征异质性造成的预测偏差；<br>②耦合多种特征选择方法筛选各个子区最优空间特征，提升特征优选的可靠性；<br>③使用集成学习算法拟合重要特征与滑坡之间的复杂非线性关系，提高模型的普适性。</p><h2 id="研究方法">研究方法</h2><p><strong>特征因子</strong>：从地形地貌、水文地质、人类工程活动、降雨等方面选取了高程、坡度、坡向、平面曲率、剖面曲率、NDVI、TWI、年降雨量、断层距离、河流距离、道路距离、POI、PGA、岩性及土地利用15个特征因子。为保证空间一致性，将所有因子的空间分辨率均重采样至30 m，借助自然间断法对连续型特征因子进行分级</p><p><strong>数据预处理</strong>：用信息量法生成非滑坡点</p><p><strong>模型构建</strong>：</p><p>1、<em><strong>特征空间异质性分区</strong></em>：根据CH指数确定簇间度量准则dmin、最优聚类簇个数，用AGNES算法对滑坡点进行分簇，基于滑坡点聚类结果构建泰森多边形，将研究区划分成不同子区</p><p>2、<em><strong>特征选择</strong></em>：在各子区中采用特征优选方法（VIF-Boruta-GeoDetector，VBG）剔除冗余特征因子<br>VIF（方差膨胀系数）：对特征因子进行多重共线性检验<br>Boruta算法:基于随机森林分类算法对特征选择<br>GeoDetector（地理探测器）:对样本空间划分和对局部线性模型的拟合来挖掘空间异质性，能够筛选出最具有区分性和解释性的特征</p><p>3、<em><strong>模型集成</strong></em>：最后基于各子区最优特征因子构建相应的Stacking集成模型（提高模型可靠性、降低过拟合风险）<br>Stacking集成学习算法：基学习器（SVM、BPNN、KNN）对训练集进行k折交叉训练（如k=5），并堆叠每个基学习器每折的输出结果作为新的训练集；然后通过每一次训练对验证集进行预测，将k次预测平均值作为新的验证集；最后将新的训练集和验证集输入到元学习器(LR)中得到最终的预测值</p><p><strong>模型精度评价</strong>：主要借助统计学指标和易发性结果图来评估模型性能<br>顾及空间异质性和特征优选的滑坡易发性评价方法（spatial feature optimized Stacking，SF-Stacking）<br>顾及空间异质性的Stacking方法（spatial Stacking，S-Stacking）<br>顾及特征优选的Stacking方法（feature optimized Stacking，F-Stacking）<br>未考虑空间异质性和冗余特征的影响，选取ACC、F1值、ROC曲线、Kappa系数、Sridevi Jadi试验概率精度（SJ）：<br>全局Stacking集成学习方法（Stacking）<br>全局机器学习方法（SVM、BPNN、KNN、LR）<br><img src="/image/%E6%B5%81%E7%A8%8B%E5%9B%BE2.jpg" alt=""></p><h2 id="主要结论">主要结论</h2><p>①空间异质性影响滑坡易发性预测精度<br>②本文提出的特征优选策略提高了模型计算效率和准确率<br>③SF-Stacking方法预测精度明显优于其他7种方法的预测精度，具有较高的稳健性、可靠性</p><p><strong>解决问题</strong>：有效解决了空间异质性和冗余特征造成的滑坡预测偏差缺陷，实现了滑坡易发性高精度制图，进而可为自然灾害风险评估、预警和防治等国家重大工程提供理论方法和技术支撑</p><p><strong>局限性</strong>：该方法需要研究区有足够的滑坡灾害点，且滑坡点与非滑坡点的比例不同可能会造成滑坡易发性评价的不确定性。</p>]]></content>
    
    
    
    <tags>
      
      <tag>文献</tag>
      
      <tag>滑坡</tag>
      
      <tag>集成学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>滑坡实验|机器学习方法</title>
    <link href="/2024/10/17/%E6%BB%91%E5%9D%A1%E5%AE%9E%E9%AA%8C%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/"/>
    <url>/2024/10/17/%E6%BB%91%E5%9D%A1%E5%AE%9E%E9%AA%8C%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h1>RF</h1><p><a href="https://blog.csdn.net/cxyxx12/article/details/134415329">代码基于此博客</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> RandomForestClassifier<br><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> confusion_matrix, classification_report, accuracy_score<br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> ShuffleSplit, cross_val_score, LeaveOneOut, StratifiedShuffleSplit<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">from</span> sklearn.tree <span class="hljs-keyword">import</span> export_graphviz<br><span class="hljs-keyword">import</span> graphviz<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> os<br><span class="hljs-comment"># os.environ[&quot;PATH&quot;] += os.pathsep + r&#x27; D:\APP\anaconda\envs\pytorch\Library\bin\graphviz\config6&#x27;</span><br><br><span class="hljs-comment"># 加载数据</span><br>df_X = pd.read_excel(<span class="hljs-string">&#x27;DataX.xlsx&#x27;</span>)<br>feature_names = df_X.columns.tolist()  <span class="hljs-comment"># 将文件列名转换为Python列表</span><br><br><br>X = df_X.values  <span class="hljs-comment"># 输入特征，返回给定DataFrame的Numpy表现形式</span><br><br>y = pd.DataFrame(pd.read_excel(<span class="hljs-string">&#x27;DataY.xlsx&#x27;</span>)).values.ravel()  <span class="hljs-comment"># 目标变量</span><br><br><span class="hljs-comment"># 创建随机森林分类器</span><br><span class="hljs-comment"># clf = RandomForestClassifier(n_estimators=100)</span><br>clf = RandomForestClassifier(class_weight=<span class="hljs-string">&#x27;balanced&#x27;</span>,<br>                             n_estimators=<span class="hljs-number">100</span>,  <span class="hljs-comment"># 树的数量</span><br>                             <span class="hljs-comment"># max_depth=10,  # 树的最大深度,设置后欠拟合</span><br>                             min_samples_split=<span class="hljs-number">2</span>,  <span class="hljs-comment"># 节点分裂所需的最小样本数</span><br>                             min_samples_leaf=<span class="hljs-number">1</span>  <span class="hljs-comment"># 叶节点的最小样本数</span><br>                             )<br><br><span class="hljs-comment"># 创建ShuffleSplit对象，用于执行自动洗牌</span><br><span class="hljs-comment"># ss = ShuffleSplit(n_splits=5, train_size=0.7, test_size=0.3, random_state=0)  # n_split:划分次数</span><br>ss = StratifiedShuffleSplit(n_splits=<span class="hljs-number">1</span>, test_size=<span class="hljs-number">0.3</span>, random_state=<span class="hljs-number">0</span>)  <span class="hljs-comment"># 实现分层、随机采样</span><br><br>accuracies = []<br>split = <span class="hljs-number">0</span><br><br><span class="hljs-comment"># 循环遍历每个拆分，并使用随机森林分类器对每个拆分进行训练和评估</span><br><span class="hljs-keyword">for</span> train_index, test_index <span class="hljs-keyword">in</span> ss.split(X, y):<br>    X_train, X_test = X[train_index], X[test_index]<br>    y_train, y_test = y[train_index], y[test_index]<br>    clf.fit(X_train, y_train)<br><br>    <span class="hljs-comment"># 评估训练集</span><br>    y_train_pred = clf.predict(X_train)<br>    train_acc = accuracy_score(y_train, y_train_pred)<br><br>    <span class="hljs-comment"># 评估测试集</span><br>    y_pred = clf.predict(X_test)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Confusion Matrix:&quot;</span>)<br>    <span class="hljs-built_in">print</span>(confusion_matrix(y_test, y_pred))  <span class="hljs-comment"># 输出分类结果矩阵</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Classification Report:&quot;</span>)<br>    <span class="hljs-built_in">print</span>(classification_report(y_test, y_pred))  <span class="hljs-comment"># 输出混淆矩阵</span><br>    acc = accuracy_score(y_test, y_pred)<br>    accuracies.append(acc)<br>    split += <span class="hljs-number">1</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;Training/Testing Accuracy for <span class="hljs-subst">&#123;split&#125;</span> split : <span class="hljs-subst">&#123;train_acc, acc&#125;</span>&#x27;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Average accuracy over <span class="hljs-subst">&#123;ss.get_n_splits()&#125;</span> splits: <span class="hljs-subst">&#123;<span class="hljs-built_in">sum</span>(accuracies) / <span class="hljs-built_in">len</span>(accuracies)&#125;</span>&quot;</span>)<br><br><span class="hljs-comment"># # 5折交叉验证</span><br><span class="hljs-comment"># cv_scores = cross_val_score(clf, X, y, cv=5, scoring=&#x27;accuracy&#x27;)  # cv=5 表示5折交叉验证</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment"># # 输出每次的得分和平均准确率</span><br><span class="hljs-comment"># print(&quot;Cross-validation scores for each fold:&quot;, cv_scores)</span><br><span class="hljs-comment"># print(&quot;Average accuracy:&quot;, cv_scores.mean())  # 0.52</span><br><br><span class="hljs-comment"># # 留一法交叉验证， 运行时间太长，别运行</span><br><span class="hljs-comment"># loo = LeaveOneOut()</span><br><span class="hljs-comment"># loo_scores = cross_val_score(clf, X, y, cv=loo, scoring=&#x27;accuracy&#x27;)</span><br><span class="hljs-comment"># print(f&quot;Average accuracy with LOO: &#123;loo_scores.mean()&#125;&quot;)</span><br><br><br><span class="hljs-comment"># 特征重要性</span><br>importances = clf.feature_importances_<br><span class="hljs-built_in">print</span>(importances)<br><br><span class="hljs-comment"># 画条形图</span><br>plt.barh(<span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(importances)), importances)<br><br><span class="hljs-comment"># 添加标题和特征名称</span><br>plt.title(<span class="hljs-string">&quot;Feature Importances&quot;</span>)<br>plt.yticks(<span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(importances)), feature_names)  <span class="hljs-comment"># 确保特征名称与特征数一致</span><br><br><span class="hljs-comment"># 保存图像</span><br>plt.savefig(<span class="hljs-string">&#x27;feature_importance.png&#x27;</span>)<br><br><span class="hljs-comment"># 可视化随机森林中的一棵树</span><br>dot_data = export_graphviz(clf.estimators_[<span class="hljs-number">0</span>], out_file=<span class="hljs-literal">None</span>,<br>                           feature_names=feature_names)<br><br><span class="hljs-comment"># 使用 graphviz 库读取 dot 文件并生成决策树可视化图形</span><br>graph = graphviz.Source(dot_data)<br>graph.render(<span class="hljs-string">&#x27;decision_tree&#x27;</span>)<br><br><br></code></pre></td></tr></table></figure><h1>SVM</h1><p><a href="https://blog.csdn.net/cxyxx12/article/details/134415329">代码基于此博客</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> svm<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">SVM参数</span><br><span class="hljs-string">C:惩罚系数，越高越不能容易出现误差，容易过拟合（越小则容易欠拟合）(但是此模型提高C不能改善欠拟合）</span><br><span class="hljs-string">gamma：选择径向基函数（RBF）作为kernel后，该函数自带的一个参数，gamma越大，支持向量越少，gamma越小，支持向量越多</span><br><span class="hljs-string"></span><br><span class="hljs-string">核函数：</span><br><span class="hljs-string">线性核Linear</span><br><span class="hljs-string">多项式核Poly</span><br><span class="hljs-string">高斯核rbf</span><br><span class="hljs-string">拉普拉斯核</span><br><span class="hljs-string">Sigmoid核</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br>svm.SVC(<br>    C=<span class="hljs-number">1.0</span>,<br>    kernel=<span class="hljs-string">&#x27;rbf&#x27;</span>,<br>    gamma=<span class="hljs-string">&#x27;scale&#x27;</span><br>)<br>df = pd.read_excel(<span class="hljs-string">&#x27;MLinput.xlsx&#x27;</span>)<br><span class="hljs-comment"># 去除后accuracy变低了</span><br><span class="hljs-comment"># df = df.drop([&#x27;X&#x27;, &#x27;Y&#x27;, &#x27;Z&#x27;, &#x27;sca&#x27;], axis=1)</span><br><br>feature = df.columns.tolist()<br><span class="hljs-keyword">del</span> feature[<span class="hljs-number">0</span>]<br><span class="hljs-comment"># print(feature)</span><br><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">3、滑坡/非滑坡数量情况统计</span><br><span class="hljs-string">观察样本集 非滑坡:0 与 滑坡:1 数量情况</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns<br><span class="hljs-keyword">from</span> matplotlib <span class="hljs-keyword">import</span> pyplot <span class="hljs-keyword">as</span> plt<br><br>sns.countplot(x=df[<span class="hljs-string">&#x27;class&#x27;</span>])<br><span class="hljs-comment"># plt.show()</span><br><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">筛选特征--&gt;降维</span><br><span class="hljs-string">特征里面两两特征之间的相关性</span><br><span class="hljs-string">    - 相关程度非常高：选择其中一个作为代表即可</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br>feature_corr = df[feature].corr()<br><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">来进行相关性的可视化</span><br><span class="hljs-string">- 热力图：颜色越浅说明相关程度越大</span><br><span class="hljs-string">    - annot=True--&gt;显示每个方格的数据</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br>plt.figure(figsize=(<span class="hljs-number">14</span>, <span class="hljs-number">8</span>))<br>sns.heatmap(feature_corr, annot=<span class="hljs-literal">False</span>)<br><span class="hljs-comment"># plt.show()</span><br><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">4  数据分割</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<br><br>train, test = train_test_split(df, test_size=<span class="hljs-number">0.3</span>)  <span class="hljs-comment"># 纯随机采样</span><br>train_X = train[feature]<br>test_X = test[feature]<br><span class="hljs-comment"># 构建 标签</span><br>train_y = train[<span class="hljs-string">&#x27;class&#x27;</span>]<br>y_test = test[<span class="hljs-string">&#x27;class&#x27;</span>]<br><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">5  数据归一化</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> StandardScaler<br><br>ss = StandardScaler()<br>train_X = ss.fit_transform(train_X)<br>test_X = ss.fit_transform(test_X)<br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">6  模型训练及预测</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-keyword">from</span> sklearn.svm <span class="hljs-keyword">import</span> SVC<br>model = SVC()<br>model.fit(train_X, train_y)<br><br>y_pred = model.predict(test_X)<br><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">7 模型评价</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> confusion_matrix, classification_report, accuracy_score<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Confusion Matrix:&quot;</span>)<br><span class="hljs-built_in">print</span>(confusion_matrix(y_test, y_pred))  <span class="hljs-comment"># 输出分类结果矩阵</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Classification Report:&quot;</span>)<br><span class="hljs-built_in">print</span>(classification_report(y_test, y_pred))  <span class="hljs-comment"># 输出混淆矩阵</span><br><br>train_acc = model.score(train_X, train_y)  <span class="hljs-comment"># 内联函数</span><br>acc = accuracy_score(y_test, y_pred)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;Training/Testing Accuracy: <span class="hljs-subst">&#123;train_acc, acc&#125;</span>&#x27;</span>)  <span class="hljs-comment"># 0.6293</span><br></code></pre></td></tr></table></figure><h1>LR</h1><p>代码基于此博客SVM</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LogisticRegression<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> GridSearchCV<br><br>LR = LogisticRegression()<br><br><br>param_grid = &#123;<br>    <span class="hljs-string">&#x27;C&#x27;</span>: [<span class="hljs-number">1</span>],<br>    <span class="hljs-string">&#x27;penalty&#x27;</span>: [<span class="hljs-string">&#x27;l1&#x27;</span>],  <span class="hljs-comment"># 决定正则化类型</span><br>    <span class="hljs-string">&#x27;solver&#x27;</span>: [<span class="hljs-string">&#x27;liblinear&#x27;</span>],  <span class="hljs-comment"># 决定优化算法</span><br>    <span class="hljs-string">&#x27;max_iter&#x27;</span>: [<span class="hljs-number">1000</span>]<br>&#125;<br><span class="hljs-comment"># 自动化参数调优</span><br>grid_search = GridSearchCV(LR, param_grid, cv=<span class="hljs-number">5</span>)<br><br>df = pd.read_excel(<span class="hljs-string">&#x27;MLinput.xlsx&#x27;</span>)<br><br>feature = df.columns.tolist()<br><span class="hljs-keyword">del</span> feature[<span class="hljs-number">0</span>]<br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">4  数据分割</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<br><br>train, test = train_test_split(df, test_size=<span class="hljs-number">0.3</span>)  <span class="hljs-comment"># 纯随机采样</span><br>train_X = train[feature]<br>test_X = test[feature]<br><span class="hljs-comment"># 构建 标签</span><br>train_y = train[<span class="hljs-string">&#x27;class&#x27;</span>]<br>y_test = test[<span class="hljs-string">&#x27;class&#x27;</span>]<br><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">5  数据归一化</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> StandardScaler<br><br>ss = StandardScaler()<br>train_X = ss.fit_transform(train_X)<br>test_X = ss.fit_transform(test_X)<br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">6  寻找最佳参数,训练模型</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br>grid_search.fit(train_X, train_y)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Best parameters:&quot;</span>, grid_search.best_params_)<br><span class="hljs-comment"># 使用最优模型进行预测</span><br>best_model = grid_search.best_estimator_<br>y_pred = best_model.predict(test_X)<br><br><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">7 模型评价</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> confusion_matrix, classification_report, accuracy_score<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Confusion Matrix:&quot;</span>)<br><span class="hljs-built_in">print</span>(confusion_matrix(y_test, y_pred))  <span class="hljs-comment"># 输出分类结果矩阵</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Classification Report:&quot;</span>)<br><span class="hljs-built_in">print</span>(classification_report(y_test, y_pred))  <span class="hljs-comment"># 输出混淆矩阵</span><br><br>train_acc = best_model.score(train_X, train_y)  <span class="hljs-comment"># 内联函数</span><br>acc = accuracy_score(y_test, y_pred)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;Training/Testing Accuracy: <span class="hljs-subst">&#123;train_acc, acc&#125;</span>&#x27;</span>)  <span class="hljs-comment"># 0.6239</span><br></code></pre></td></tr></table></figure><h1>分类器筛选</h1><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-keyword">from</span> sklearn.ensemble import RandomForestClassifier<br><span class="hljs-keyword">from</span> sklearn.linear_model import LogisticRegression<br><span class="hljs-keyword">from</span> sklearn.svm import SVC<br><span class="hljs-keyword">from</span> sklearn.neighbors import KNeighborsClassifier<br><span class="hljs-keyword">from</span> sklearn.tree import DecisionTreeClassifier, export_graphviz<br><span class="hljs-keyword">from</span> sklearn.metrics import confusion_matrix, classification_report, accuracy_score<br><span class="hljs-keyword">from</span> sklearn.model_selection import ShuffleSplit, cross_val_score, LeaveOneOut, StratifiedShuffleSplit<br>import pandas as pd<br><span class="hljs-keyword">from</span> sklearn.preprocessing import StandardScaler<br><span class="hljs-keyword">from</span> sklearn.ensemble import GradientBoostingClassifier<br><span class="hljs-keyword">from</span> xgboost import XGBClassifier<br><span class="hljs-keyword">from</span> sklearn.ensemble import AdaBoostClassifier<br><span class="hljs-keyword">from</span> lightgbm import LGBMClassifier<br><span class="hljs-keyword">from</span> sklearn.neural_network import MLPClassifier<br><br><br><span class="hljs-comment"># # 加载数据</span><br><span class="hljs-comment"># df1 = pd.read_excel(&#x27;DataX.xlsx&#x27;)</span><br><span class="hljs-comment"># XX = df1.values  # 输入特征，返回给定DataFrame的Numpy表现形式</span><br><span class="hljs-comment"># feature_names = df1.columns.tolist()  # 将文件列名转换为Python列表</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment"># df2 = pd.read_excel(&#x27;DataY.xlsx&#x27;)</span><br><span class="hljs-comment"># yy = df2.values.ravel()  # 目标变量</span><br><br>data = pd.read_csv(<span class="hljs-string">&#x27;sxcoor.csv&#x27;</span>)<br><span class="hljs-comment"># 假设最后一列是标签，前面的列是特征</span><br>XX = data.iloc[1:, 1:].values  # 特征<br>yy = data.iloc[1:, 0].values   # 标签<br><br>s = StratifiedShuffleSplit(<span class="hljs-attribute">n_splits</span>=1, <span class="hljs-attribute">test_size</span>=0.5, <span class="hljs-attribute">random_state</span>=0)<br><br><span class="hljs-keyword">for</span> half1, half2 <span class="hljs-keyword">in</span> s.split(XX, yy):<br>    X, new_data = XX[half1], XX[half2]<br>    y, new_data_y = yy[half1], yy[half2]<br><br><br><span class="hljs-comment"># 用于存储准确率大于 0.7 的模型及其最佳参数</span><br>qualified_models = &#123;&#125;<br><br><span class="hljs-comment"># 创建随机森林分类器</span><br><span class="hljs-comment"># clf = RandomForestClassifier(n_estimators=100)</span><br>RF = RandomForestClassifier(<span class="hljs-attribute">class_weight</span>=<span class="hljs-string">&#x27;balanced&#x27;</span>,<br>                             <span class="hljs-attribute">n_estimators</span>=100,  # 树的数量<br>                             # <span class="hljs-attribute">max_depth</span>=10,  # 树的最大深度,设置后欠拟合<br>                             <span class="hljs-attribute">min_samples_split</span>=2,  # 节点分裂所需的最小样本数<br>                             <span class="hljs-attribute">min_samples_leaf</span>=1  # 叶节点的最小样本数<br>                             )<br><br><br>LR = LogisticRegression(<span class="hljs-attribute">C</span>=1, <span class="hljs-attribute">penalty</span>=<span class="hljs-string">&#x27;l1&#x27;</span>, <span class="hljs-attribute">solver</span>=<span class="hljs-string">&#x27;liblinear&#x27;</span>, <span class="hljs-attribute">max_iter</span>=1000)<br><br>SVM = SVC(<br>    <span class="hljs-attribute">C</span>=1.0,<br>    <span class="hljs-attribute">kernel</span>=<span class="hljs-string">&#x27;rbf&#x27;</span>,<br>    <span class="hljs-attribute">gamma</span>=<span class="hljs-string">&#x27;scale&#x27;</span><br>)<br><br>KNN = KNeighborsClassifier(<span class="hljs-attribute">n_neighbors</span>=6)<br><br>DT = DecisionTreeClassifier(<span class="hljs-attribute">random_state</span>=42)<br><br>GBC = GradientBoostingClassifier(<span class="hljs-attribute">n_estimators</span>=100, <span class="hljs-attribute">learning_rate</span>=0.1, <span class="hljs-attribute">max_depth</span>=3, <span class="hljs-attribute">random_state</span>=42)<br><br>XGB = XGBClassifier(<span class="hljs-attribute">n_estimators</span>=100, <span class="hljs-attribute">learning_rate</span>=0.1, <span class="hljs-attribute">max_depth</span>=3, <span class="hljs-attribute">random_state</span>=42)<br><br>ABC = AdaBoostClassifier(<span class="hljs-attribute">n_estimators</span>=100, <span class="hljs-attribute">random_state</span>=42)<br><br>LGBM = LGBMClassifier(<span class="hljs-attribute">n_estimators</span>=100, <span class="hljs-attribute">num_leaves</span>=31, <span class="hljs-attribute">max_depth</span>=-1, <span class="hljs-attribute">learning_rate</span>=0.1, <span class="hljs-attribute">random_state</span>=42,<br>                      <span class="hljs-attribute">force_col_wise</span>=<span class="hljs-string">&#x27;true&#x27;</span>)<br><br>MLP = MLPClassifier(hidden_layer_sizes=(100,), <span class="hljs-attribute">max_iter</span>=500, <span class="hljs-attribute">activation</span>=<span class="hljs-string">&#x27;relu&#x27;</span>, <span class="hljs-attribute">solver</span>=<span class="hljs-string">&#x27;adam&#x27;</span>, <span class="hljs-attribute">random_state</span>=42)<br><br>model = &#123;<br>    <span class="hljs-string">&#x27;RF&#x27;</span>: RF,<br>    <span class="hljs-string">&#x27;LR&#x27;</span>: LR,<br>    <span class="hljs-string">&#x27;SVM&#x27;</span>: SVM,<br>    <span class="hljs-string">&#x27;KNN&#x27;</span>: KNN,<br>    <span class="hljs-string">&#x27;DT&#x27;</span>: DT,<br>    <span class="hljs-string">&#x27;GBC&#x27;</span>: GBC,<br>    <span class="hljs-string">&#x27;XGB&#x27;</span>: XGB,<br>    <span class="hljs-string">&#x27;MLP&#x27;</span>: MLP,<br>    <span class="hljs-string">&#x27;ABC&#x27;</span>: ABC,<br>    <span class="hljs-string">&#x27;LGBM&#x27;</span>: LGBM<br>&#125;<br><br><span class="hljs-comment"># 创建ShuffleSplit对象，用于执行自动洗牌</span><br>ss = StratifiedShuffleSplit(<span class="hljs-attribute">n_splits</span>=1, <span class="hljs-attribute">test_size</span>=0.3, <span class="hljs-attribute">random_state</span>=0)  # 实现分层、随机采样<br><br>scaler = StandardScaler()<br><span class="hljs-keyword">for</span> name, clf <span class="hljs-keyword">in</span> model.items():<br><br>    accuracies = []<br>    split = 0<br><br>    # 循环遍历每个拆分，并使用随机森林分类器对每个拆分进行训练和评估<br>    <span class="hljs-keyword">for</span> train_index, test_index <span class="hljs-keyword">in</span> ss.split(X, y):<br>        X_train, X_test = X[train_index], X[test_index]<br>        y_train, y_test = y[train_index], y[test_index]<br><br>        # 树形模型不需要归一化<br>        <span class="hljs-keyword">if</span> name != <span class="hljs-string">&#x27;RF&#x27;</span> <span class="hljs-keyword">and</span> name != <span class="hljs-string">&#x27;DT&#x27;</span>:<br><br>            X_train = scaler.fit_transform(X_train)<br>            X_test = scaler.transform(X_test)<br><br>        clf.fit(X_train, y_train)<br><br>        # 评估训练集<br>        y_train_pred = clf.predict(X_train)<br>        train_acc = accuracy_score(y_train, y_train_pred)<br><br>        # 评估测试集<br>        y_pred = clf.predict(X_test)<br>        acc = accuracy_score(y_test, y_pred)<br>        accuracies.append(acc)<br>        split += 1<br>        # 筛选准确率大于 0.69 的模型<br><br>        # <span class="hljs-built_in">print</span>(f<span class="hljs-string">&#x27;Training/Testing Accuracy for &#123;split&#125; split : &#123;train_acc, acc&#125;&#x27;</span>)<br>    accuracy = sum(accuracies) / len(accuracies)<br>    <span class="hljs-built_in">print</span>(f<span class="hljs-string">&quot;&#123;clf.__class__.__name__&#125; learner Average accuracy over &#123;split&#125; splits: &#123;accuracy&#125;&quot;</span>)<br>    <span class="hljs-keyword">if</span> accuracy &gt;= 0.69:<br>        qualified_models[name] = clf<br>        <span class="hljs-built_in">print</span>(f<span class="hljs-string">&quot;&#123;name&#125; 模型被选为合格模型&quot;</span>)<br><br><span class="hljs-keyword">for</span> name, clf <span class="hljs-keyword">in</span> qualified_models.items():<br>    <span class="hljs-keyword">if</span> name != <span class="hljs-string">&#x27;RF&#x27;</span> <span class="hljs-keyword">and</span> name != <span class="hljs-string">&#x27;DT&#x27;</span>:<br>        new_data_scaled = scaler.transform(new_data)<br>    <span class="hljs-keyword">else</span>:<br>        new_data_scaled = new_data<br>    new_predictions = clf.predict(new_data_scaled)<br>    acc = accuracy_score(new_data_y, new_predictions)<br>    <span class="hljs-built_in">print</span>(f<span class="hljs-string">&quot;&#123;name&#125; Accuracy: &#123;acc&#125;&quot;</span>)<br></code></pre></td></tr></table></figure>]]></content>
    
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>代码</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>文献笔记|基于物理数据的数据驱动模型用于三峡库区滑坡易发性评估</title>
    <link href="/2024/10/17/%E6%96%87%E7%8C%AE%E7%AC%94%E8%AE%B0%E5%9F%BA%E4%BA%8E%E7%89%A9%E7%90%86%E6%95%B0%E6%8D%AE%E7%9A%84%E6%95%B0%E6%8D%AE%E9%A9%B1%E5%8A%A8%E6%A8%A1%E5%9E%8B%E7%94%A8%E4%BA%8E%E4%B8%89%E5%B3%A1%E5%BA%93%E5%8C%BA%E6%BB%91%E5%9D%A1%E6%98%93%E5%8F%91%E6%80%A7%E8%AF%84%E4%BC%B0/"/>
    <url>/2024/10/17/%E6%96%87%E7%8C%AE%E7%AC%94%E8%AE%B0%E5%9F%BA%E4%BA%8E%E7%89%A9%E7%90%86%E6%95%B0%E6%8D%AE%E7%9A%84%E6%95%B0%E6%8D%AE%E9%A9%B1%E5%8A%A8%E6%A8%A1%E5%9E%8B%E7%94%A8%E4%BA%8E%E4%B8%89%E5%B3%A1%E5%BA%93%E5%8C%BA%E6%BB%91%E5%9D%A1%E6%98%93%E5%8F%91%E6%80%A7%E8%AF%84%E4%BC%B0/</url>
    
    <content type="html"><![CDATA[<h2 id="文献推荐链接"><a href="https://mp.weixin.qq.com/s/3fUJaTqLTu-UgQkbqGmKxw">文献推荐链接</a></h2><p><a href="https://www.sciencedirect.com/science/article/pii/S1674987123000889?ref=pdf_download&amp;fr=RR-2&amp;rr=8d341b785e93ddc2#ab010">原文链接</a></p><h2 id="总述">总述</h2><p><img src="/image/%E6%B5%81%E7%A8%8B%E5%9B%BE.jpg" alt=""></p><table><thead><tr><th>数据处理</th><th>预测模型</th><th>评估方法</th></tr></thead><tbody><tr><td>Scoops 3D</td><td>随机森林法</td><td>AUC</td></tr></tbody></table><p>问题：非滑坡样本的生成<br>解决现状：分形理论模型生成的低坡区、无滑坡区和极低易发区的随机选择</p><p>Scoops 3D：一种具有很高的可解释性的物理方法，三维极限平衡方法，考虑了边坡的几何形状、材料特性和边界条件</p><h3 id="数据">数据</h3><p>研究区域：巴东县、秭归县；<br>方法：ArcGIS处理、提取收集的数据，将处理后的土地深度数据和相应的岩土参数输入到Scoops 3D，（安全系数）FS大于1.5被随机选择为负样本，从而建立完整数据集<br>数据收集：影响因子根据过往经验选取<br><img src="/image/%E5%BD%B1%E5%93%8D%E5%9B%A0%E5%AD%90.png" alt=""><br>数据处理：使用ArcGIS</p>]]></content>
    
    
    
    <tags>
      
      <tag>文献</tag>
      
      <tag>滑坡</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>环境搭建|fluid搭建指南</title>
    <link href="/2024/10/16/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BAfluid%E6%90%AD%E5%BB%BA%E6%8C%87%E5%8D%97/"/>
    <url>/2024/10/16/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BAfluid%E6%90%AD%E5%BB%BA%E6%8C%87%E5%8D%97/</url>
    
    <content type="html"><![CDATA[<p><a href="https://blog.csdn.net/yaorongke/article/details/119089190?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522DEC1D3C5-1243-434F-A99D-A0CFCB1371B1%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=DEC1D3C5-1243-434F-A99D-A0CFCB1371B1&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-119089190-null-null.142%5Ev100%5Epc_search_result_base8&amp;utm_term=github%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E7%BD%91%E7%AB%99&amp;spm=1018.2226.3001.4187">hexo配置指南</a><br><a href="https://hexo.fluid-dev.com/docs/guide/#%E5%85%B3%E4%BA%8E%E6%8C%87%E5%8D%97">fluid配置指南</a><br>使用了katex渲染公式</p>]]></content>
    
    
    
    <tags>
      
      <tag>环境搭建</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>组合数学</title>
    <link href="/2024/10/16/%E7%BB%84%E5%90%88%E6%95%B0%E5%AD%A6/"/>
    <url>/2024/10/16/%E7%BB%84%E5%90%88%E6%95%B0%E5%AD%A6/</url>
    
    <content type="html"><![CDATA[<h1>组合数学考试速成</h1><p>第一章、排列与组合</p><ol><li>高中学过的。</li><li>允许重复的组合、不相邻组合、格路模型</li><li>插空法、隔板法。</li><li>排序生成算法（三选一、考定义和具体实操）</li></ol><p>第二章、递推关系和母函数</p><ol><li>给母函数求an</li><li>给递推关系求母函数和an</li><li>给母函数求递推关系</li><li>解常系数齐次递推关系</li><li>司特林数</li></ol><p>第三章、容斥原理和鸽巢原理</p><ol><li>考得容易，基本都是考鸽巢原理。但也有容斥原理。</li><li>棋盘多项式。有禁区的排列。</li><li>Ramsey数，低概率。</li></ol><p>第四章、Burnside引理与Polya定理</p><ol><li>只考Polya定理，Burnside引理能解决的Polya定理也能干。</li></ol><p>第五章、区组设计</p><ol><li><p>什么是拉丁方，如何构造拉丁方？</p></li><li><p>（b，v，r，k，朗达）</p></li></ol><p><img src="/_posts/%E7%BB%84%E5%90%88%E6%95%B0%E5%AD%A6/image-20220111165410730.png" alt="image-20220111165410730"></p>]]></content>
    
    
    
    <tags>
      
      <tag>数学</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>重生之我是天才程序员</title>
    <link href="/2024/10/16/%E9%87%8D%E7%94%9F%E4%B9%8B%E6%88%91%E6%98%AF%E5%A4%A9%E6%89%8D%E7%A8%8B%E5%BA%8F%E5%91%98/"/>
    <url>/2024/10/16/%E9%87%8D%E7%94%9F%E4%B9%8B%E6%88%91%E6%98%AF%E5%A4%A9%E6%89%8D%E7%A8%8B%E5%BA%8F%E5%91%98/</url>
    
    <content type="html"><![CDATA[<h1>sandaokansini为何那样</h1><h2 id="10-16之debug顺序">10.16之debug顺序</h2><p>关于配置的问题应该先看<strong>README</strong><br>再网页<strong>搜索</strong><br>实在不行问GPT（不推荐）</p>]]></content>
    
    
    
    <tags>
      
      <tag>日志</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>机器学习|基础知识</title>
    <link href="/2024/10/11/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    <url>/2024/10/11/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/</url>
    
    <content type="html"><![CDATA[<h1>机器学习</h1><h2 id="绪论">绪论</h2><p>根据训练数据是否拥有标记信息，学习任务可大致分为<strong>监督学习</strong>（分类、回归）和 <strong>无监督学习</strong>（聚类）<br>聚类：将训练集数据分组，每组称为一个“簇”<br>泛化能力：学得模型适用于新样本的能力</p><h2 id="模型评估与选择">模型评估与选择</h2><p>精度=1-错误率<br><strong>过拟合</strong>：学习器将训练样本自身的特点当作所有潜在样本都具有的性质，导致泛化能力的下降<br>欠拟合：训练样本的一般性质未学好</p><h3 id="评估方法">评估方法</h3><p>将数据集D划分为训练集S和测试集T<br><strong>留出法</strong>：直接将D划分为两个互斥的集合，分层采样<br><strong>交叉验证法</strong>（p次k折交叉验证）：将D划分为k个大小相似的互斥子集，分层采样，每次用k-1个子集的并集作为训练集，剩下的为测试集，共k组训练/测试集，此过程重复p次<br>特例：留一法（LOO，p=k）<br><strong>自助法</strong>：通过<strong>自助采样</strong>生成D’（D中样本不在D’中出现的概率为(（1-<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mn>1</mn><mi>m</mi></mfrac></mrow><annotation encoding="application/x-tex">\frac{1}{m}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1901em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8451em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>）<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mi>m</mi></msup></mrow><annotation encoding="application/x-tex">^{m}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6644em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6644em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span></span></span></span></span></span></span></span></span></span></span></span>=<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mn>1</mn><mi>e</mi></mfrac></mrow><annotation encoding="application/x-tex">\frac{1}{e}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1901em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8451em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">e</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>=0.368），将D’作为训练集（改变初始数据集分布引入偏差），D\D’（:集合减法）作为测试集，使实际评估的模型和期望估计的模型使用训练样本数一致，且约1/3的没在训练集中出现的样本用于测试，这样的测试结果称为<strong>包外估计</strong><br>验证集：模型评估与选择中用于评估测试的数据集</p><h3 id="性能度量">性能度量</h3><p>衡量泛化性能的评估标准<br><strong>错误率与精度accuracy</strong>：相加等于1<br><strong>查准率precision</strong>：P=<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi>T</mi><mi>P</mi></mrow><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>P</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{TP}{TP+FP}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2757em;vertical-align:-0.4033em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8723em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">TP</span><span class="mbin mtight">+</span><span class="mord mathnormal mtight" style="margin-right:0.13889em;">FP</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">TP</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4033em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span><br><strong>查全率recall</strong>：R=<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi>T</mi><mi>P</mi></mrow><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>N</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{TP}{TP+FN}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2757em;vertical-align:-0.4033em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8723em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">TP</span><span class="mbin mtight">+</span><span class="mord mathnormal mtight" style="margin-right:0.10903em;">FN</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">TP</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4033em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span><br>混淆矩阵（confusion matrix）</p><table><thead><tr><th>正例</th><th>反例</th></tr></thead><tbody><tr><td>TP：true positive真正例</td><td>FP：false positive假正例</td></tr><tr><td>TN：true negative真反例</td><td>FN：false negative假反例</td></tr></tbody></table><p>P-R曲线：面积越大，学习器性能越好<br>平衡点（BEP）：P=R时取值，值越大性能越优</p><p><strong><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>F</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">F_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></strong>=<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mn>2</mn><mo>∗</mo><mi>P</mi><mo>∗</mo><mi>R</mi></mrow><mrow><mi>P</mi><mo>+</mo><mi>R</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{2*P*R}{P+R}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2757em;vertical-align:-0.4033em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8723em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">P</span><span class="mbin mtight">+</span><span class="mord mathnormal mtight" style="margin-right:0.00773em;">R</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span><span class="mbin mtight">∗</span><span class="mord mathnormal mtight" style="margin-right:0.13889em;">P</span><span class="mbin mtight">∗</span><span class="mord mathnormal mtight" style="margin-right:0.00773em;">R</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4033em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>=<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mn>2</mn><mo>∗</mo><mi>T</mi><mi>P</mi></mrow><mrow><mi>m</mi><mo>+</mo><mi>T</mi><mi>P</mi><mo>−</mo><mi>T</mi><mi>N</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{2*TP}{m+TP-TN}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2757em;vertical-align:-0.4033em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8723em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mbin mtight">+</span><span class="mord mathnormal mtight" style="margin-right:0.13889em;">TP</span><span class="mbin mtight">−</span><span class="mord mathnormal mtight" style="margin-right:0.10903em;">TN</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span><span class="mbin mtight">∗</span><span class="mord mathnormal mtight" style="margin-right:0.13889em;">TP</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4033em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span><br><em><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>F</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">F_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>:基于查准率和查全率的调和平均</em><br><em><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>F</mi><mi>β</mi></msub></mrow><annotation encoding="application/x-tex">F_β</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05278em;">β</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span>：加权调和平均</em></p><p><strong>ROC</strong>：受试者工作特性<br>横轴：假正例率（FPR）=<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi>F</mi><mi>P</mi></mrow><mrow><mi>T</mi><mi>N</mi><mo>+</mo><mi>F</mi><mi>P</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{FP}{TN+FP}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2757em;vertical-align:-0.4033em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8723em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">TN</span><span class="mbin mtight">+</span><span class="mord mathnormal mtight" style="margin-right:0.13889em;">FP</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">FP</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4033em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span><br>纵轴：真正例率（TPR）=<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi>T</mi><mi>P</mi></mrow><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>N</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{TP}{TP+FN}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2757em;vertical-align:-0.4033em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8723em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">TP</span><span class="mbin mtight">+</span><span class="mord mathnormal mtight" style="margin-right:0.10903em;">FN</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">TP</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4033em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span><br>用面积大小（即<strong>AUC</strong>）评价性能好坏</p><p><strong>代价敏感错误率与代价曲线</strong><br>代价敏感错误率<br><img src="/image/%E4%BB%A3%E4%BB%B7%E6%95%8F%E6%84%9F%E9%94%99%E8%AF%AF%E7%8E%87.png" alt=""><br>代价曲线图（cost curve）<br>横轴：正例概率代价<br>纵轴：归一化代价<br><a href="https://blog.csdn.net/lg201601/article/details/106200046">具体见</a></p><h3 id="比较检验">比较检验</h3><p><strong>交叉验证t检验</strong><a href="https://blog.csdn.net/orDream/article/details/123409819">解释</a><br><strong>McNemar检验</strong> <a href="https://blog.csdn.net/orDream/article/details/122540099">解释</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>知识库</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ArcGIS|实验问题</title>
    <link href="/2024/10/10/ArcGIS%E5%AE%9E%E9%AA%8C%E9%97%AE%E9%A2%98/"/>
    <url>/2024/10/10/ArcGIS%E5%AE%9E%E9%AA%8C%E9%97%AE%E9%A2%98/</url>
    
    <content type="html"><![CDATA[<h1>使用ArcGIS处理数据</h1><h2 id="基本步骤">基本步骤</h2><p>1、将需要的数据下载到本地<br>（例：DEM栅格文件）<br>2、栅格文件要素值提取至点，生成包含新的元素的点要素文件</p><h2 id="可能出现的问题">可能出现的问题</h2><p>1、属性表不见了（双屏操作，在边边上拖过来就行了）<br>2、栅格文件导入时闪退（可能与原有点要素文件不兼容，建议更换文件）</p>]]></content>
    
    
    
    <tags>
      
      <tag>知识库</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
