<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>滑坡实验|特征因子选择</title>
    <link href="/2024/11/12/%E6%BB%91%E5%9D%A1%E5%AE%9E%E9%AA%8C%E7%89%B9%E5%BE%81%E5%9B%A0%E5%AD%90%E9%80%89%E6%8B%A9/"/>
    <url>/2024/11/12/%E6%BB%91%E5%9D%A1%E5%AE%9E%E9%AA%8C%E7%89%B9%E5%BE%81%E5%9B%A0%E5%AD%90%E9%80%89%E6%8B%A9/</url>
    
    <content type="html"><![CDATA[<h1>VIF</h1><p><a href="https://mp.weixin.qq.com/s/CnJiMmO2DzkX6E2E2rnKwQ">去除多重共线性的5种方法</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">from</span> statsmodels.regression.linear_model <span class="hljs-keyword">import</span> OLS<br><span class="hljs-keyword">import</span> statsmodels.api <span class="hljs-keyword">as</span> sm<br><span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LinearRegression<br><span class="hljs-keyword">from</span> statsmodels.stats.outliers_influence <span class="hljs-keyword">import</span> variance_inflation_factor<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">from</span> math <span class="hljs-keyword">import</span> pi<br><br><span class="hljs-comment"># 使用 statsmodels 计算 VIF，基于相关系数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">variance_inflation_factor_sm_corr</span>(<span class="hljs-params">exog, exog_idx</span>):<br>    k_vars = exog.shape[<span class="hljs-number">1</span>]  <span class="hljs-comment"># 特征总数</span><br>    exog = np.asarray(exog)<br>    x_i = exog[:, exog_idx]  <span class="hljs-comment"># 当前特征</span><br>    mask = np.arange(k_vars) != exog_idx  <span class="hljs-comment"># 掩码，选择非当前特征</span><br>    x_noti = exog[:, mask]  <span class="hljs-comment"># 其他特征</span><br>    r_squared_i = OLS(x_i, sm.add_constant(x_noti)).fit().rsquared  <span class="hljs-comment"># R²</span><br>    vif = <span class="hljs-number">1.</span> / (<span class="hljs-number">1.</span> - r_squared_i)  <span class="hljs-comment"># VIF计算</span><br>    <span class="hljs-keyword">return</span> vif<br><br><br><span class="hljs-comment"># 使用 sklearn 计算 VIF</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">variance_inflation_factor_sklearn</span>(<span class="hljs-params">exog, exog_idx</span>):<br>    k_vars = exog.shape[<span class="hljs-number">1</span>]<br>    exog = np.asarray(exog)<br>    x_i = exog[:, exog_idx]<br>    mask = np.arange(k_vars) != exog_idx<br>    x_noti = exog[:, mask]<br>    r_squared_i = LinearRegression(fit_intercept=<span class="hljs-literal">True</span>).fit(x_noti, x_i).score(x_noti, x_i)<br>    vif = <span class="hljs-number">1.</span> / (<span class="hljs-number">1.</span> - r_squared_i)<br>    <span class="hljs-keyword">return</span> vif<br><br><br><span class="hljs-comment"># 计算数据集的 VIF，支持多种计算方法</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">calc_vif</span>(<span class="hljs-params">df, method=<span class="hljs-string">&#x27;all&#x27;</span></span>):<br>    vif_data = pd.DataFrame()  <span class="hljs-comment"># 保存 VIF 数据</span><br>    vif_data[<span class="hljs-string">&quot;feature&quot;</span>] = df.columns<br><br>    <span class="hljs-keyword">if</span> method == <span class="hljs-string">&#x27;sm_corr&#x27;</span>:  <span class="hljs-comment"># 使用 statsmodels 相关系数方法</span><br>        vif_data[<span class="hljs-string">&quot;VIF&quot;</span>] = [variance_inflation_factor_sm_corr(df.values, i)<br>                           <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(df.columns))]<br>    <span class="hljs-keyword">elif</span> method == <span class="hljs-string">&#x27;sklearn&#x27;</span>:  <span class="hljs-comment"># 使用 sklearn</span><br>        vif_data[<span class="hljs-string">&quot;VIF&quot;</span>] = [variance_inflation_factor_sklearn(df.values, i)<br>                           <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(df.columns))]<br>    <span class="hljs-keyword">elif</span> method == <span class="hljs-string">&#x27;sm&#x27;</span>:  <span class="hljs-comment"># 使用 statsmodels OLS</span><br>        vif_data[<span class="hljs-string">&quot;VIF&quot;</span>] = [variance_inflation_factor(df.values, i)<br>                           <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(df.columns))]<br>    <span class="hljs-keyword">else</span>:  <span class="hljs-comment"># 默认计算所有方法的 VIF</span><br>        vif_data[<span class="hljs-string">&quot;VIF_sm_corr&quot;</span>] = [variance_inflation_factor_sm_corr(df.values, i)<br>                                   <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(df.columns))]<br>        vif_data[<span class="hljs-string">&quot;VIF_sklearn&quot;</span>] = [variance_inflation_factor_sklearn(df.values, i)<br>                                   <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(df.columns))]<br>        vif_data[<span class="hljs-string">&quot;VIF_sm&quot;</span>] = [variance_inflation_factor(df.values, i)<br>                              <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(df.columns))]<br><br>    <span class="hljs-keyword">return</span> vif_data<br><br><br><span class="hljs-comment"># 读取数据</span><br>df1 = pd.read_excel(<span class="hljs-string">&#x27;cluster0.xlsx&#x27;</span>)<br>df2 = pd.read_excel(<span class="hljs-string">&#x27;cluster1.xlsx&#x27;</span>)<br><br><br>df = [df1, df2]<br>i = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> df <span class="hljs-keyword">in</span> df:<br>    num_df = df.drop(columns=[<span class="hljs-string">&#x27;class&#x27;</span>, <span class="hljs-string">&#x27;cluster&#x27;</span>, <span class="hljs-string">&#x27;X&#x27;</span>, <span class="hljs-string">&#x27;Y&#x27;</span>])<br>    result = calc_vif(num_df, method=<span class="hljs-string">&#x27;all&#x27;</span>)<br>    <span class="hljs-built_in">print</span>(result)<br><br><br>    <span class="hljs-comment"># 假设 VIF 结果已保存在 result DataFrame 中，选择 &quot;feature&quot; 和一个 VIF 列（如 &quot;VIF_sm_corr&quot;）用于绘制</span><br>    vif_data = result[[<span class="hljs-string">&#x27;feature&#x27;</span>, <span class="hljs-string">&#x27;VIF_sm_corr&#x27;</span>]]<br><br>    <span class="hljs-comment"># 准备数据</span><br>    labels = vif_data[<span class="hljs-string">&#x27;feature&#x27;</span>].values  <span class="hljs-comment"># 特征名称</span><br>    stats = vif_data[<span class="hljs-string">&#x27;VIF_sm_corr&#x27;</span>].values  <span class="hljs-comment"># VIF 值</span><br><br>    <span class="hljs-comment"># 雷达图设置</span><br>    num_vars = <span class="hljs-built_in">len</span>(labels)<br><br>    <span class="hljs-comment"># 计算每个特征的角度</span><br>    angles = np.linspace(<span class="hljs-number">0</span>, <span class="hljs-number">2</span> * np.pi, num_vars, endpoint=<span class="hljs-literal">False</span>).tolist()<br><br>    <span class="hljs-comment"># 为了使雷达图闭合，连接起点</span><br>    stats = np.concatenate((stats, [stats[<span class="hljs-number">0</span>]]))<br>    angles += angles[:<span class="hljs-number">1</span>]<br><br>    <span class="hljs-comment"># 创建雷达图</span><br>    fig, ax = plt.subplots(figsize=(<span class="hljs-number">8</span>, <span class="hljs-number">8</span>), subplot_kw=<span class="hljs-built_in">dict</span>(polar=<span class="hljs-literal">True</span>))<br><br>    <span class="hljs-comment"># 绘制填充区域和边界线</span><br>    ax.fill(angles, stats, color=<span class="hljs-string">&#x27;red&#x27;</span>, alpha=<span class="hljs-number">0.25</span>)<br>    ax.plot(angles, stats, color=<span class="hljs-string">&#x27;red&#x27;</span>, linewidth=<span class="hljs-number">2</span>)<br><br>    <span class="hljs-comment"># 设置特征名称（每个轴的标签）</span><br>    ax.set_xticks(angles[:-<span class="hljs-number">1</span>])<br>    ax.set_xticklabels(labels, fontsize=<span class="hljs-number">12</span>)<br><br>    <span class="hljs-comment"># 设置径向坐标的标签（显示每个VIF值的大小）</span><br>    ax.set_yticks(<span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-built_in">int</span>(<span class="hljs-built_in">max</span>(stats)) + <span class="hljs-number">2</span>))<br>    ax.set_yticklabels(<span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-built_in">int</span>(<span class="hljs-built_in">max</span>(stats)) + <span class="hljs-number">2</span>), color=<span class="hljs-string">&quot;grey&quot;</span>, size=<span class="hljs-number">10</span>)<br><br>    <span class="hljs-comment"># 添加标题</span><br>    plt.title(<span class="hljs-string">f&#x27;VIF cluster<span class="hljs-subst">&#123;i&#125;</span>&#x27;</span>, size=<span class="hljs-number">15</span>, color=<span class="hljs-string">&#x27;black&#x27;</span>, y=<span class="hljs-number">1.1</span>)<br>    i +=<span class="hljs-number">1</span><br><span class="hljs-comment"># 显示图形</span><br>plt.show()<br><br></code></pre></td></tr></table></figure><h1>Boruta算法</h1><p><a href="https://mp.weixin.qq.com/s/UktBNhErmS4HdfKsEDYcww">机器学习|特征选择利器：森林之神(Boruta)</a><br><a href="https://mp.weixin.qq.com/s/wX1B_2wxp3kUVjQKQE5XLw">特征选择：基于随机森林的Boruta算法应用</a><br><a href="https://mp.weixin.qq.com/s/rp1qDlACzvvtdQbdEL6Lbg">特征选择：Lasso和Boruta算法的结合应用</a></p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br></pre></td><td class="code"><pre><code class="hljs routeros">import seaborn as sns<br>import pandas as pd<br>import numpy as np<br>import matplotlib.pyplot as plt<br><span class="hljs-keyword">from</span> sklearn.model_selection import train_test_split<br><br><br>df1 = pd.read_excel(<span class="hljs-string">&#x27;cluster1.xlsx&#x27;</span>)<br>df0 = pd.read_excel(<span class="hljs-string">&#x27;cluster0.xlsx&#x27;</span>)<br>df = [df0 , df1]<br><span class="hljs-attribute">j</span>=0<br><span class="hljs-keyword">for</span> df <span class="hljs-keyword">in</span> df:<br>    # 划分特征和目标变量<br>    X = df.drop([<span class="hljs-string">&#x27;class&#x27;</span>, <span class="hljs-string">&#x27;cluster&#x27;</span>, <span class="hljs-string">&#x27;X&#x27;</span>, <span class="hljs-string">&#x27;Y&#x27;</span>], <span class="hljs-attribute">axis</span>=1)<br>    y = df[<span class="hljs-string">&#x27;class&#x27;</span>]<br>    # 划分训练集和测试集<br>    X_train, X_test, y_train, y_test = train_test_split(X, y, <span class="hljs-attribute">test_size</span>=0.2,<br>                                                        <span class="hljs-attribute">random_state</span>=42, <span class="hljs-attribute">stratify</span>=df[<span class="hljs-string">&#x27;class&#x27;</span>])<br>    df.head()<br><br><br>    <span class="hljs-keyword">from</span> sklearn.ensemble import RandomForestClassifier<br>    <span class="hljs-keyword">from</span> boruta import BorutaPy<br>    <span class="hljs-string">&#x27;&#x27;</span><span class="hljs-string">&#x27;</span><br><span class="hljs-string">    多次运行Boruta算法以评估特征排名稳定性</span><br><span class="hljs-string">    &#x27;</span><span class="hljs-string">&#x27;&#x27;</span><br>    # 初始化随机森林模型<br>    rf = RandomForestClassifier(<span class="hljs-attribute">n_jobs</span>=-1, <span class="hljs-attribute">class_weight</span>=<span class="hljs-string">&#x27;balanced&#x27;</span>, <span class="hljs-attribute">max_depth</span>=5)<br><br>    # 初始化存储特征排名的 DataFrame<br>    ranking_df = pd.DataFrame(<span class="hljs-attribute">index</span>=range(1, 2), <span class="hljs-attribute">columns</span>=X_train.columns)<br>    support_df = pd.DataFrame(<span class="hljs-attribute">index</span>=range(1, 2), <span class="hljs-attribute">columns</span>=X_train.columns)<br>    # 运行 Boruta 20 次<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(1):<br>        # <span class="hljs-built_in">print</span>(f<span class="hljs-string">&quot;Iteration &#123;i + 1&#125;&quot;</span>)<br><br>        # 初始化Boruta特征选择器<br>        boruta_selector = BorutaPy(rf, <span class="hljs-attribute">n_estimators</span>=<span class="hljs-string">&#x27;auto&#x27;</span>, <span class="hljs-attribute">verbose</span>=2, <span class="hljs-attribute">random_state</span>=i, <span class="hljs-attribute">max_iter</span>=50)<br><br>        # 对训练数据进行特征选择<br>        boruta_selector.fit(X_train.values, y_train.values)<br><br>        # 获取特征排名<br>        feature_ranks = boruta_selector.ranking_<br>        feature_support = boruta_selector.support_<br><br>        # 将特征排名保存到 DataFrame 中<br>        ranking_df.loc[i + 1] = feature_ranks<br>        support_df.loc[i + 1] = feature_support<br>    ranking_df.to_csv(f<span class="hljs-string">&#x27;cluster-&#123;i&#125;-boruta-feature-ranking.csv&#x27;</span>, <span class="hljs-attribute">index</span>=<span class="hljs-literal">False</span>)<br>    support_df.to_csv(f<span class="hljs-string">&#x27;cluster-&#123;i&#125;-boruta-feature-support.csv&#x27;</span>, <span class="hljs-attribute">index</span>=<span class="hljs-literal">False</span>)<br>    #水平柱状图#<br><br>    numeric_ranking_df = ranking_df.apply(pd.to_numeric, <span class="hljs-attribute">errors</span>=<span class="hljs-string">&#x27;coerce&#x27;</span>)<br><br>    # 计算每个特征的中位数<br>    median_values = 1/numeric_ranking_df.median()<br><br>    # 根据中位数对特征进行排序<br>    sorted_median_values = median_values.sort_values(<span class="hljs-attribute">ascending</span>=<span class="hljs-literal">False</span>)  # 值越大，特征越重要<br><br>    # 创建水平柱状图<br>    plt.figure(figsize=(6, 5))<br>    sns.barplot(<span class="hljs-attribute">x</span>=sorted_median_values.values, <span class="hljs-attribute">y</span>=sorted_median_values.index, <span class="hljs-attribute">palette</span>=<span class="hljs-string">&quot;viridis&quot;</span>)<br><br>    # 设置图表标题和标签<br>    plt.title(f<span class="hljs-string">&quot;Cluster &#123;j&#125; Feature Importance by Boruta (Horizontal Bar Chart)&quot;</span>, <span class="hljs-attribute">fontsize</span>=10)<br>    plt.xlabel(<span class="hljs-string">&quot;Feature Importance (1 / Median Ranking))&quot;</span>, <span class="hljs-attribute">fontsize</span>=10)<br>    plt.ylabel(<span class="hljs-string">&quot;Features&quot;</span>, <span class="hljs-attribute">fontsize</span>=10)<br>    j = j+1<br><br><br><span class="hljs-comment"># 显示图表</span><br>plt.tight_layout()<br>plt.show()<br><br><span class="hljs-string">&#x27;&#x27;</span><span class="hljs-string">&#x27;</span><br><span class="hljs-string">可视化排名稳定性</span><br><span class="hljs-string">&#x27;</span><span class="hljs-string">&#x27;&#x27;</span><br><span class="hljs-comment"># # 确保数据集中只有数值列</span><br><span class="hljs-comment"># numeric_ranking_df = ranking_df.apply(pd.to_numeric, errors=&#x27;coerce&#x27;)</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment"># # 计算每个特征的中位数</span><br><span class="hljs-comment"># median_values = numeric_ranking_df.median()</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment"># # 根据中位数对列进行排序</span><br><span class="hljs-comment"># sorted_columns = median_values.sort_values().index</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment"># # 设置绘图风格</span><br><span class="hljs-comment"># plt.figure(figsize=(15, 8))</span><br><span class="hljs-comment"># sns.set(style=&quot;whitegrid&quot;)</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment"># # 绘制箱线图</span><br><span class="hljs-comment"># sns.boxplot(data=numeric_ranking_df[sorted_columns], palette=&quot;Greens&quot;)</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment"># plt.xticks(rotation=90)</span><br><span class="hljs-comment"># plt.title(&quot;Sorted Feature Ranking Distribution by Boruta&quot;, fontsize=16)</span><br><span class="hljs-comment"># plt.xlabel(&quot;Attributes&quot;, fontsize=14)</span><br><span class="hljs-comment"># plt.ylabel(&quot;Importance&quot;, fontsize=14)</span><br><span class="hljs-comment"># plt.tight_layout()</span><br><span class="hljs-comment"># plt.show()</span><br></code></pre></td></tr></table></figure>]]></content>
    
    
    
    <tags>
      
      <tag>代码</tag>
      
      <tag>算法</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>机器学习|融合模型</title>
    <link href="/2024/11/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%9E%8D%E5%90%88%E6%A8%A1%E5%9E%8B/"/>
    <url>/2024/11/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%9E%8D%E5%90%88%E6%A8%A1%E5%9E%8B/</url>
    
    <content type="html"><![CDATA[<p><a href="https://mp.weixin.qq.com/s?__biz=MzU2MjczMTU0OA==&amp;mid=2247486230&amp;idx=1&amp;sn=9955e586d7dca0c6bbda12d2784e32fa&amp;chksm=fc644465cb13cd73e347650c1b8643760d9c605215a633f6d357a02240f2777e7ec36429a159&amp;token=900428772&amp;lang=zh_CN&amp;scene=21#wechat_redirect">模型竞赛大杀器-融合模型(stacking)</a><br><a href="https://mp.weixin.qq.com/s/QmU7LAFub8BGB3zUNJAo8A">机器学习融合模型stacking14条经验总结和5个成功案例(互联网最全，硬核收藏)</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>文献笔记|基于栅格单元的区域滑坡易发性评价关键影响因素研究</title>
    <link href="/2024/11/07/%E6%96%87%E7%8C%AE%E7%AC%94%E8%AE%B0%E5%9F%BA%E4%BA%8E%E6%A0%85%E6%A0%BC%E5%8D%95%E5%85%83%E7%9A%84%E5%8C%BA%E5%9F%9F%E6%BB%91%E5%9D%A1%E6%98%93%E5%8F%91%E6%80%A7%E8%AF%84%E4%BB%B7%E5%85%B3%E9%94%AE%E5%BD%B1%E5%93%8D%E5%9B%A0%E7%B4%A0%E7%A0%94%E7%A9%B6/"/>
    <url>/2024/11/07/%E6%96%87%E7%8C%AE%E7%AC%94%E8%AE%B0%E5%9F%BA%E4%BA%8E%E6%A0%85%E6%A0%BC%E5%8D%95%E5%85%83%E7%9A%84%E5%8C%BA%E5%9F%9F%E6%BB%91%E5%9D%A1%E6%98%93%E5%8F%91%E6%80%A7%E8%AF%84%E4%BB%B7%E5%85%B3%E9%94%AE%E5%BD%B1%E5%93%8D%E5%9B%A0%E7%B4%A0%E7%A0%94%E7%A9%B6/</url>
    
    <content type="html"><![CDATA[<p><a href="https://mp.weixin.qq.com/s/jmdc68hLV6WO4mGBj683AA">推荐链接</a><br><a href="https://www.sciencedirect.com/science/article/pii/S1674987124000069">原文链接</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>文献</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>滑坡实验|聚类</title>
    <link href="/2024/11/06/%E6%BB%91%E5%9D%A1%E5%AE%9E%E9%AA%8C%E8%81%9A%E7%B1%BB/"/>
    <url>/2024/11/06/%E6%BB%91%E5%9D%A1%E5%AE%9E%E9%AA%8C%E8%81%9A%E7%B1%BB/</url>
    
    <content type="html"><![CDATA[<h2 id="基础知识">基础知识</h2><p><strong>聚类</strong><br><a href="https://mp.weixin.qq.com/s/o09VSfP07Y0uymGKzYUQIw">7种常见的聚类算法原理及代码实现</a></p><p><strong>空间聚类</strong><br><a href="https://mp.weixin.qq.com/s/XsKeP1etV_void_bociznw">【方法解读】Nat Methods：如何选择最佳的空间聚类方法</a><br><a href="https://zhuanlan.zhihu.com/p/401518862">新版白话空间统计（39）：空间聚类的特点与分类</a><br>经纬度数值越大权重越大————&gt;<strong>归一化</strong>（Normalization：数据变成（0，1）或者之间的小数）/<strong>标准化</strong>（Standardization：使每个特征中的数值平均变为0、标准差变为1）</p><p>数据归一化</p><h2 id="实例">实例</h2><p><a href="https://mp.weixin.qq.com/s/-ikGHsL0G0LWQrRe3w2qMw">Plotly+Pandas+Sklearn：实现用户聚类分群！</a><br><a href="https://mp.weixin.qq.com/s/s6147o2ZlPrctM0KcNIjlQ">顶刊论文复现：一种网络视角下的空间聚类算法的实现与应用-以城市道路分区为例</a></p><h2 id="其他">其他</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">from</span> sklearn.cluster <span class="hljs-keyword">import</span> KMeans, AgglomerativeClustering, DBSCAN<br><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> calinski_harabasz_score<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> StandardScaler<br><br><br><span class="hljs-comment"># 加载数据</span><br>df = pd.read_excel(<span class="hljs-string">&quot;yibingforagnes.xlsx&quot;</span>)  <span class="hljs-comment"># 替换为实际数据路径</span><br><span class="hljs-comment"># 删除或填充整个data中的NaN值（以均值填充为例）</span><br>df = df.fillna(df.mean())<br><span class="hljs-comment"># 定义features</span><br>data = df.drop(columns=[<span class="hljs-string">&#x27;class&#x27;</span>]).values<br><span class="hljs-comment"># data = df.drop(columns=[&#x27;class&#x27;, &#x27;X&#x27;, &#x27;Y&#x27;]).values</span><br><span class="hljs-comment"># 标准化数据</span><br>scaler = StandardScaler()<br>data = scaler.fit_transform(data)<br><span class="hljs-comment"># 定义簇数范围</span><br>n_clusters_range = <span class="hljs-built_in">range</span>(<span class="hljs-number">2</span>, <span class="hljs-number">15</span>)<br><br><span class="hljs-comment"># 存储各簇数下的CH指数</span><br>ch_scores_kmeans = []<br>ch_scores_agg = []<br><br><span class="hljs-comment"># 距离度量方法列表</span><br>linkage_methods = [<span class="hljs-string">&#x27;single&#x27;</span>, <span class="hljs-string">&#x27;complete&#x27;</span>, <span class="hljs-string">&#x27;average&#x27;</span>]<br>ch_scores = &#123;method: [] <span class="hljs-keyword">for</span> method <span class="hljs-keyword">in</span> linkage_methods&#125;<br><br><span class="hljs-comment"># 计算不同距离度量方法下的CH指数</span><br>cluster_range = <span class="hljs-built_in">range</span>(<span class="hljs-number">2</span>, <span class="hljs-number">15</span>)<br><span class="hljs-keyword">for</span> method <span class="hljs-keyword">in</span> linkage_methods:<br>    <span class="hljs-keyword">for</span> n_clusters <span class="hljs-keyword">in</span> cluster_range:<br>        agnes = AgglomerativeClustering(n_clusters=n_clusters, linkage=method)<br>        labels = agnes.fit_predict(data)<br>        score = calinski_harabasz_score(data, labels)<br>        ch_scores[method].append(score)<br><span class="hljs-comment"># K-Means聚类</span><br><span class="hljs-keyword">for</span> n_clusters <span class="hljs-keyword">in</span> n_clusters_range:<br>    kmeans = KMeans(n_clusters=n_clusters, random_state=<span class="hljs-number">0</span>)<br>    labels = kmeans.fit_predict(data)<br>    ch_score = calinski_harabasz_score(data, labels)<br>    ch_scores_kmeans.append(ch_score)<br><br><span class="hljs-comment"># 层次聚类</span><br><span class="hljs-keyword">for</span> n_clusters <span class="hljs-keyword">in</span> n_clusters_range:<br>    agg = AgglomerativeClustering(n_clusters=n_clusters)<br>    labels = agg.fit_predict(data)<br>    ch_score = calinski_harabasz_score(data, labels)<br>    ch_scores_agg.append(ch_score)<br><br><span class="hljs-comment"># 绘制CH指数图</span><br>plt.figure(figsize=(<span class="hljs-number">10</span>, <span class="hljs-number">6</span>))<br>plt.plot(n_clusters_range, ch_scores_kmeans, label=<span class="hljs-string">&#x27;K-Means&#x27;</span>, marker=<span class="hljs-string">&#x27;o&#x27;</span>)<br>plt.plot(n_clusters_range, ch_scores_agg, label=<span class="hljs-string">&#x27;Agglomerative Clustering&#x27;</span>, marker=<span class="hljs-string">&#x27;s&#x27;</span>)<br><span class="hljs-keyword">for</span> method, scores <span class="hljs-keyword">in</span> ch_scores.items():<br>    plt.plot(cluster_range, scores, marker=<span class="hljs-string">&#x27;o&#x27;</span>, label=<span class="hljs-string">f&#x27;Distance: <span class="hljs-subst">&#123;method&#125;</span>&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;Number of Clusters&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Calinski-Harabasz Index&#x27;</span>)<br>plt.title(<span class="hljs-string">&#x27;CH Index for Different Clustering Methods&#x27;</span>)<br>plt.legend()<br><span class="hljs-comment"># plt.show()</span><br><br><br>n_clusters = <span class="hljs-number">2</span><br>alg = KMeans(n_clusters=n_clusters, random_state=<span class="hljs-number">0</span>)<br><span class="hljs-comment"># alg = AgglomerativeClustering(n_clusters=n_clusters)</span><br>df[<span class="hljs-string">&#x27;cluster&#x27;</span>] = alg.fit_predict(data)<br><br><span class="hljs-comment"># 导出含有聚类标签的 Excel 文件</span><br>output_file = <span class="hljs-string">&#x27;clusters.xlsx&#x27;</span><br><span class="hljs-keyword">with</span> pd.ExcelWriter(output_file) <span class="hljs-keyword">as</span> writer:<br>    df.to_excel(writer, sheet_name=<span class="hljs-string">&#x27;Clustered_Data&#x27;</span>, index=<span class="hljs-literal">False</span>)<br><br><span class="hljs-comment"># 绘制聚类图</span><br>plt.figure(figsize=(<span class="hljs-number">10</span>, <span class="hljs-number">8</span>))<br><span class="hljs-keyword">for</span> cluster_id <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n_clusters):<br>    cluster_data = df[df[<span class="hljs-string">&#x27;cluster&#x27;</span>] == cluster_id]<br>    plt.scatter(cluster_data[<span class="hljs-string">&#x27;X&#x27;</span>], cluster_data[<span class="hljs-string">&#x27;Y&#x27;</span>], label=<span class="hljs-string">f&#x27;Cluster <span class="hljs-subst">&#123;cluster_id&#125;</span>&#x27;</span>, s=<span class="hljs-number">50</span>)<br><br><span class="hljs-comment"># 添加图例和标题</span><br>plt.xlabel(<span class="hljs-string">&#x27;X Coordinate&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Y Coordinate&#x27;</span>)<br>plt.title(<span class="hljs-string">&#x27;Geographic Points with Cluster Labels&#x27;</span>)<br>plt.legend()<br>plt.grid(<span class="hljs-literal">True</span>)<br>plt.show()<br><br><br></code></pre></td></tr></table></figure>]]></content>
    
    
    
    <tags>
      
      <tag>代码</tag>
      
      <tag>算法</tag>
      
      <tag>聚类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ArcGIS|gis距离分析和地貌因子计算</title>
    <link href="/2024/11/05/ArcGISgis%E8%B7%9D%E7%A6%BB%E5%88%86%E6%9E%90/"/>
    <url>/2024/11/05/ArcGISgis%E8%B7%9D%E7%A6%BB%E5%88%86%E6%9E%90/</url>
    
    <content type="html"><![CDATA[<h1>道路/河流/断层 POI</h1><h2 id="计算欧式距离时，单位问题">计算欧式距离时，单位问题</h2><p>将输入栅格数据（通常来自osm）坐标系转化<br><a href="https://www.cnblogs.com/bltstop/p/18090135">osm数据说明</a><br>ArcToolbox-数据管理工具-投影与变换-要素-投影<br><a href="https://blog.csdn.net/weixin_45481399/article/details/142679030">解决方法</a></p><h2 id="距离计算">距离计算</h2><p>空间分析工具-距离-欧式距离<br>可在环境中选择计算范围<br><a href="https://blog.csdn.net/weixin_45481399/article/details/142685036"></a></p><h2 id="值提取至点，点要素图层属性表不显示值？">值提取至点，点要素图层属性表不显示值？</h2><p>用多值提取至点</p><p><a href="https://www.jianshu.com/p/3bca53f2334f">gmt文件转化为shp文件</a><br>ogr2ogr -lco encoding=UTF-8 -f “ESRI Shapefile” C:\temp\cnFaults.shp C:\temp\CN-faults-name.gmt</p><h1>地貌</h1><p><a href="https://developer.aliyun.com/article/1350615#:~:text=%E5%9C%B0%E5%BD%A2%E8%B5%B7%E4%BC%8F%E5%BA%A6%E3%80%81%E5%9C%B0%E5%BD%A2"></a></p>]]></content>
    
    
    
    <tags>
      
      <tag>gis</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>滑坡实验|stacking集成</title>
    <link href="/2024/11/03/%E6%BB%91%E5%9D%A1%E5%AE%9E%E9%AA%8Cstacking%E9%9B%86%E6%88%90/"/>
    <url>/2024/11/03/%E6%BB%91%E5%9D%A1%E5%AE%9E%E9%AA%8Cstacking%E9%9B%86%E6%88%90/</url>
    
    <content type="html"><![CDATA[<p><a href="https://blog.csdn.net/ueke1/article/details/137190677">代码基于博客</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> matplotlib <span class="hljs-keyword">as</span> mpl<br><br>mpl.rcParams[<span class="hljs-string">&#x27;font.sans-serif&#x27;</span>] = [<span class="hljs-string">u&#x27;simHei&#x27;</span>]<br><br><span class="hljs-keyword">import</span> time<br><br><span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> LabelEncoder<br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<br><span class="hljs-keyword">from</span> sklearn.neighbors <span class="hljs-keyword">import</span> KNeighborsClassifier<br><span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> RandomForestClassifier, GradientBoostingClassifier<br><span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LogisticRegression<br><span class="hljs-keyword">from</span> sklearn.pipeline <span class="hljs-keyword">import</span> Pipeline<br><span class="hljs-keyword">from</span> sklearn.svm <span class="hljs-keyword">import</span> SVC<br><span class="hljs-keyword">from</span> mlxtend.classifier <span class="hljs-keyword">import</span> StackingClassifier<br><span class="hljs-keyword">from</span> sklearn.neural_network <span class="hljs-keyword">import</span> MLPClassifier<br><br><br><span class="hljs-keyword">from</span> mlxtend.feature_selection <span class="hljs-keyword">import</span> ColumnSelector<br><br><span class="hljs-comment">###1、读取数据</span><br><span class="hljs-comment"># datas = pd.read_csv(&#x27;iris.data&#x27;, sep=&#x27;,&#x27;, header=None, names=[&#x27;X1&#x27;, &#x27;X2&#x27;, &#x27;X3&#x27;, &#x27;X4&#x27;, &#x27;Y&#x27;])</span><br><span class="hljs-comment"># print(datas.head())</span><br><span class="hljs-comment"># print(datas.info())</span><br><br><span class="hljs-comment">### 2、数据清洗</span><br><br><span class="hljs-comment">### 3、获取特征属性X和目标属性Y</span><br><span class="hljs-comment"># X = datas.iloc[:, :-1]</span><br><span class="hljs-comment"># Y = datas.iloc[:, -1]</span><br><span class="hljs-comment"># print(X.shape)</span><br><span class="hljs-comment"># print(Y.shape)</span><br><span class="hljs-comment"># print(Y.value_counts())  ##看下目标属性的值</span><br><br>df_X = pd.read_excel(<span class="hljs-string">&#x27;DataX.xlsx&#x27;</span>)<br>feature_names = df_X.columns.tolist()  <span class="hljs-comment"># 将文件列名转换为Python列表</span><br><br>X = df_X.values  <span class="hljs-comment"># 输入特征，返回给定DataFrame的Numpy表现形式</span><br><br>Y = pd.DataFrame(pd.read_excel(<span class="hljs-string">&#x27;DataY.xlsx&#x27;</span>)).values.ravel()  <span class="hljs-comment"># 目标变量</span><br><br><span class="hljs-comment"># LabelEncoder  0,1, 2</span><br>labelencoder = LabelEncoder()<br><span class="hljs-comment"># print(Y.ravel())</span><br>Y = labelencoder.fit_transform(Y)<br><span class="hljs-comment"># print(Y)</span><br><br><span class="hljs-comment">### 4、分割数据集</span><br>x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=<span class="hljs-number">0.2</span>, random_state=<span class="hljs-number">28</span>)<br><span class="hljs-comment"># print(x_train.shape)</span><br><span class="hljs-comment"># print(y_train.shape)</span><br><br><span class="hljs-comment">### 5、特征工程</span><br><br><span class="hljs-comment">### 模型构建</span><br><span class="hljs-comment"># a、构造基学习器 knn、RF、softmax、GBDT。。。。</span><br>knn = KNeighborsClassifier(n_neighbors=<span class="hljs-number">7</span>)<br><span class="hljs-comment"># softmax = LogisticRegression(C=0.1, solver=&#x27;lbfgs&#x27;, multi_class=&#x27;multinomial&#x27;, fit_intercept=False)</span><br><span class="hljs-comment"># gbdt = GradientBoostingClassifier(learning_rate=0.1, n_estimators=100, max_depth=3)</span><br><span class="hljs-comment"># rf = RandomForestClassifier(max_depth=5, n_estimators=150)</span><br>svm = SVC(<br>    C=<span class="hljs-number">1.0</span>,<br>    kernel=<span class="hljs-string">&#x27;rbf&#x27;</span>,<br>    gamma=<span class="hljs-string">&#x27;scale&#x27;</span>,<br>    probability=<span class="hljs-literal">True</span><br>)<br>bpnn = MLPClassifier(hidden_layer_sizes=(<span class="hljs-number">50</span>,), max_iter=<span class="hljs-number">300</span>)<br><br><span class="hljs-comment"># b、元学习器</span><br>lr = LogisticRegression(C=<span class="hljs-number">0.1</span>, solver=<span class="hljs-string">&#x27;lbfgs&#x27;</span>, multi_class=<span class="hljs-string">&#x27;multinomial&#x27;</span>)<br><br><span class="hljs-comment">### stacking学习器</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">1、最基本的使用方法，用前面基学习器的输出作为元学习器的输入</span><br><span class="hljs-string">2、使用基学习器的输出类别的概率值作为元学习器输入，use_probas=True,若average_probas=True，那么这些基分类器对每一个类别产生的概率进行平均，否者直接拼接</span><br><span class="hljs-string"> classifier1  = [0.2,0.5,0.3]</span><br><span class="hljs-string"> classifier2  = [0.3,0.3,0.4]</span><br><span class="hljs-string">  average_probas=True: [0.25,0.4,0.35]</span><br><span class="hljs-string">  average_probas=Flase: [0.2,0.5,0.3,0.3,0.3,0.4]</span><br><span class="hljs-string"></span><br><span class="hljs-string">3、对训练集中的特征维度进行操作，每次训练不同的基学习器的时候用不同的特征，比如我再训练KNN的时候只用前两个特征，训练RF的时候用其他的几个特征</span><br><span class="hljs-string">    通过pipline来实现</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">classifiers, 基学习器</span><br><span class="hljs-string">meta_classifier, 元学习器</span><br><span class="hljs-string">use_probas=False, </span><br><span class="hljs-string">drop_last_proba=False,</span><br><span class="hljs-string">average_probas=False, </span><br><span class="hljs-string">verbose=0,</span><br><span class="hljs-string">use_features_in_secondary=False,</span><br><span class="hljs-string">store_train_meta_features=False,</span><br><span class="hljs-string">use_clones=True</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><br><span class="hljs-comment">###方式一</span><br>stacking01 = StackingClassifier(meta_classifier=lr,<br>                                <span class="hljs-comment"># classifiers=[knn, softmax, gbdt, rf]</span><br>                                classifiers=[knn, svm, bpnn]<br>                                )<br><br><span class="hljs-comment">###方式二</span><br>stacking02 = StackingClassifier(meta_classifier=lr,<br>                                <span class="hljs-comment"># classifiers=[knn, softmax, gbdt, rf],</span><br>                                classifiers=[knn, svm, bpnn],<br>                                use_probas=<span class="hljs-literal">True</span>,<br>                                average_probas=<span class="hljs-literal">False</span>)<br><br><span class="hljs-comment">###方式三</span><br><span class="hljs-comment"># 基学习器</span><br>pipe_knn = Pipeline([(<span class="hljs-string">&#x27;x&#x27;</span>, ColumnSelector([<span class="hljs-number">0</span>, <span class="hljs-number">5</span>])),<br>                     (<span class="hljs-string">&#x27;knn&#x27;</span>, knn)])<br><span class="hljs-comment"># pipe_softmax = Pipeline([(&#x27;x&#x27;, ColumnSelector([5, 10])),</span><br><span class="hljs-comment">#                          (&#x27;softmax&#x27;, softmax)])</span><br><span class="hljs-comment"># pipe_rf = Pipeline([(&#x27;x&#x27;, ColumnSelector([10, 17])),</span><br><span class="hljs-comment">#                     (&#x27;rf&#x27;, rf)])</span><br><span class="hljs-comment"># pipe_gbdt = Pipeline([(&#x27;x&#x27;, ColumnSelector([0, 15])),</span><br><span class="hljs-comment">#                       (&#x27;gbdt&#x27;, gbdt)])</span><br>pipe_svm = Pipeline([(<span class="hljs-string">&#x27;x&#x27;</span>, ColumnSelector([<span class="hljs-number">5</span>, <span class="hljs-number">11</span>])),<br>                     (<span class="hljs-string">&#x27;svm&#x27;</span>, svm)])<br>pipe_bpnn = Pipeline([(<span class="hljs-string">&#x27;x&#x27;</span>, ColumnSelector([<span class="hljs-number">0</span>, <span class="hljs-number">17</span>])),<br>                     (<span class="hljs-string">&#x27;bpnn&#x27;</span>, bpnn)])<br><span class="hljs-comment">##stacking</span><br>stacking03 = StackingClassifier(meta_classifier=lr,<br>                                <span class="hljs-comment"># classifiers=[pipe_knn, pipe_softmax, pipe_rf, pipe_gbdt]</span><br>                                classifiers=[pipe_knn, pipe_svm, pipe_bpnn]<br>                                )<br><br><span class="hljs-comment">###模型训练与比较</span><br>scores_train = []<br>scores_test = []<br>models = []<br>times = []<br><br><span class="hljs-keyword">for</span> clf, modelname <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(<br>                        <span class="hljs-comment"># [knn, softmax, gbdt, rf, stacking01, stacking02, stacking03],</span><br>                        <span class="hljs-comment">#   [&#x27;knn&#x27;, &#x27;softmax&#x27;, &#x27;gbdt&#x27;, &#x27;rf&#x27;, &#x27;stacking01&#x27;, &#x27;stacking02&#x27;, &#x27;stacking03&#x27;]):</span><br>                            [knn, svm, bpnn, stacking01, stacking02, stacking03],<br>                            [<span class="hljs-string">&#x27;knn&#x27;</span>, <span class="hljs-string">&#x27;svm&#x27;</span>, <span class="hljs-string">&#x27;bpnn&#x27;</span>, <span class="hljs-string">&#x27;stacking01&#x27;</span>, <span class="hljs-string">&#x27;stacking02&#x27;</span>, <span class="hljs-string">&#x27;stacking03&#x27;</span>]):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;start:%s&#x27;</span> % (modelname))<br>    start = time.time()<br>    clf.fit(x_train, y_train)<br>    end = time.time()<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;耗时：&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(end - start))<br>    score_train = clf.score(x_train, y_train)<br>    score_test = clf.score(x_test, y_test)<br>    scores_train.append(score_train)<br>    scores_test.append(score_test)<br>    models.append(modelname)<br>    times.append(end - start)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;scores_train:&#x27;</span>, scores_train)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;scores_test&#x27;</span>, scores_test)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;models:&#x27;</span>, models)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;开始画图----------&#x27;</span>)<br>plt.figure(num=<span class="hljs-number">1</span>)<br>plt.plot([<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>], scores_train, <span class="hljs-string">&#x27;r&#x27;</span>, label=<span class="hljs-string">u&#x27;训练集&#x27;</span>)<br>plt.plot([<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>], scores_test, <span class="hljs-string">&#x27;b&#x27;</span>, label=<span class="hljs-string">u&#x27;测试集&#x27;</span>)<br>plt.title(<span class="hljs-string">u&#x27;滑坡数据不同分类器准确率比较&#x27;</span>, fontsize=<span class="hljs-number">16</span>)<br>plt.xticks([<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>], models, rotation=<span class="hljs-number">0</span>)<br>plt.legend(loc=<span class="hljs-string">&#x27;lower left&#x27;</span>)<br>plt.figure(num=<span class="hljs-number">2</span>)<br>plt.plot([<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>], times)<br>plt.title(<span class="hljs-string">u&#x27;滑坡数据不同分类器训练时间比较&#x27;</span>, fontsize=<span class="hljs-number">16</span>)<br>plt.xticks([<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>], models, rotation=<span class="hljs-number">0</span>)<br>plt.legend(loc=<span class="hljs-string">&#x27;lower left&#x27;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure>]]></content>
    
    
    
    <tags>
      
      <tag>代码</tag>
      
      <tag>实验</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>文献笔记|顾及空间异质性和特征优选的滑坡易发性评价方法</title>
    <link href="/2024/10/28/%E6%96%87%E7%8C%AE%E7%AC%94%E8%AE%B0%E9%A1%BE%E5%8F%8A%E7%A9%BA%E9%97%B4%E5%BC%82%E8%B4%A8%E6%80%A7%E5%92%8C%E7%89%B9%E5%BE%81%E4%BC%98%E9%80%89%E7%9A%84%E6%BB%91%E5%9D%A1%E6%98%93%E5%8F%91%E6%80%A7%E8%AF%84%E4%BB%B7%E6%96%B9%E6%B3%95/"/>
    <url>/2024/10/28/%E6%96%87%E7%8C%AE%E7%AC%94%E8%AE%B0%E9%A1%BE%E5%8F%8A%E7%A9%BA%E9%97%B4%E5%BC%82%E8%B4%A8%E6%80%A7%E5%92%8C%E7%89%B9%E5%BE%81%E4%BC%98%E9%80%89%E7%9A%84%E6%BB%91%E5%9D%A1%E6%98%93%E5%8F%91%E6%80%A7%E8%AF%84%E4%BB%B7%E6%96%B9%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<p><a href="https://mp.weixin.qq.com/s/y564_uB6Li0-kzHLI4Q6lw">文献推荐链接</a></p><h2 id="论文摘要">论文摘要</h2><p>高效、精准、可靠的滑坡易发性评价方法是灾前科学预警和全面防治的关键手段。然而，传统滑坡易发性评价方法未能有效解决空间异质性和冗余特征造成的预测偏差。针对该问题，本文提出了一种顾及空间异质性和特征优选的滑坡易发性评价方法（spatial feature optimized Stacking，SF-Stacking）。该方法首先使用AGNES聚类（agglomerative nesting）将研究区全局栅格单元分成若干个局部子区，然后采用一种特征优选策略为每个子区选择最优致灾因子组合，最后采用Stacking集成技术耦合多种机器学习算法实现滑坡易发性评价。以宜宾市为研究区，基于滑坡灾害易发性分区图和统计学指标，将SF-Stacking方法与7种传统方法对比表明，SF-Stacking方法的准确性最优，稳健性最强，可解释性最高。</p><h2 id="论文创新点">论文创新点</h2><p>提出一种顾及空间异质性和特征优选的滑坡易发性评价方法（spatial feature optimized Stacking，SF-Stacking）<br>①采用聚类分析将研究区分成若干个均质子区，消除空间特征异质性造成的预测偏差；<br>②耦合多种特征选择方法筛选各个子区最优空间特征，提升特征优选的可靠性；<br>③使用集成学习算法拟合重要特征与滑坡之间的复杂非线性关系，提高模型的普适性。</p><h2 id="研究方法">研究方法</h2><p><strong>特征因子</strong>：从地形地貌、水文地质、人类工程活动、降雨等方面选取了高程、坡度、坡向、平面曲率、剖面曲率、NDVI、TWI、年降雨量、断层距离、河流距离、道路距离、POI、PGA、岩性及土地利用15个特征因子。为保证空间一致性，将所有因子的空间分辨率均重采样至30 m，借助自然间断法对连续型特征因子进行分级</p><p><strong>数据预处理</strong>：用信息量法生成非滑坡点</p><p><strong>模型构建</strong>：</p><p>1、<em><strong>特征空间异质性分区</strong></em>：根据CH指数确定簇间度量准则dmin、最优聚类簇个数，用AGNES算法对滑坡点进行分簇，基于滑坡点聚类结果构建泰森多边形，将研究区划分成不同子区</p><p>2、<em><strong>特征选择</strong></em>：在各子区中采用特征优选方法（VIF-Boruta-GeoDetector，VBG）剔除冗余特征因子<br>VIF（方差膨胀系数）：对特征因子进行多重共线性检验<br>Boruta算法:基于随机森林分类算法对特征选择<br>GeoDetector（地理探测器）:对样本空间划分和对局部线性模型的拟合来挖掘空间异质性，能够筛选出最具有区分性和解释性的特征</p><p>3、<em><strong>模型集成</strong></em>：最后基于各子区最优特征因子构建相应的Stacking集成模型（提高模型可靠性、降低过拟合风险）<br>Stacking集成学习算法：基学习器（SVM、BPNN、KNN）对训练集进行k折交叉训练（如k=5），并堆叠每个基学习器每折的输出结果作为新的训练集；然后通过每一次训练对验证集进行预测，将k次预测平均值作为新的验证集；最后将新的训练集和验证集输入到元学习器(LR)中得到最终的预测值</p><p><strong>模型精度评价</strong>：主要借助统计学指标和易发性结果图来评估模型性能<br>顾及空间异质性和特征优选的滑坡易发性评价方法（spatial feature optimized Stacking，SF-Stacking）<br>顾及空间异质性的Stacking方法（spatial Stacking，S-Stacking）<br>顾及特征优选的Stacking方法（feature optimized Stacking，F-Stacking）<br>未考虑空间异质性和冗余特征的影响，选取ACC、F1值、ROC曲线、Kappa系数、Sridevi Jadi试验概率精度（SJ）：<br>全局Stacking集成学习方法（Stacking）<br>全局机器学习方法（SVM、BPNN、KNN、LR）<br><img src="/image/%E6%B5%81%E7%A8%8B%E5%9B%BE2.jpg" alt=""></p><h2 id="主要结论">主要结论</h2><p>①空间异质性影响滑坡易发性预测精度<br>②本文提出的特征优选策略提高了模型计算效率和准确率<br>③SF-Stacking方法预测精度明显优于其他7种方法的预测精度，具有较高的稳健性、可靠性</p><p><strong>解决问题</strong>：有效解决了空间异质性和冗余特征造成的滑坡预测偏差缺陷，实现了滑坡易发性高精度制图，进而可为自然灾害风险评估、预警和防治等国家重大工程提供理论方法和技术支撑</p><p><strong>局限性</strong>：该方法需要研究区有足够的滑坡灾害点，且滑坡点与非滑坡点的比例不同可能会造成滑坡易发性评价的不确定性。</p>]]></content>
    
    
    
    <tags>
      
      <tag>文献</tag>
      
      <tag>滑坡</tag>
      
      <tag>集成学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>滑坡实验|机器学习方法</title>
    <link href="/2024/10/17/%E6%BB%91%E5%9D%A1%E5%AE%9E%E9%AA%8C%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/"/>
    <url>/2024/10/17/%E6%BB%91%E5%9D%A1%E5%AE%9E%E9%AA%8C%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h1>RF</h1><p><a href="https://blog.csdn.net/cxyxx12/article/details/134415329">代码基于此博客</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> RandomForestClassifier<br><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> confusion_matrix, classification_report, accuracy_score<br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> ShuffleSplit, cross_val_score, LeaveOneOut, StratifiedShuffleSplit<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">from</span> sklearn.tree <span class="hljs-keyword">import</span> export_graphviz<br><span class="hljs-keyword">import</span> graphviz<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> os<br><span class="hljs-comment"># os.environ[&quot;PATH&quot;] += os.pathsep + r&#x27; D:\APP\anaconda\envs\pytorch\Library\bin\graphviz\config6&#x27;</span><br><br><span class="hljs-comment"># 加载数据</span><br>df_X = pd.read_excel(<span class="hljs-string">&#x27;DataX.xlsx&#x27;</span>)<br>feature_names = df_X.columns.tolist()  <span class="hljs-comment"># 将文件列名转换为Python列表</span><br><br><br>X = df_X.values  <span class="hljs-comment"># 输入特征，返回给定DataFrame的Numpy表现形式</span><br><br>y = pd.DataFrame(pd.read_excel(<span class="hljs-string">&#x27;DataY.xlsx&#x27;</span>)).values.ravel()  <span class="hljs-comment"># 目标变量</span><br><br><span class="hljs-comment"># 创建随机森林分类器</span><br><span class="hljs-comment"># clf = RandomForestClassifier(n_estimators=100)</span><br>clf = RandomForestClassifier(class_weight=<span class="hljs-string">&#x27;balanced&#x27;</span>,<br>                             n_estimators=<span class="hljs-number">100</span>,  <span class="hljs-comment"># 树的数量</span><br>                             <span class="hljs-comment"># max_depth=10,  # 树的最大深度,设置后欠拟合</span><br>                             min_samples_split=<span class="hljs-number">2</span>,  <span class="hljs-comment"># 节点分裂所需的最小样本数</span><br>                             min_samples_leaf=<span class="hljs-number">1</span>  <span class="hljs-comment"># 叶节点的最小样本数</span><br>                             )<br><br><span class="hljs-comment"># 创建ShuffleSplit对象，用于执行自动洗牌</span><br><span class="hljs-comment"># ss = ShuffleSplit(n_splits=5, train_size=0.7, test_size=0.3, random_state=0)  # n_split:划分次数</span><br>ss = StratifiedShuffleSplit(n_splits=<span class="hljs-number">1</span>, test_size=<span class="hljs-number">0.3</span>, random_state=<span class="hljs-number">0</span>)  <span class="hljs-comment"># 实现分层、随机采样</span><br><br>accuracies = []<br>split = <span class="hljs-number">0</span><br><br><span class="hljs-comment"># 循环遍历每个拆分，并使用随机森林分类器对每个拆分进行训练和评估</span><br><span class="hljs-keyword">for</span> train_index, test_index <span class="hljs-keyword">in</span> ss.split(X, y):<br>    X_train, X_test = X[train_index], X[test_index]<br>    y_train, y_test = y[train_index], y[test_index]<br>    clf.fit(X_train, y_train)<br><br>    <span class="hljs-comment"># 评估训练集</span><br>    y_train_pred = clf.predict(X_train)<br>    train_acc = accuracy_score(y_train, y_train_pred)<br><br>    <span class="hljs-comment"># 评估测试集</span><br>    y_pred = clf.predict(X_test)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Confusion Matrix:&quot;</span>)<br>    <span class="hljs-built_in">print</span>(confusion_matrix(y_test, y_pred))  <span class="hljs-comment"># 输出分类结果矩阵</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Classification Report:&quot;</span>)<br>    <span class="hljs-built_in">print</span>(classification_report(y_test, y_pred))  <span class="hljs-comment"># 输出混淆矩阵</span><br>    acc = accuracy_score(y_test, y_pred)<br>    accuracies.append(acc)<br>    split += <span class="hljs-number">1</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;Training/Testing Accuracy for <span class="hljs-subst">&#123;split&#125;</span> split : <span class="hljs-subst">&#123;train_acc, acc&#125;</span>&#x27;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Average accuracy over <span class="hljs-subst">&#123;ss.get_n_splits()&#125;</span> splits: <span class="hljs-subst">&#123;<span class="hljs-built_in">sum</span>(accuracies) / <span class="hljs-built_in">len</span>(accuracies)&#125;</span>&quot;</span>)<br><br><span class="hljs-comment"># # 5折交叉验证</span><br><span class="hljs-comment"># cv_scores = cross_val_score(clf, X, y, cv=5, scoring=&#x27;accuracy&#x27;)  # cv=5 表示5折交叉验证</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment"># # 输出每次的得分和平均准确率</span><br><span class="hljs-comment"># print(&quot;Cross-validation scores for each fold:&quot;, cv_scores)</span><br><span class="hljs-comment"># print(&quot;Average accuracy:&quot;, cv_scores.mean())  # 0.52</span><br><br><span class="hljs-comment"># # 留一法交叉验证， 运行时间太长，别运行</span><br><span class="hljs-comment"># loo = LeaveOneOut()</span><br><span class="hljs-comment"># loo_scores = cross_val_score(clf, X, y, cv=loo, scoring=&#x27;accuracy&#x27;)</span><br><span class="hljs-comment"># print(f&quot;Average accuracy with LOO: &#123;loo_scores.mean()&#125;&quot;)</span><br><br><br><span class="hljs-comment"># 特征重要性</span><br>importances = clf.feature_importances_<br><span class="hljs-built_in">print</span>(importances)<br><br><span class="hljs-comment"># 画条形图</span><br>plt.barh(<span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(importances)), importances)<br><br><span class="hljs-comment"># 添加标题和特征名称</span><br>plt.title(<span class="hljs-string">&quot;Feature Importances&quot;</span>)<br>plt.yticks(<span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(importances)), feature_names)  <span class="hljs-comment"># 确保特征名称与特征数一致</span><br><br><span class="hljs-comment"># 保存图像</span><br>plt.savefig(<span class="hljs-string">&#x27;feature_importance.png&#x27;</span>)<br><br><span class="hljs-comment"># 可视化随机森林中的一棵树</span><br>dot_data = export_graphviz(clf.estimators_[<span class="hljs-number">0</span>], out_file=<span class="hljs-literal">None</span>,<br>                           feature_names=feature_names)<br><br><span class="hljs-comment"># 使用 graphviz 库读取 dot 文件并生成决策树可视化图形</span><br>graph = graphviz.Source(dot_data)<br>graph.render(<span class="hljs-string">&#x27;decision_tree&#x27;</span>)<br><br><br></code></pre></td></tr></table></figure><h1>SVM</h1><p><a href="https://blog.csdn.net/cxyxx12/article/details/134415329">代码基于此博客</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> svm<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">SVM参数</span><br><span class="hljs-string">C:惩罚系数，越高越不能容易出现误差，容易过拟合（越小则容易欠拟合）(但是此模型提高C不能改善欠拟合）</span><br><span class="hljs-string">gamma：选择径向基函数（RBF）作为kernel后，该函数自带的一个参数，gamma越大，支持向量越少，gamma越小，支持向量越多</span><br><span class="hljs-string"></span><br><span class="hljs-string">核函数：</span><br><span class="hljs-string">线性核Linear</span><br><span class="hljs-string">多项式核Poly</span><br><span class="hljs-string">高斯核rbf</span><br><span class="hljs-string">拉普拉斯核</span><br><span class="hljs-string">Sigmoid核</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br>svm.SVC(<br>    C=<span class="hljs-number">1.0</span>,<br>    kernel=<span class="hljs-string">&#x27;rbf&#x27;</span>,<br>    gamma=<span class="hljs-string">&#x27;scale&#x27;</span><br>)<br>df = pd.read_excel(<span class="hljs-string">&#x27;MLinput.xlsx&#x27;</span>)<br><span class="hljs-comment"># 去除后accuracy变低了</span><br><span class="hljs-comment"># df = df.drop([&#x27;X&#x27;, &#x27;Y&#x27;, &#x27;Z&#x27;, &#x27;sca&#x27;], axis=1)</span><br><br>feature = df.columns.tolist()<br><span class="hljs-keyword">del</span> feature[<span class="hljs-number">0</span>]<br><span class="hljs-comment"># print(feature)</span><br><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">3、滑坡/非滑坡数量情况统计</span><br><span class="hljs-string">观察样本集 非滑坡:0 与 滑坡:1 数量情况</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns<br><span class="hljs-keyword">from</span> matplotlib <span class="hljs-keyword">import</span> pyplot <span class="hljs-keyword">as</span> plt<br><br>sns.countplot(x=df[<span class="hljs-string">&#x27;class&#x27;</span>])<br><span class="hljs-comment"># plt.show()</span><br><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">筛选特征--&gt;降维</span><br><span class="hljs-string">特征里面两两特征之间的相关性</span><br><span class="hljs-string">    - 相关程度非常高：选择其中一个作为代表即可</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br>feature_corr = df[feature].corr()<br><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">来进行相关性的可视化</span><br><span class="hljs-string">- 热力图：颜色越浅说明相关程度越大</span><br><span class="hljs-string">    - annot=True--&gt;显示每个方格的数据</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br>plt.figure(figsize=(<span class="hljs-number">14</span>, <span class="hljs-number">8</span>))<br>sns.heatmap(feature_corr, annot=<span class="hljs-literal">False</span>)<br><span class="hljs-comment"># plt.show()</span><br><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">4  数据分割</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<br><br>train, test = train_test_split(df, test_size=<span class="hljs-number">0.3</span>)  <span class="hljs-comment"># 纯随机采样</span><br>train_X = train[feature]<br>test_X = test[feature]<br><span class="hljs-comment"># 构建 标签</span><br>train_y = train[<span class="hljs-string">&#x27;class&#x27;</span>]<br>y_test = test[<span class="hljs-string">&#x27;class&#x27;</span>]<br><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">5  数据归一化</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> StandardScaler<br><br>ss = StandardScaler()<br>train_X = ss.fit_transform(train_X)<br>test_X = ss.fit_transform(test_X)<br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">6  模型训练及预测</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-keyword">from</span> sklearn.svm <span class="hljs-keyword">import</span> SVC<br>model = SVC()<br>model.fit(train_X, train_y)<br><br>y_pred = model.predict(test_X)<br><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">7 模型评价</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> confusion_matrix, classification_report, accuracy_score<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Confusion Matrix:&quot;</span>)<br><span class="hljs-built_in">print</span>(confusion_matrix(y_test, y_pred))  <span class="hljs-comment"># 输出分类结果矩阵</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Classification Report:&quot;</span>)<br><span class="hljs-built_in">print</span>(classification_report(y_test, y_pred))  <span class="hljs-comment"># 输出混淆矩阵</span><br><br>train_acc = model.score(train_X, train_y)  <span class="hljs-comment"># 内联函数</span><br>acc = accuracy_score(y_test, y_pred)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;Training/Testing Accuracy: <span class="hljs-subst">&#123;train_acc, acc&#125;</span>&#x27;</span>)  <span class="hljs-comment"># 0.6293</span><br></code></pre></td></tr></table></figure><h1>LR</h1><p>代码基于此博客SVM</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LogisticRegression<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> GridSearchCV<br><br>LR = LogisticRegression()<br><br><br>param_grid = &#123;<br>    <span class="hljs-string">&#x27;C&#x27;</span>: [<span class="hljs-number">1</span>],<br>    <span class="hljs-string">&#x27;penalty&#x27;</span>: [<span class="hljs-string">&#x27;l1&#x27;</span>],  <span class="hljs-comment"># 决定正则化类型</span><br>    <span class="hljs-string">&#x27;solver&#x27;</span>: [<span class="hljs-string">&#x27;liblinear&#x27;</span>],  <span class="hljs-comment"># 决定优化算法</span><br>    <span class="hljs-string">&#x27;max_iter&#x27;</span>: [<span class="hljs-number">1000</span>]<br>&#125;<br><span class="hljs-comment"># 自动化参数调优</span><br>grid_search = GridSearchCV(LR, param_grid, cv=<span class="hljs-number">5</span>)<br><br>df = pd.read_excel(<span class="hljs-string">&#x27;MLinput.xlsx&#x27;</span>)<br><br>feature = df.columns.tolist()<br><span class="hljs-keyword">del</span> feature[<span class="hljs-number">0</span>]<br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">4  数据分割</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<br><br>train, test = train_test_split(df, test_size=<span class="hljs-number">0.3</span>)  <span class="hljs-comment"># 纯随机采样</span><br>train_X = train[feature]<br>test_X = test[feature]<br><span class="hljs-comment"># 构建 标签</span><br>train_y = train[<span class="hljs-string">&#x27;class&#x27;</span>]<br>y_test = test[<span class="hljs-string">&#x27;class&#x27;</span>]<br><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">5  数据归一化</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> StandardScaler<br><br>ss = StandardScaler()<br>train_X = ss.fit_transform(train_X)<br>test_X = ss.fit_transform(test_X)<br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">6  寻找最佳参数,训练模型</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br>grid_search.fit(train_X, train_y)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Best parameters:&quot;</span>, grid_search.best_params_)<br><span class="hljs-comment"># 使用最优模型进行预测</span><br>best_model = grid_search.best_estimator_<br>y_pred = best_model.predict(test_X)<br><br><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">7 模型评价</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> confusion_matrix, classification_report, accuracy_score<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Confusion Matrix:&quot;</span>)<br><span class="hljs-built_in">print</span>(confusion_matrix(y_test, y_pred))  <span class="hljs-comment"># 输出分类结果矩阵</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Classification Report:&quot;</span>)<br><span class="hljs-built_in">print</span>(classification_report(y_test, y_pred))  <span class="hljs-comment"># 输出混淆矩阵</span><br><br>train_acc = best_model.score(train_X, train_y)  <span class="hljs-comment"># 内联函数</span><br>acc = accuracy_score(y_test, y_pred)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;Training/Testing Accuracy: <span class="hljs-subst">&#123;train_acc, acc&#125;</span>&#x27;</span>)  <span class="hljs-comment"># 0.6239</span><br></code></pre></td></tr></table></figure><h1>分类器筛选</h1><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-keyword">from</span> sklearn.ensemble import RandomForestClassifier<br><span class="hljs-keyword">from</span> sklearn.linear_model import LogisticRegression<br><span class="hljs-keyword">from</span> sklearn.svm import SVC<br><span class="hljs-keyword">from</span> sklearn.neighbors import KNeighborsClassifier<br><span class="hljs-keyword">from</span> sklearn.tree import DecisionTreeClassifier, export_graphviz<br><span class="hljs-keyword">from</span> sklearn.metrics import confusion_matrix, classification_report, accuracy_score<br><span class="hljs-keyword">from</span> sklearn.model_selection import ShuffleSplit, cross_val_score, LeaveOneOut, StratifiedShuffleSplit<br>import pandas as pd<br><span class="hljs-keyword">from</span> sklearn.preprocessing import StandardScaler<br><span class="hljs-keyword">from</span> sklearn.ensemble import GradientBoostingClassifier<br><span class="hljs-keyword">from</span> xgboost import XGBClassifier<br><span class="hljs-keyword">from</span> sklearn.ensemble import AdaBoostClassifier<br><span class="hljs-keyword">from</span> lightgbm import LGBMClassifier<br><span class="hljs-keyword">from</span> sklearn.neural_network import MLPClassifier<br><br><br><span class="hljs-comment"># # 加载数据</span><br><span class="hljs-comment"># df1 = pd.read_excel(&#x27;DataX.xlsx&#x27;)</span><br><span class="hljs-comment"># XX = df1.values  # 输入特征，返回给定DataFrame的Numpy表现形式</span><br><span class="hljs-comment"># feature_names = df1.columns.tolist()  # 将文件列名转换为Python列表</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment"># df2 = pd.read_excel(&#x27;DataY.xlsx&#x27;)</span><br><span class="hljs-comment"># yy = df2.values.ravel()  # 目标变量</span><br><br>data = pd.read_csv(<span class="hljs-string">&#x27;sxcoor.csv&#x27;</span>)<br><span class="hljs-comment"># 假设最后一列是标签，前面的列是特征</span><br>XX = data.iloc[1:, 1:].values  # 特征<br>yy = data.iloc[1:, 0].values   # 标签<br><br>s = StratifiedShuffleSplit(<span class="hljs-attribute">n_splits</span>=1, <span class="hljs-attribute">test_size</span>=0.5, <span class="hljs-attribute">random_state</span>=0)<br><br><span class="hljs-keyword">for</span> half1, half2 <span class="hljs-keyword">in</span> s.split(XX, yy):<br>    X, new_data = XX[half1], XX[half2]<br>    y, new_data_y = yy[half1], yy[half2]<br><br><br><span class="hljs-comment"># 用于存储准确率大于 0.7 的模型及其最佳参数</span><br>qualified_models = &#123;&#125;<br><br><span class="hljs-comment"># 创建随机森林分类器</span><br><span class="hljs-comment"># clf = RandomForestClassifier(n_estimators=100)</span><br>RF = RandomForestClassifier(<span class="hljs-attribute">class_weight</span>=<span class="hljs-string">&#x27;balanced&#x27;</span>,<br>                             <span class="hljs-attribute">n_estimators</span>=100,  # 树的数量<br>                             # <span class="hljs-attribute">max_depth</span>=10,  # 树的最大深度,设置后欠拟合<br>                             <span class="hljs-attribute">min_samples_split</span>=2,  # 节点分裂所需的最小样本数<br>                             <span class="hljs-attribute">min_samples_leaf</span>=1  # 叶节点的最小样本数<br>                             )<br><br><br>LR = LogisticRegression(<span class="hljs-attribute">C</span>=1, <span class="hljs-attribute">penalty</span>=<span class="hljs-string">&#x27;l1&#x27;</span>, <span class="hljs-attribute">solver</span>=<span class="hljs-string">&#x27;liblinear&#x27;</span>, <span class="hljs-attribute">max_iter</span>=1000)<br><br>SVM = SVC(<br>    <span class="hljs-attribute">C</span>=1.0,<br>    <span class="hljs-attribute">kernel</span>=<span class="hljs-string">&#x27;rbf&#x27;</span>,<br>    <span class="hljs-attribute">gamma</span>=<span class="hljs-string">&#x27;scale&#x27;</span><br>)<br><br>KNN = KNeighborsClassifier(<span class="hljs-attribute">n_neighbors</span>=6)<br><br>DT = DecisionTreeClassifier(<span class="hljs-attribute">random_state</span>=42)<br><br>GBC = GradientBoostingClassifier(<span class="hljs-attribute">n_estimators</span>=100, <span class="hljs-attribute">learning_rate</span>=0.1, <span class="hljs-attribute">max_depth</span>=3, <span class="hljs-attribute">random_state</span>=42)<br><br>XGB = XGBClassifier(<span class="hljs-attribute">n_estimators</span>=100, <span class="hljs-attribute">learning_rate</span>=0.1, <span class="hljs-attribute">max_depth</span>=3, <span class="hljs-attribute">random_state</span>=42)<br><br>ABC = AdaBoostClassifier(<span class="hljs-attribute">n_estimators</span>=100, <span class="hljs-attribute">random_state</span>=42)<br><br>LGBM = LGBMClassifier(<span class="hljs-attribute">n_estimators</span>=100, <span class="hljs-attribute">num_leaves</span>=31, <span class="hljs-attribute">max_depth</span>=-1, <span class="hljs-attribute">learning_rate</span>=0.1, <span class="hljs-attribute">random_state</span>=42,<br>                      <span class="hljs-attribute">force_col_wise</span>=<span class="hljs-string">&#x27;true&#x27;</span>)<br><br>MLP = MLPClassifier(hidden_layer_sizes=(100,), <span class="hljs-attribute">max_iter</span>=500, <span class="hljs-attribute">activation</span>=<span class="hljs-string">&#x27;relu&#x27;</span>, <span class="hljs-attribute">solver</span>=<span class="hljs-string">&#x27;adam&#x27;</span>, <span class="hljs-attribute">random_state</span>=42)<br><br>model = &#123;<br>    <span class="hljs-string">&#x27;RF&#x27;</span>: RF,<br>    <span class="hljs-string">&#x27;LR&#x27;</span>: LR,<br>    <span class="hljs-string">&#x27;SVM&#x27;</span>: SVM,<br>    <span class="hljs-string">&#x27;KNN&#x27;</span>: KNN,<br>    <span class="hljs-string">&#x27;DT&#x27;</span>: DT,<br>    <span class="hljs-string">&#x27;GBC&#x27;</span>: GBC,<br>    <span class="hljs-string">&#x27;XGB&#x27;</span>: XGB,<br>    <span class="hljs-string">&#x27;MLP&#x27;</span>: MLP,<br>    <span class="hljs-string">&#x27;ABC&#x27;</span>: ABC,<br>    <span class="hljs-string">&#x27;LGBM&#x27;</span>: LGBM<br>&#125;<br><br><span class="hljs-comment"># 创建ShuffleSplit对象，用于执行自动洗牌</span><br>ss = StratifiedShuffleSplit(<span class="hljs-attribute">n_splits</span>=1, <span class="hljs-attribute">test_size</span>=0.3, <span class="hljs-attribute">random_state</span>=0)  # 实现分层、随机采样<br><br>scaler = StandardScaler()<br><span class="hljs-keyword">for</span> name, clf <span class="hljs-keyword">in</span> model.items():<br><br>    accuracies = []<br>    split = 0<br><br>    # 循环遍历每个拆分，并使用随机森林分类器对每个拆分进行训练和评估<br>    <span class="hljs-keyword">for</span> train_index, test_index <span class="hljs-keyword">in</span> ss.split(X, y):<br>        X_train, X_test = X[train_index], X[test_index]<br>        y_train, y_test = y[train_index], y[test_index]<br><br>        # 树形模型不需要归一化<br>        <span class="hljs-keyword">if</span> name != <span class="hljs-string">&#x27;RF&#x27;</span> <span class="hljs-keyword">and</span> name != <span class="hljs-string">&#x27;DT&#x27;</span>:<br><br>            X_train = scaler.fit_transform(X_train)<br>            X_test = scaler.transform(X_test)<br><br>        clf.fit(X_train, y_train)<br><br>        # 评估训练集<br>        y_train_pred = clf.predict(X_train)<br>        train_acc = accuracy_score(y_train, y_train_pred)<br><br>        # 评估测试集<br>        y_pred = clf.predict(X_test)<br>        acc = accuracy_score(y_test, y_pred)<br>        accuracies.append(acc)<br>        split += 1<br>        # 筛选准确率大于 0.69 的模型<br><br>        # <span class="hljs-built_in">print</span>(f<span class="hljs-string">&#x27;Training/Testing Accuracy for &#123;split&#125; split : &#123;train_acc, acc&#125;&#x27;</span>)<br>    accuracy = sum(accuracies) / len(accuracies)<br>    <span class="hljs-built_in">print</span>(f<span class="hljs-string">&quot;&#123;clf.__class__.__name__&#125; learner Average accuracy over &#123;split&#125; splits: &#123;accuracy&#125;&quot;</span>)<br>    <span class="hljs-keyword">if</span> accuracy &gt;= 0.69:<br>        qualified_models[name] = clf<br>        <span class="hljs-built_in">print</span>(f<span class="hljs-string">&quot;&#123;name&#125; 模型被选为合格模型&quot;</span>)<br><br><span class="hljs-keyword">for</span> name, clf <span class="hljs-keyword">in</span> qualified_models.items():<br>    <span class="hljs-keyword">if</span> name != <span class="hljs-string">&#x27;RF&#x27;</span> <span class="hljs-keyword">and</span> name != <span class="hljs-string">&#x27;DT&#x27;</span>:<br>        new_data_scaled = scaler.transform(new_data)<br>    <span class="hljs-keyword">else</span>:<br>        new_data_scaled = new_data<br>    new_predictions = clf.predict(new_data_scaled)<br>    acc = accuracy_score(new_data_y, new_predictions)<br>    <span class="hljs-built_in">print</span>(f<span class="hljs-string">&quot;&#123;name&#125; Accuracy: &#123;acc&#125;&quot;</span>)<br></code></pre></td></tr></table></figure>]]></content>
    
    
    
    <tags>
      
      <tag>代码</tag>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>文献笔记|基于物理数据的数据驱动模型用于三峡库区滑坡易发性评估</title>
    <link href="/2024/10/17/%E6%96%87%E7%8C%AE%E7%AC%94%E8%AE%B0%E5%9F%BA%E4%BA%8E%E7%89%A9%E7%90%86%E6%95%B0%E6%8D%AE%E7%9A%84%E6%95%B0%E6%8D%AE%E9%A9%B1%E5%8A%A8%E6%A8%A1%E5%9E%8B%E7%94%A8%E4%BA%8E%E4%B8%89%E5%B3%A1%E5%BA%93%E5%8C%BA%E6%BB%91%E5%9D%A1%E6%98%93%E5%8F%91%E6%80%A7%E8%AF%84%E4%BC%B0/"/>
    <url>/2024/10/17/%E6%96%87%E7%8C%AE%E7%AC%94%E8%AE%B0%E5%9F%BA%E4%BA%8E%E7%89%A9%E7%90%86%E6%95%B0%E6%8D%AE%E7%9A%84%E6%95%B0%E6%8D%AE%E9%A9%B1%E5%8A%A8%E6%A8%A1%E5%9E%8B%E7%94%A8%E4%BA%8E%E4%B8%89%E5%B3%A1%E5%BA%93%E5%8C%BA%E6%BB%91%E5%9D%A1%E6%98%93%E5%8F%91%E6%80%A7%E8%AF%84%E4%BC%B0/</url>
    
    <content type="html"><![CDATA[<h2 id="文献推荐链接"><a href="https://mp.weixin.qq.com/s/3fUJaTqLTu-UgQkbqGmKxw">文献推荐链接</a></h2><p><a href="https://www.sciencedirect.com/science/article/pii/S1674987123000889?ref=pdf_download&amp;fr=RR-2&amp;rr=8d341b785e93ddc2#ab010">原文链接</a></p><h2 id="总述">总述</h2><p><img src="/image/%E6%B5%81%E7%A8%8B%E5%9B%BE.jpg" alt=""></p><table><thead><tr><th>数据处理</th><th>预测模型</th><th>评估方法</th></tr></thead><tbody><tr><td>Scoops 3D</td><td>随机森林法</td><td>AUC</td></tr></tbody></table><p>问题：非滑坡样本的生成<br>解决现状：分形理论模型生成的低坡区、无滑坡区和极低易发区的随机选择</p><p>Scoops 3D：一种具有很高的可解释性的物理方法，三维极限平衡方法，考虑了边坡的几何形状、材料特性和边界条件</p><h3 id="数据">数据</h3><p>研究区域：巴东县、秭归县；<br>方法：ArcGIS处理、提取收集的数据，将处理后的土地深度数据和相应的岩土参数输入到Scoops 3D，（安全系数）FS大于1.5被随机选择为负样本，从而建立完整数据集<br>数据收集：影响因子根据过往经验选取<br><img src="/image/%E5%BD%B1%E5%93%8D%E5%9B%A0%E5%AD%90.png" alt=""><br>数据处理：使用ArcGIS</p>]]></content>
    
    
    
    <tags>
      
      <tag>文献</tag>
      
      <tag>滑坡</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>环境搭建|fluid搭建指南</title>
    <link href="/2024/10/16/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BAfluid%E6%90%AD%E5%BB%BA%E6%8C%87%E5%8D%97/"/>
    <url>/2024/10/16/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BAfluid%E6%90%AD%E5%BB%BA%E6%8C%87%E5%8D%97/</url>
    
    <content type="html"><![CDATA[<p><a href="https://blog.csdn.net/yaorongke/article/details/119089190?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522DEC1D3C5-1243-434F-A99D-A0CFCB1371B1%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=DEC1D3C5-1243-434F-A99D-A0CFCB1371B1&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-119089190-null-null.142%5Ev100%5Epc_search_result_base8&amp;utm_term=github%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E7%BD%91%E7%AB%99&amp;spm=1018.2226.3001.4187">hexo配置指南</a><br><a href="https://hexo.fluid-dev.com/docs/guide/#%E5%85%B3%E4%BA%8E%E6%8C%87%E5%8D%97">fluid配置指南</a><br>使用了katex渲染公式</p>]]></content>
    
    
    
    <tags>
      
      <tag>环境搭建</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>组合数学</title>
    <link href="/2024/10/16/%E7%BB%84%E5%90%88%E6%95%B0%E5%AD%A6/"/>
    <url>/2024/10/16/%E7%BB%84%E5%90%88%E6%95%B0%E5%AD%A6/</url>
    
    <content type="html"><![CDATA[<h1>组合数学考试速成</h1><p>第一章、排列与组合</p><ol><li>高中学过的。</li><li>允许重复的组合、不相邻组合、格路模型</li><li>插空法、隔板法。</li><li>排序生成算法（三选一、考定义和具体实操）</li></ol><p>第二章、递推关系和母函数</p><ol><li>给母函数求an</li><li>给递推关系求母函数和an</li><li>给母函数求递推关系</li><li>解常系数齐次递推关系</li><li>司特林数</li></ol><p>第三章、容斥原理和鸽巢原理</p><ol><li>考得容易，基本都是考鸽巢原理。但也有容斥原理。</li><li>棋盘多项式。有禁区的排列。</li><li>Ramsey数，低概率。</li></ol><p>第四章、Burnside引理与Polya定理</p><ol><li>只考Polya定理，Burnside引理能解决的Polya定理也能干。</li></ol><p>第五章、区组设计</p><ol><li><p>什么是拉丁方，如何构造拉丁方？</p></li><li><p>（b，v，r，k，朗达）</p></li></ol><p><img src="/_posts/%E7%BB%84%E5%90%88%E6%95%B0%E5%AD%A6/image-20220111165410730.png" alt="image-20220111165410730"></p>]]></content>
    
    
    
    <tags>
      
      <tag>数学</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>重生之我是天才程序员</title>
    <link href="/2024/10/16/%E9%87%8D%E7%94%9F%E4%B9%8B%E6%88%91%E6%98%AF%E5%A4%A9%E6%89%8D%E7%A8%8B%E5%BA%8F%E5%91%98/"/>
    <url>/2024/10/16/%E9%87%8D%E7%94%9F%E4%B9%8B%E6%88%91%E6%98%AF%E5%A4%A9%E6%89%8D%E7%A8%8B%E5%BA%8F%E5%91%98/</url>
    
    <content type="html"><![CDATA[<h1>sandaokansini为何那样</h1><h2 id="10-16之debug顺序">10.16之debug顺序</h2><p>关于配置的问题应该先看<strong>README</strong><br>再网页<strong>搜索</strong><br>实在不行问GPT（不推荐）</p>]]></content>
    
    
    
    <tags>
      
      <tag>日志</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>机器学习|基础知识</title>
    <link href="/2024/10/11/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    <url>/2024/10/11/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/</url>
    
    <content type="html"><![CDATA[<h1>机器学习</h1><h2 id="绪论">绪论</h2><p>根据训练数据是否拥有标记信息，学习任务可大致分为<strong>监督学习</strong>（分类、回归）和 <strong>无监督学习</strong>（聚类）<br>聚类：将训练集数据分组，每组称为一个“簇”<br>泛化能力：学得模型适用于新样本的能力</p><h2 id="模型评估与选择">模型评估与选择</h2><p>精度=1-错误率<br><strong>过拟合</strong>：学习器将训练样本自身的特点当作所有潜在样本都具有的性质，导致泛化能力的下降<br>欠拟合：训练样本的一般性质未学好</p><h3 id="评估方法">评估方法</h3><p>将数据集D划分为训练集S和测试集T<br><strong>留出法</strong>：直接将D划分为两个互斥的集合，分层采样<br><strong>交叉验证法</strong>（p次k折交叉验证）：将D划分为k个大小相似的互斥子集，分层采样，每次用k-1个子集的并集作为训练集，剩下的为测试集，共k组训练/测试集，此过程重复p次<br>特例：留一法（LOO，p=k）<br><strong>自助法</strong>：通过<strong>自助采样</strong>生成D’（D中样本不在D’中出现的概率为(（1-<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mn>1</mn><mi>m</mi></mfrac></mrow><annotation encoding="application/x-tex">\frac{1}{m}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1901em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8451em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>）<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mi>m</mi></msup></mrow><annotation encoding="application/x-tex">^{m}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6644em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6644em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span></span></span></span></span></span></span></span></span></span></span></span>=<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mn>1</mn><mi>e</mi></mfrac></mrow><annotation encoding="application/x-tex">\frac{1}{e}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1901em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8451em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">e</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>=0.368），将D’作为训练集（改变初始数据集分布引入偏差），D\D’（:集合减法）作为测试集，使实际评估的模型和期望估计的模型使用训练样本数一致，且约1/3的没在训练集中出现的样本用于测试，这样的测试结果称为<strong>包外估计</strong><br>验证集：模型评估与选择中用于评估测试的数据集</p><h3 id="性能度量">性能度量</h3><p>衡量泛化性能的评估标准<br><strong>错误率与精度accuracy</strong>：相加等于1<br><strong>查准率precision</strong>：P=<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi>T</mi><mi>P</mi></mrow><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>P</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{TP}{TP+FP}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2757em;vertical-align:-0.4033em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8723em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">TP</span><span class="mbin mtight">+</span><span class="mord mathnormal mtight" style="margin-right:0.13889em;">FP</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">TP</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4033em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span><br><strong>查全率recall</strong>：R=<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi>T</mi><mi>P</mi></mrow><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>N</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{TP}{TP+FN}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2757em;vertical-align:-0.4033em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8723em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">TP</span><span class="mbin mtight">+</span><span class="mord mathnormal mtight" style="margin-right:0.10903em;">FN</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">TP</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4033em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span><br>混淆矩阵（confusion matrix）</p><table><thead><tr><th>正例</th><th>反例</th></tr></thead><tbody><tr><td>TP：true positive真正例</td><td>FP：false positive假正例</td></tr><tr><td>TN：true negative真反例</td><td>FN：false negative假反例</td></tr></tbody></table><p>P-R曲线：面积越大，学习器性能越好<br>平衡点（BEP）：P=R时取值，值越大性能越优</p><p><strong><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>F</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">F_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></strong>=<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mn>2</mn><mo>∗</mo><mi>P</mi><mo>∗</mo><mi>R</mi></mrow><mrow><mi>P</mi><mo>+</mo><mi>R</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{2*P*R}{P+R}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2757em;vertical-align:-0.4033em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8723em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">P</span><span class="mbin mtight">+</span><span class="mord mathnormal mtight" style="margin-right:0.00773em;">R</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span><span class="mbin mtight">∗</span><span class="mord mathnormal mtight" style="margin-right:0.13889em;">P</span><span class="mbin mtight">∗</span><span class="mord mathnormal mtight" style="margin-right:0.00773em;">R</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4033em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>=<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mn>2</mn><mo>∗</mo><mi>T</mi><mi>P</mi></mrow><mrow><mi>m</mi><mo>+</mo><mi>T</mi><mi>P</mi><mo>−</mo><mi>T</mi><mi>N</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{2*TP}{m+TP-TN}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2757em;vertical-align:-0.4033em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8723em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mbin mtight">+</span><span class="mord mathnormal mtight" style="margin-right:0.13889em;">TP</span><span class="mbin mtight">−</span><span class="mord mathnormal mtight" style="margin-right:0.10903em;">TN</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span><span class="mbin mtight">∗</span><span class="mord mathnormal mtight" style="margin-right:0.13889em;">TP</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4033em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span><br><em><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>F</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">F_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>:基于查准率和查全率的调和平均</em><br><em><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>F</mi><mi>β</mi></msub></mrow><annotation encoding="application/x-tex">F_β</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05278em;">β</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span>：加权调和平均</em></p><p><strong>ROC</strong>：受试者工作特性<br>横轴：假正例率（FPR）=<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi>F</mi><mi>P</mi></mrow><mrow><mi>T</mi><mi>N</mi><mo>+</mo><mi>F</mi><mi>P</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{FP}{TN+FP}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2757em;vertical-align:-0.4033em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8723em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">TN</span><span class="mbin mtight">+</span><span class="mord mathnormal mtight" style="margin-right:0.13889em;">FP</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">FP</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4033em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span><br>纵轴：真正例率（TPR）=<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi>T</mi><mi>P</mi></mrow><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>N</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{TP}{TP+FN}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2757em;vertical-align:-0.4033em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8723em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">TP</span><span class="mbin mtight">+</span><span class="mord mathnormal mtight" style="margin-right:0.10903em;">FN</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">TP</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4033em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span><br>用面积大小（即<strong>AUC</strong>）评价性能好坏</p><p><strong>代价敏感错误率与代价曲线</strong><br>代价敏感错误率<br><img src="/image/%E4%BB%A3%E4%BB%B7%E6%95%8F%E6%84%9F%E9%94%99%E8%AF%AF%E7%8E%87.png" alt=""><br>代价曲线图（cost curve）<br>横轴：正例概率代价<br>纵轴：归一化代价<br><a href="https://blog.csdn.net/lg201601/article/details/106200046">具体见</a></p><h3 id="比较检验">比较检验</h3><p><strong>交叉验证t检验</strong><a href="https://blog.csdn.net/orDream/article/details/123409819">解释</a><br><strong>McNemar检验</strong> <a href="https://blog.csdn.net/orDream/article/details/122540099">解释</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>知识库</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ArcGIS|实验问题</title>
    <link href="/2024/10/10/ArcGIS%E5%AE%9E%E9%AA%8C%E9%97%AE%E9%A2%98/"/>
    <url>/2024/10/10/ArcGIS%E5%AE%9E%E9%AA%8C%E9%97%AE%E9%A2%98/</url>
    
    <content type="html"><![CDATA[<h1>使用ArcGIS处理数据</h1><h2 id="基本步骤">基本步骤</h2><p>1、将需要的数据下载到本地<br>（例：DEM栅格文件）<br>2、栅格文件要素值提取至点，生成包含新的元素的点要素文件</p><h2 id="可能出现的问题">可能出现的问题</h2><p>1、属性表不见了（双屏操作，在边边上拖过来就行了）<br>2、栅格文件导入时闪退（可能与原有点要素文件不兼容，建议更换文件）</p>]]></content>
    
    
    
    <tags>
      
      <tag>知识库</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
